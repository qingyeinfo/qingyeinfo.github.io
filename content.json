{"pages":[{"title":"","text":"个人简介 青叶: 但得夕阳无限好,何须惆怅近黄昏 职业: 运维工程师 目标: 人管代码，代码管机器 E-mail: qingyeinfo@gmail.com 博客信息 本站搭建于2019-11-05，是青叶的个人博客。 本站主要总结记录个人技术上面的经验，也会借鉴技术大佬的文笔。 本博客框架主要参考了水寒和辣椒の酱的博客,多谢大佬开源。 计划2020计划 2020.03.05 2020-GOALS 增加体重为首要目标 增加锻炼，至少一周一次锻炼 学习更多的开发技术，今年要求自己至少买5本实体书并看一遍 学习理财方面的知识 生活领悟","link":"/about/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：http://cloud.qingye.info/images/avatar.jpg 网站名称：青叶の博客 网站地址：http://qingye.info/ 网站简介：业务运维，技术分享 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"","text":"宋慧乔 刘涛 陈数 图片搜集于互联网，侵权请留言，马上处理😊。","link":"/album/index.html"},{"title":"","text":"&nbsp;&nbsp;千千音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出. L2Dwidget.init({\"pluginRootPath\":\"live2dw/\",\"pluginJsPath\":\"lib/\",\"pluginModelPath\":\"assets/\",\"tagMode\":false,\"debug\":false,\"model\":{\"jsonPath\":\"/live2dw/assets/miku.model.json\"},\"display\":{\"position\":\"right\",\"width\":300,\"height\":600},\"mobile\":{\"show\":true},\"react\":{\"opacity\":0.7},\"log\":false});","link":"/media/index.html"},{"title":"","text":"--- 温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"客至唐 · 杜甫舍南舍北皆春水，但见群鸥日日来。花径不曾缘客扫，蓬门今始为君开。盘飧市远无兼味，樽酒家贫只旧醅。肯与邻翁相对饮，隔篱呼取尽馀杯。","link":"/message/index.html"},{"title":"","text":"碎碎念 tips：github登录后按时间正序查看、可点赞加❤️、本插件地址..「+99次查看」 var gitalk = new Gitalk({ clientID: '46a9f3481b46ea0129d8', clientSecret: '79c7c9cb847e141757d7864453bcbf89f0655b24', id: '666666', repo: 'issue_database', owner: 'removeif', admin: \"removeif\", createIssueManually: true, distractionFreeMode: false }) gitalk.render('comment-container1')","link":"/self-talking/index.html"},{"title":"k8s专题[1.k8s基础概念]","text":"1.概念Kubernetes是一个容器编排系统，也就是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。 Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。 Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。 2.特点 可移植: 支持公有云，私有云，混合云，多重云（multi-cloud） 可扩展: 模块化, 插件化, 可挂载, 可组合 自动化: 自动部署，自动重启，自动复制，自动伸缩/扩展 3.Kubernetes 架构 4.分层架构Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示： 核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等） 接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS 应用、ChatOps 等Kubernetes 内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等 5.生态系统","link":"/self-talking/k8s-demo.html"},{"title":"","text":"电影 电视剧 动画片","link":"/share/index.html"},{"title":"","text":"《若能绽放光芒》 Document L2Dwidget.init({\"pluginRootPath\":\"live2dw/\",\"pluginJsPath\":\"lib/\",\"pluginModelPath\":\"assets/\",\"tagMode\":false,\"debug\":false,\"model\":{\"jsonPath\":\"/live2dw/assets/miku.model.json\"},\"display\":{\"position\":\"right\",\"width\":300,\"height\":600},\"mobile\":{\"show\":true},\"react\":{\"opacity\":0.7},\"log\":false}); const dp = new DPlayer({ container: document.getElementById('dplayer'), video: { url: 'demo.mp4', pic: 'https://cloud.qingye.info/images/music/5cf8c5d9c57b510947.png', thumbnails: 'https://cloud.qingye.info/images/music/5cf8c5d94521136430.png', }, });","link":"/video/index.html"}],"posts":[{"title":"Centos6 Tengine开启http2传输协议","text":"1.前言最近在优化网站的访问速度，为网站开启http2协议，这个协议有什么优点呢？如下： http2是下一代的传输协议，以后都会普遍用它，是一个趋势。 http2有多路复用特性，意思是访问一个域名下面的资源，多个请求共用一条TCP链路，所以比http1.1要快得多。 2.准备工作 需要重新编译openssl1.0.2以上版本，因为我们系统的版本都是centos6的，不支持直接yum更新openssl，如果是centos7，直接yum update openssl -y 即可更新 编译完成openssl后，需要重新使用openssl的库文件重新编译tengine，我们使用的Tengine版本是Tengine/2.2.2。 3.操作步骤 安装 openssl-1.0.2t 123456789101112131415161718192021222324#进入/usr/local/src,一般软件包都放这里cd /usr/local/src#下载安装包wget https://www.openssl.org/source/openssl-1.0.2t.tar.gztar -zxvf openssl-1.0.2t.tar.gzcd openssl-1.0.2t./config shared zlib#默认安装找/usr/local/sslmake &amp;&amp; make install#先备份之前的老版本mv /usr/bin/openssl /usr/bin/openssl.oldmv /usr/include/openssl /usr/include/openssl.old#建立软连接ln -s /usr/local/ssl/bin/openssl /usr/bin/opensslln -s /usr/local/ssl/include/openssl /usr/include/openssl#把动态库加入系统配置路径echo \"/usr/local/ssl/lib\" &gt;&gt; /etc/ld.so.conf#查看动态库是否生效ldconfig -p#检查openssl版本openssl version 安装Tengine 12345678910111213141516171819202122232425262728293031323334cd /usr/local/srcwget http://tengine.taobao.org/download/tengine-2.2.2.tar.gztar tengine-2.2.2.tar.gzcd tengine-2.2.2#这里需要修改一下tengine的代码，因为我们是手动编译的openssl，依赖库路径和原来系统安装的不太一样，所以需要手动指定vim auto/lib/openssl/conf#在大概32行配置原来配置如下：CORE_INCS=\"$CORE_INCS $OPENSSL/.openssl/include\"CORE_DEPS=\"$CORE_DEPS $OPENSSL/.openssl/include/openssl/ssl.h\"CORE_LIBS=\"$CORE_LIBS $OPENSSL/.openssl/lib/libssl.a\"CORE_LIBS=\"$CORE_LIBS $OPENSSL/.openssl/lib/libcrypto.a\"#修改成如下，保存退出CORE_INCS=\"$CORE_INCS $OPENSSL/include\"CORE_DEPS=\"$CORE_DEPS $OPENSSL/include/openssl/ssl.h\"CORE_LIBS=\"$CORE_LIBS $OPENSSL/lib/libssl.a\"CORE_LIBS=\"$CORE_LIBS $OPENSSL/lib/libcrypto.a\"#先安装一些依赖(我是升级，其实不需要安装依赖了，如果是首次安装tengine，就需要安装依赖)yum -y install zlib zlib-devel openssl openssl-devel pcre pcre-devel gcc gcc-c++ autoconf automake jemalloc jemalloc-devel#开始编译tenginecd /usr/local/src/tengine-2.2.2 &amp;&amp; ./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-http_concat_module --with-jemalloc --with-http_v2_module --with-http_secure_link_module --with-openssl=/usr/local/sslmake##注意，如果是第一安装tegninx，只需要只需执行以下命令make install#但是我是安装过了，所以需要备份老的tenginecp -af /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bakcp -af /usr/local/nginx/sbin/dso_tool /usr/local/nginx/sbin/dso_tool_bak#拷贝编译好的tengine到对应目录cp /usr/local/src/tengine-2.2.2/objs/nginx /usr/local/nginx/sbin/cp /usr/local/src/tengine-2.2.2/objs/dso_tool /usr/local/nginx/sbin/#然后重启tengine，就算编译安装完成啦 tengine http2配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#配置http2很简单，如下：server { #http 不支持http2的传输协议，所以80端口不变 listen 80 # listen在原https配置文件基础上添加http2 listen 443 ssl http2; server_name www.oneq.work; .....}#另外附上一份完整的支持http2的tengine配置upstream server_backend { server ip:80 weight=10; server ip:80 weight=10; keepalive 800;#下面检测端口的配置需要tengine的才有效，不是tengine需要安装额外的插件或者直接注释即可 check interval=5000 rise=3 fall=3 timeout=5000 type=tcp;}server { listen 80; listen 443 ssl http2; server_name xxx.xxx.xxx; req_status server; ssl_certificate /usr/local/nginx/certs/xxx.xxx.xxx.crt; ssl_certificate_key /usr/local/nginx/certs/xxx.xxx.xxx.key; ssl_session_timeout 5m; ssl_protocols TLSv1.1 TLSv1.2 TLSv1; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass_header User-Agent; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Connection \"\"; proxy_http_version 1.1; access_log logs/access.log main;location / { proxy_pass http://server_backend/; access_log logs/server_backend.log main; }error_page 404 /404.html; location = /404.html { root html; }error_page 500 502 503 504 /50x.html; location = /50x.html { root html; }} 4.效果展示 5.总结1.第一次安装tengine和升级步骤有所区别，需要注意下 2.http不支持http2的传输协议，所以80端口还是使用http1.1的协议，https使用http2的传输协议","link":"/2019/11/05/linux/Centos6%20Tengine%20Use%20http2/"},{"title":"跨域请求以及实现跨域的方案","text":"1.什么是跨域请求A cross-domain solution (CDS) is a means of information assurance that provides the ability to manually or automatically access or transfer between two or more differing security domains. 解决两个安全域之间的信息传递，这个就叫做CDS——跨域解决方案. 在 HTML 中，&lt;a&gt;, &lt;form&gt;, &lt;img&gt;, &lt;script&gt;, &lt;iframe&gt;, &lt;link&gt;等标签以及 Ajax(异步 JavaScript 和 XML) 都可以指向一个资源地址，而所谓的跨域请求就是指：当前发起请求的域与该请求指向的资源所在的域不一样。这里的域指的是这样的一个概念：我们认为若协议 + 域名 + 端口号均相同，那么就是同域. 2.跨域请求的安全问题通常，浏览器会对上面提到的跨域请求作出限制。浏览器之所以要对跨域请求作出限制，是出于安全方面的考虑，因为跨域请求有可能被不法分子利用来发动 CSRF攻击。 CSRF攻击：CSRF（Cross-site request forgery），中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。CSRF攻击者在用户已经登录目标网站之后，诱使用户访问一个攻击页面，利用目标网站对用户的信任，以用户身份在攻击页面对目标网站发起伪造用户操作的请求，达到攻击目的。 CSRF 攻击的原理大致描述如下：有两个网站，其中A网站是真实受信任的网站，而B网站是危险网站。在用户登陆了受信任的A网站是，本地会存储A网站相关的Cookie，并且浏览器也维护这一个Session会话。这时，如果用户在没有登出A网站的情况下访问危险网站B，那么危险网站B就可以模拟发出一个对A网站的请求（跨域请求）对A网站进行操作，而在A网站的角度来看是并不知道请求是由B网站发出来的（Session和Cookie均为A网站的），这时便成功发动一次CSRF 攻击。 因而 CSRF 攻击可以简单理解为：攻击者盗用了你的身份，以你的名义发送而已请求。CSRF能够做的事情包括：以你名义发送邮件，发消息，盗取你的账号，甚至于购买商品，虚拟货币转账……造成的问题包括：个人隐私泄露以及财产安全。 3.同源策略在客户端编程语言中，如javascript和ActionScript，同源策略是一个很重要的安全理念，它在保证数据的安全性方面有着重要的意义。同源策略规定跨域之间的脚本是隔离的，一个域的脚本不能访问和操作另外一个域的绝大部分属性和方法 概述：同源策略是 Netscape 提出的一个著名的安全策略同源策略是浏览器最核心最基础的安全策略现在所有的可支持 Javascript 的浏览器都会使用这个策略web构建在同源策略基础之上，浏览器对非同源脚本的限制措施是对同源策略的具体实现 同源策略的含义：DOM 层面的同源策略：限制了来自不同源的”Document”对象或 JS 脚本，对当前“document”对象的读取或设置某些属性Cookie和XMLHttprequest层面的同源策略：禁止 Ajax 直接发起跨域HTTP请求（其实可以发送请求，结果被浏览器拦截，不展示），同时 Ajax 请求不能携带与本网站不同源的 Cookie。同源策略的非绝对性：&lt;script&gt;&lt;img&gt;&lt;iframe&gt;&lt;link&gt;&lt;video&gt;&lt;audio&gt;等带有src属性的标签可以从不同的域加载和执行资源。其他插件的同源策略：flash、java applet、silverlight、googlegears等浏览器加载的第三方插件也有各自的同源策略，只是这些同源策略不属于浏览器原生的同源策略，如果有漏洞则可能被黑客利用，从而留下XSS攻击的后患 跨域例子： 特例: Web页面上调用js文件时则不受是否跨域的影响,不仅如此，凡是拥有”src”这个属性的标签都拥有跨域的能力，比如&lt;script&gt;、&lt;img&gt;、&lt;iframe&gt;. 4.跨域解决方法虽然在安全层面上同源限制是必要的，但有时同源策略会对我们的合理用途造成影响，为了避免开发的应用受到限制，有多种方式可以绕开同源策略，我们经常使用的 JSONP, CORS 方法 4.1 JSONP原理： JSONP 是一种非官方的跨域数据交互协议 JSONP 本质上是利用 &lt;script&gt;&lt;img&gt;&lt;iframe&gt; 等标签不受同源策略限制，可以从不同域加载并执行资源的特性，来实现数据跨域传输。 JSONP由两部分组成：回调函数和数据。回调函数是当响应到来时应该在页面中调用的函数，而数据就是传入回调函数中的JSON数据。 JSONP 的理念就是，与服务端约定好一个回调函数名，服务端接收到请求后，将返回一段 Javascript，在这段 Javascript 代码中调用了约定好的回调函数，并且将数据作为参数进行传递。当网页接收到这段Javascript 代码后，就会执行这个回调函数，这时数据已经成功传输到客户端了。示例：定义两个域名,分别为a.test.com和b.test.com,在a.test.com/jsonp3.html页面定义一个函数，然后在远程b.test.com/remote3.js中传入数据进行调用 a.test.com/jsonp3.html 12345678910111213141516171819202122&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;&lt;/title&gt; &lt;script type=\"text/javascript\"&gt; // 得到航班信息查询结果后的回调函数 var flightHandler = function(data){ alert('你查询的航班结果是：票价 ' + data.price + ' 元，' + '余票 ' + data.tickets + ' 张。'); }; // 提供jsonp服务的url地址（不管是什么类型的地址，最终生成的返回值都是一段javascript代码） var url = \"http://b.test.com/remote3.js?callback=flightHandler\"; // 创建script标签，设置其属性 var script = document.createElement('script'); script.setAttribute('src', url); // 把script标签加入head，此时调用开始 document.getElementsByTagName('head')[0].appendChild(script); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; b.test.com/remote3.js里面的内容 12345flightHandler({ \"price\": 1780, \"tickets\": 5},); 访问http://a.test.com/jsonp3.html JSONP优缺点：JSONP 的优点是：它不像XMLHttpRequest对象实现的Ajax请求那样受到同源策略的限制；它的兼容性更好，在更加古老的浏览器中都可以运行。 JSONP 的缺点是：它只支持 GET 请求，而不支持 POST 请求等其他类型的 HTTP 请求 4.2 CORS介绍跨源资源共享 Cross-Origin Resource Sharing(CORS) 是一个新的 W3C 标准，它新增的一组HTTP首部字段，允许服务端其声明哪些源站有权限访问哪些资源。换言之，它允许浏览器向声明了 CORS 的跨域服务器，发出 XMLHttpReuest 请求，从而克服 Ajax 只能同源使用的限制。 另外，规范也要求对于非简单请求，浏览器必须首先使用 OPTION 方法发起一个预检请求(preflight request)，从而获知服务端是否允许该跨域请求，在服务器确定允许后，才发起实际的HTTP请求。对于简单请求、非简单请求以及预检请求的详细资料可以阅读HTTP访问控制（CORS） 。 HTTP 协议 Header 简析下面对 CORS 中新增的 HTTP 首部字段进行简析： Access-Control-Allow-Origin 响应首部中可以携带这个头部表示服务器允许哪些域可以访问该资源，其语法如下： Access-Control-Allow-Origin: &lt;origin&gt; | *其中，origin 参数的值指定了允许访问该资源的外域 URI。对于不需要携带身份凭证的请求，服务器可以指定该字段的值为通配符，表示允许来自所有域的请求。 Access-Control-Allow-Methods 该首部字段用于预检请求的响应，指明实际请求所允许使用的HTTP方法。其语法如下： Access-Control-Allow-Methods: &lt;method&gt;[, &lt;method&gt;]*Access-Control-Allow-Headers 该首部字段用于预检请求的响应。指明了实际请求中允许携带的首部字段。其语法如下： Access-Control-Allow-Headers: &lt;field-name&gt;[, &lt;field-name&gt;]*Access-Control-Max-Age 该首部字段用于预检请求的响应，指定了预检请求能够被缓存多久，其语法如下： Access-Control-Max-Age: &lt;delta-seconds&gt; Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。其语法如下： Access-Control-Allow-Credentials: true另外，如果要把 Cookie 发送到服务器，除了服务端要带上Access-Control-Allow-Credentials首部字段外，另一方面请求中也要带上withCredentials属性。 但是需要注意的是：如果需要在 Ajax 中设置和获取 Cookie，那么Access-Control-Allow-Origin首部字段不能设置为* ，必须设置为具体的 origin 源站。详细可阅读文章CORS 跨域 Cookie 的设置与获取 示例在a.test.com/test.html页面配置获取b.test.com/test2.html页面的内容,下图 &lt;img src=”http://b.test.com/test.png&quot; /&gt; 是为了证明html带有src标签都不受同源策略的限制 http://a.test.com/test.html 12345678910111213141516171819&lt;html&gt; &lt;head&gt; &lt;script&gt; var http = new XMLHttpRequest(); http.open(\"get\", \"http://b.test.com/test2.html\", true);if (http){ http.onload = function() { alert(http.responseText); } http.send(); } &lt;/script&gt; &lt;img src=\"http://b.test.com/test.png\" /&gt;&lt;/head&gt;&lt;body&gt; &lt;/body&gt;&lt;/html&gt; http://b.test.com/test2.html 1234567&lt;html&gt; &lt;head&gt; &lt;/head&gt; &lt;body&gt; hello &lt;/body&gt; &lt;/html&gt; 在b.test.com的服务端未配置跨域配置访问http://a.test.com/test.html结果 在b.test.com的nginx服务端配置跨域配置访问http://a.test.com/test.html 123456789101112131415161718192021server {listen 80 ;server_name b.test.com;index index.jsp;req_status server;proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_pass_header User-Agent;proxy_set_header X-Forwarded-Proto $scheme;access_log logs/access.log main_1; add_header Access-Control-Allow-Origin http://a.test.com;add_header Access-Control-Allow-Methods GET,POST,OPTIONS;add_header Access-Control-Allow-Credentials true;add_header Access-Control-Allow-Headers DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type;add_header Access-Control-Max-Age 1728000;location / {root html;}} 访问效果,可以正常访问 与 JSONP 的比较 JSONP 只能实现 GET 请求，而 CORS 支持所有类型的 HTTP 请求 使用 CORS ，开发者可以是使用普通的 XMLHttpRequest 发起请求和获取数据，比起 JSONP 有更好的错误处理虽然绝大多数现代的浏览器都已经支持 CORS，但是 CORS 的兼容性比不上 JSONP，一些比较老的浏览器只支持 JSONP 5.参考资料1.https://www.jianshu.com/p/f880878c1398 2.http://http//www.cnblogs.com/dowinning/archive/2012/04/19/json-jsonp-jquery.html 3.http://www.cnblogs.com/hustskyking/articles/ten-methods-cross-domain.html 4.http://www.cnblogs.com/chopper/archive/2012/03/24/2403945.html","link":"/2020/03/29/linux/Cross-domain-requests/"},{"title":"GlusterFS集群文件系统专题一(基础原理)","text":"(版权声明：本文为CSDN博主「刘爱贵」的原创文章。原文链接) 1.概述GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBand RDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。GlusterFS基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。 GlusterFS支持运行在任何标准IP网络上标准应用程序的标准客户端，如图2所示，用户可以在全局统一的命名空间中使用NFS/CIFS等标准协议来访问应用数据。GlusterFS使得用户可摆脱原有的独立、高成本的封闭存储系统，能够利用普通廉价的存储设备来部署可集中管理、横向扩展、虚拟化的存储池，存储容量可扩展至TB/PB级。GlusterFS主要特征如下： 扩展性和高性能 GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（Elastic Hash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。 高可用性 GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至是在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。GlusterFS没有设计自己的私有数据文件格式，而是采用操作系统中主流标准的磁盘文件系统（如EXT3、ZFS）来存储文件，因此数据可以使用各种标准工具进行复制和访问。 全局统一命名空间 全局统一命名空间将磁盘和内存资源聚集成一个单一的虚拟存储池，对上层用户和应用屏蔽了底层的物理硬件。存储资源可以根据需要在虚拟存储池中进行弹性扩展，比如扩容或收缩。当存储虚拟机映像时，存储的虚拟映像文件没有数量限制，成千虚拟机均通过单一挂载点进行数据共享。虚拟机I/O可在命名空间内的所有服务器上自动进行负载均衡，消除了SAN环境中经常发生的访问热点和性能瓶颈问题。 一致性哈希算法 GlusterFS采用弹性哈希算法在存储池中定位数据，而不是采用集中式或分布式元数据服务器索引。在其他的Scale-Out存储系统中，元数据服务器通常会导致I/O性能瓶颈和单点故障问题。GlusterFS中，所有在Scale-Out存储配置中的存储系统都可以智能地定位任意数据分片，不需要查看索引或者向其他服务器查询。这种设计机制完全并行化了数据访问，实现了真正的线性性能扩展。 弹性卷管理 数据储存在逻辑卷中，逻辑卷可以从虚拟化的物理存储池进行独立逻辑划分而得到。存储服务器可以在线进行增加和移除，不会导致应用中断。逻辑卷可以在所有配置服务器中增长和缩减，可以在不同服务器迁移进行容量均衡，或者增加和移除系统，这些操作都可在线进行。文件系统配置更改也可以实时在线进行并应用，从而可以适应工作负载条件变化或在线性能调优。 工作于标准协议 Gluster存储服务支持NFS, CIFS, HTTP, FTP以及Gluster原生协议，完全与POSIX标准兼容。现有应用程序不需要作任何修改或使用专用API，就可以对Gluster中的数据进行访问。这在公有云环境中部署Gluster时非常有用，Gluster对云服务提供商专用API进行抽象，然后提供标准POSIX接口。 2.设计目标GlusterFS的设计思想显著区别有现有并行/集群/分布式文件系统。如果GlusterFS在设计上没有本质性的突破，难以在与Lustre、PVFS2、Ceph等的竞争中占据优势，更别提与GPFS、StorNext、ISILON、IBRIX等具有多年技术沉淀和市场积累的商用文件系统竞争。其核心设计目标包括如下三个： 弹性存储系统（Elasticity） 存储系统具有弹性能力，意味着企业可以根据业务需要灵活地增加或缩减数据存储以及增删存储池中的资源，而不需要中断系统运行。GlusterFS设计目标之一就是弹性，允许动态增删数据卷、扩展或缩减数据卷、增删存储服务器等，不影响系统正常运行和业务服务。GlusterFS早期版本中弹性不足，部分管理工作需要中断服务，目前最新的3.1.X版本已经弹性十足，能够满足对存储系统弹性要求高的应用需求，尤其是对云存储服务系统而言意义更大。GlusterFS主要通过存储虚拟化技术和逻辑卷管理来实现这一设计目标。 线性横向扩展（Linear Scale-Out） 线性扩展对于存储系统而言是非常难以实现的，通常系统规模扩展与性能提升之间是LOG对数曲线关系，因为同时会产生相应负载而消耗了部分性能的提升。现在的很多并行/集群/分布式文件系统都具很高的扩展能力，Luster存储节点可以达到1000个以上，客户端数量能够达到25000以上，这个扩展能力是非常强大的，但是Lustre也不是线性扩展的。 纵向扩展（Scale-Up）旨在提高单个节点的存储容量或性能，往往存在理论上或物理上的各种限制，而无法满足存储需求。横向扩展（Scale-Out）通过增加存储节点来提升整个系统的容量或性能，这一扩展机制是目前的存储技术热点，能有效应对容量、性能等存储需求。目前的并行/集群/分布式文件系统大多都具备横向扩展能力。 GlusterFS是线性横向扩展架构，它通过横向扩展存储节点即可以获得线性的存储容量和性能的提升。因此，结合纵向扩展GlusterFS可以获得多维扩展能力，增加每个节点的磁盘可增加存储容量，增加存储节点可以提高性能，从而将更多磁盘、内存、I/O资源聚集成更大容量、更高性能的虚拟存储池。GlusterFS利用三种基本技术来获得线性横向扩展能力： 1)消除元数据服务 2)高效数据分布，获得扩展性和可靠性 3)通过完全分布式架构的并行化获得性能的最大化 可靠性（Reliability） 与GFS（Google File System）类似，GlusterFS可以构建在普通的服务器和存储设备之上，因此可靠性显得尤为关键。GlusterFS从设计之初就将可靠性纳入核心设计，采用了多种技术来实现这一设计目标。首先，它假设故障是正常事件，包括硬件、磁盘、网络故障以及管理员误操作造成的数据损坏等。GlusterFS设计支持自动复制和自动修复功能来保证数据可靠性，不需要管理员的干预。其次，GlusterFS利用了底层EXT3/ZFS等磁盘文件系统的日志功能来提供一定的数据可靠性，而没有自己重新发明轮子。再次，GlusterFS是无元数据服务器设计，不需要元数据的同步或者一致性维护，很大程度上降低了系统复杂性，不仅提高了性能，还大大提高了系统可靠性。 3.技术特点GlusterFS在技术实现上与传统存储系统或现有其他分布式文件系统有显著不同之处，主要体现在如下几个方面。 全软件实现（Software Only） GlusterFS认为存储是软件问题，不能够把用户局限于使用特定的供应商或硬件配置来解决。GlusterFS采用开放式设计，广泛支持工业标准的存储、网络和计算机设备，而非与定制化的专用硬件设备捆绑。对于商业客户，GlusterFS可以以虚拟装置的形式交付，也可以与虚拟机容器打包，或者是公有云中部署的映像。开源社区中，GlusterFS被大量部署在基于廉价闲置硬件的各种操作系统上，构成集中统一的虚拟存储资源池。简而言之，GlusterFS是开放的全软件实现，完全独立于硬件和操作系统。 完整的存储操作系统栈（Complete Storage Operating System Stack） GlusterFS不仅提供了一个分布式文件系统，而且还提供了许多其他重要的分布式功能，比如分布式内存管理、I/O调度、软RAID和自我修复等。GlusterFS汲取了微内核架构的经验教训，借鉴了GNU/Hurd操作系统的设计思想，在用户空间实现了完整的存储操作系统栈。 用户空间实现（User Space） 与传统的文件系统不同，GlusterFS在用户空间实现，这使得其安装和升级特别简便。另外，这也极大降低了普通用户基于源码修改GlusterFS的门槛，仅仅需要通用的C程序设计技能，而不需要特别的内核编程经验。 模块化堆栈式架构（Modular Stackable Architecture） GlusterFS采用模块化、堆栈式的架构，可通过灵活的配置支持高度定制化的应用环境，比如大文件存储、海量小文件存储、云存储、多传输协议应用等。每个功能以模块形式实现，然后以积木方式进行简单的组合，即可实现复杂的功能。比如，Replicate模块可实现RAID1，Stripe模块可实现RAID0，通过两者的组合可实现RAID10和RAID01，同时获得高性能和高可靠性。 原始数据格式存储（Data Stored in Native Formats） GlusterFS以原始数据格式（如EXT3、EXT4、XFS、ZFS）储存数据，并实现多种数据自动修复机制。因此，系统极具弹性，即使离线情形下文件也可以通过其他标准工具进行访问。如果用户需要从GlusterFS中迁移数据，不需要作任何修改仍然可以完全使用这些数据。 无元数据服务设计（No Metadata with the Elastic Hash Algorithm） 对Scale-Out存储系统而言，最大的挑战之一就是记录数据逻辑与物理位置的映像关系，即数据元数据，可能还包括诸如属性和访问权限等信息。传统分布式存储系统使用集中式或分布式元数据服务来维护元数据，集中式元数据服务会导致单点故障和性能瓶颈问题，而分布式元数据服务存在性能负载和元数据同步一致性问题。特别是对于海量小文件的应用，元数据问题是个非常大的挑战。 GlusterFS独特地采用无元数据服务的设计，取而代之使用算法来定位文件，元数据和数据没有分离而是一起存储。集群中的所有存储系统服务器都可以智能地对文件数据分片进行定位，仅仅根据文件名和路径并运用算法即可，而不需要查询索引或者其他服务器。这使得数据访问完全并行化，从而实现真正的线性性能扩展。无元数据服务器极大提高了GlusterFS的性能、可靠性和稳定性。 4.总体架构与设计 GlusterFS总体架构与组成部分如图2所示，它主要由存储服务器（Brick Server）、客户端以及NFS/Samba存储网关组成。不难发现，GlusterFS架构中没有元数据服务器组件，这是其最大的设计这点，对于提升整个系统的性能、可靠性和稳定性都有着决定性的意义。GlusterFS支持TCP/IP和InfiniBand RDMA高速网络互联，客户端可通过原生Glusterfs协议访问数据，其他没有运行GlusterFS客户端的终端可通过NFS/CIFS标准协议通过存储网关访问数据。 存储服务器主要提供基本的数据存储功能，最终的文件数据通过统一的调度策略分布在不同的存储服务器上。它们上面运行着Glusterfsd进行，负责处理来自其他组件的数据服务请求。如前所述，数据以原始格式直接存储在服务器的本地文件系统上，如EXT3、EXT4、XFS、ZFS等，运行服务时指定数据存储路径。多个存储服务器可以通过客户端或存储网关上的卷管理器组成集群，如Stripe（RAID0）、Replicate（RAID1）和DHT（分布式Hash）存储集群，也可利用嵌套组合构成更加复杂的集群，如RAID10。 由于没有了元数据服务器，客户端承担了更多的功能，包括数据卷管理、I/O调度、文件定位、数据缓存等功能。客户端上运行Glusterfs进程，它实际是Glusterfsd的符号链接，利用FUSE（File system in User Space）模块将GlusterFS挂载到本地文件系统之上，实现POSIX兼容的方式来访问系统数据。在最新的3.1.X版本中，客户端不再需要独立维护卷配置信息，改成自动从运行在网关上的glusterd弹性卷管理服务进行获取和更新，极大简化了卷管理。GlusterFS客户端负载相对传统分布式文件系统要高，包括CPU占用率和内存占用。 GlusterFS存储网关提供弹性卷管理和NFS/CIFS访问代理功能，其上运行Glusterd和Glusterfs进程，两者都是Glusterfsd符号链接。卷管理器负责逻辑卷的创建、删除、容量扩展与缩减、容量平滑等功能，并负责向客户端提供逻辑卷信息及主动更新通知功能等。GlusterFS 3.1.X实现了逻辑卷的弹性和自动化管理，不需要中断数据服务或上层应用业务。对于Windows客户端或没有安装GlusterFS的客户端，需要通过NFS/CIFS代理网关来访问，这时网关被配置成NFS或Samba服务器。相对原生客户端，网关在性能上要受到NFS/Samba的制约。 GlusterFS是模块化堆栈式的架构设计，如图3所示。模块称为Translator，是GlusterFS提供的一种强大机制，借助这种良好定义的接口可以高效简便地扩展文件系统的功能。服务端与客户端模块接口是兼容的，同一个translator可同时在两边加载。每个translator都是SO动态库，运行时根据配置动态加载。每个模块实现特定基本功能，GlusterFS中所有的功能都是通过translator实现，比如Cluster, Storage, Performance, Protocol, Features等，基本简单的模块可以通过堆栈式的组合来实现复杂的功能。这一设计思想借鉴了GNU/Hurd微内核的虚拟文件系统设计，可以把对外部系统的访问转换成目标系统的适当调用。大部分模块都运行在客户端，比如合成器、I/O调度器和性能优化等，服务端相对简单许多。客户端和存储服务器均有自己的存储栈，构成了一棵Translator功能树，应用了若干模块。模块化和堆栈式的架构设计，极大降低了系统设计复杂性，简化了系统的实现、升级以及系统维护。 5.弹性哈希算法对于分布式系统而言，元数据处理是决定系统扩展性、性能以及稳定性的关键。GlusterFS另辟蹊径，彻底摒弃了元数据服务，使用弹性哈希算法代替传统分布式文件系统中的集中或分布式元数据服务。这根本性解决了元数据这一难题，从而获得了接近线性的高扩展性，同时也提高了系统性能和可靠性。GlusterFS使用算法进行数据定位，集群中的任何服务器和客户端只需根据路径和文件名就可以对数据进行定位和读写访问。换句话说，GlusterFS不需要将元数据与数据进行分离，因为文件定位可独立并行化进行。GlusterFS中数据访问流程如下： 1、计算hash值，输入参数为文件路径和文件名； 2、根据hash值在集群中选择子卷（存储服务器），进行文件定位； 3、对所选择的子卷进行数据访问。 GlusterFS目前使用Davies-Meyer算法计算文件名hash值，获得一个32位整数。Davies-Meyer算法具有非常好的hash分布性，计算效率很高。假设逻辑卷中的存储服务器有N个，则32位整数空间被平均划分为N个连续子空间，每个空间分别映射到一个存储服务器。这样，计算得到的32位hash值就会被投射到一个存储服务器，即我们要选择的子卷。难道真是如此简单？现在让我们来考虑一下存储节点加入和删除、文件改名等情况，GlusterFS如何解决这些问题而具备弹性的呢？ 逻辑卷中加入一个新存储节点，如果不作其他任何处理，hash值映射空间将会发生变化，现有的文件目录可能会被重新定位到其他的存储服务器上，从而导致定位失败。解决问题的方法是对文件目录进行重新分布，把文件移动到正确的存储服务器上去，但这大大加重了系统负载，尤其是对于已经存储大量的数据的海量存储系统来说显然是不可行的。另一种方法是使用一致性哈希算法，修改新增节点及相邻节点的hash映射空间，仅需要移动相邻节点上的部分数据至新增节点，影响相对小了很多。然而，这又带来另外一个问题，即系统整体负载不均衡。GlusterFS没有采用上述两种方法，而是设计了更为弹性的算法。GlusterFS的哈希分布是以目录为基本单位的，文件的父目录利用扩展属性记录了子卷映射信息，其下面子文件目录在父目录所属存储服务器中进行分布。由于文件目录事先保存了分布信息，因此新增节点不会影响现有文件存储分布，它将从此后的新创建目录开始参与存储分布调度。这种设计，新增节点不需要移动任何文件，但是负载均衡没有平滑处理，老节点负载较重。GlusterFS在设计中考虑了这一问题，在新建文件时会优先考虑容量负载最轻的节点，在目标存储节点上创建文件链接直向真正存储文件的节点。另外，GlusterFS弹性卷管理工具可以在后台以人工方式来执行负载平滑，将进行文件移动和重新分布，此后所有存储服务器都会均会被调度。 GlusterFS目前对存储节点删除支持有限，还无法做到完全无人干预的程度。如果直接删除节点，那么所在存储服务器上的文件将无法浏览和访问，创建文件目录也会失败。当前人工解决方法有两个，一是将节点上的数据重新复制到GlusterFS中，二是使用新的节点来替换删除节点并保持原有数据。 如果一个文件被改名，显然hash算法将产生不同的值，非常可能会发生文件被定位到不同的存储服务器上，从而导致文件访问失败。采用数据移动的方法，对于大文件是很难在实时完成的。为了不影响性能和服务中断，GlusterFS采用了文件链接来解决文件重命名问题，在目标存储服务器上创建一个链接指向实际的存储服务器，访问时由系统解析并进行重定向。另外，后台同时进行文件迁移，成功后文件链接将被自动删除。对于文件移动也作类似处理，好处是前台操作可实时处理，物理数据迁移置于后台选择适当时机执行。 弹性哈希算法为文件分配逻辑卷，那么GlusterFS如何为逻辑卷分配物理卷呢？GlusterFS3.1.X实现了真正的弹性卷管理，如图4所示。存储卷是对底层硬件的抽象，可以根据需要进行扩容和缩减，以及在不同物理系统之间进行迁移。存储服务器可以在线增加和移除，并能在集群之间自动进行数据负载平衡，数据总是在线可用，没有应用中断。文件系统配置更新也可以在线执行，所作配置变动能够快速动态地在集群中传播，从而自动适应负载波动和性能调优。 弹性哈希算法本身并没有提供数据容错功能，GlusterFS使用镜像或复制来保证数据可用性，推荐使用镜像或3路复制。复制模式下，存储服务器使用同步写复制到其他的存储服务器，单个服务器故障完全对客户端透明。此外，GlusterFS没有对复制数量进行限制，读被分散到所有的镜像存储节点，可以提高读性能。弹性哈希算法分配文件到唯一的逻辑卷，而复制可以保证数据至少保存在两个不同存储节点，两者结合使得GlusterFS具备更高的弹性。 6.Translators如前所述，Translators是GlusterFS提供的一种强大文件系统功能扩展机制，这一设计思想借鉴于GNU/Hurd微内核操作系统。GlusterFS中所有的功能都通过Translator机制实现，运行时以动态库方式进行加载，服务端和客户端相互兼容。GlusterFS 3.1.X中，主要包括以下几类Translator： （1） Cluster：存储集群分布，目前有AFR, DHT, Stripe三种方式 （2） Debug：跟踪GlusterFS内部函数和系统调用 （3） Encryption：简单的数据加密实现 （4） Features：访问控制、锁、Mac兼容、静默、配额、只读、回收站等 （5） Mgmt：弹性卷管理 （6） Mount：FUSE接口实现 （7） Nfs：内部NFS服务器 （8） Performance：io-cache, io-threads, quick-read, read-ahead, stat-prefetch, sysmlink-cache, write-behind等性能优化 （9） Protocol：服务器和客户端协议实现 （10）Storage：底层文件系统POSIX接口实现 这里我们重点介绍一下Cluster Translators，它是实现GlusterFS集群存储的核心，它包括AFR（Automatic File Replication）、DHT（Distributed Hash Table）和Stripe三种类型。 AFR相当于RAID1，同一文件在多个存储节点上保留多份，主要用于实现高可用性以及数据自动修复。AFR所有子卷上具有相同的名字空间，查找文件时从第一个节点开始，直到搜索成功或最后节点搜索完毕。读数据时，AFR会把所有请求调度到所有存储节点，进行负载均衡以提高系统性能。写数据时，首先需要在所有锁服务器上对文件加锁，默认第一个节点为锁服务器，可以指定多个。然后，AFR以日志事件方式对所有服务器进行写数据操作，成功后删除日志并解锁。AFR会自动检测并修复同一文件的数据不一致性，它使用更改日志来确定好的数据副本。自动修复在文件目录首次访问时触发，如果是目录将在所有子卷上复制正确数据，如果文件不存则创建，文件信息不匹配则修复，日志指示更新则进行更新。 DHT即上面所介绍的弹性哈希算法，它采用hash方式进行数据分布，名字空间分布在所有节点上。查找文件时，通过弹性哈希算法进行，不依赖名字空间。但遍历文件目录时，则实现较为复杂和低效，需要搜索所有的存储节点。单一文件只会调度到唯一的存储节点，一旦文件被定位后，读写模式相对简单。DHT不具备容错能力，需要借助AFR实现高可用性, 如图5所示应用案例。 Stripe相当于RAID0，即分片存储，文件被划分成固定长度的数据分片以Round-Robin轮转方式存储在所有存储节点。Stripe所有存储节点组成完整的名字空间，查找文件时需要询问所有节点，这点非常低效。读写数据时，Stripe涉及全部分片存储节点，操作可以在多个节点之间并发执行，性能非常高。Stripe通常与AFR组合使用，构成RAID10/RAID01，同时获得高性能和高可用性，当然存储利用率会低于50%。 7. 设计讨论GlusterFS是一个具有高扩展性、高性能、高可用性、可横向扩展的弹性分布式文件系统，在架构设计上非常有特点，比如无元数据服务器设计、堆栈式架构等。然而，存储应用问题是很复杂的，GlusterFS也不可能满足所有的存储需求，设计实现上也一定有考虑不足之处，下面我们作简要分析。 元数据服务器 vs 元数据服务器 无元数据服务器设计的好处是没有单点故障和性能瓶颈问题，可提高系统扩展性、性能、可靠性和稳定性。对于海量小文件应用，这种设计能够有效解决元数据的难点问题。它的负面影响是，数据一致问题更加复杂，文件目录遍历操作效率低下，缺乏全局监控管理功能。同时也导致客户端承担了更多的职能，比如文件定位、名字空间缓存、逻辑卷视图维护等等，这些都增加了客户端的负载，占用相当的CPU和内存。 户空间 vs 内核空间 用户空间实现起来相对要简单许多，对开发者技能要求较低，运行相对安全。用户空间效率低，数据需要多次与内核空间交换，另外GlusterFS借助FUSE来实现标准文件系统接口，性能上又有所损耗。内核空间实现可以获得很高的数据吞吐量，缺点是实现和调试非常困难，程序出错经常会导致系统崩溃，安全性低。纵向扩展上，内核空间要优于用户空间，GlusterFS有横向扩展能力来弥补。 栈式 vs 非堆栈式 这有点像操作系统的微内核设计与单一内核设计之争。GlusterFS堆栈式设计思想源自GNU/Hurd微内核操作系统，具有很强的系统扩展能力，系统设计实现复杂性降低很多，基本功能模块的堆栈式组合就可以实现强大的功能。查看GlusterFS卷配置文件我们可以发现，translator功能树通常深达10层以上，一层一层进行调用，效率可见一斑。非堆栈式设计可看成类似Linux的单一内核设计，系统调用通过中断实现，非常高效。后者的问题是系统核心臃肿，实现和扩展复杂，出现问题调试困难。 始存储格式 vs 私有存储格式 usterFS使用原始格式存储文件或数据分片，可以直接使用各种标准的工具进行访问，数据互操作性好，迁移和数据管理非常方便。然而，数据安全成了问题，因为数据是以平凡的方式保存的，接触数据的人可以直接复制和查看。这对很多应用显然是不能接受的，比如云存储系统，用户特别关心数据安全，这也是影响公有云存储发展的一个重要原因。私有存储格式可以保证数据的安全性，即使泄露也是不可知的。GlusterFS要实现自己的私有格式，在设计实现和数据管理上相对复杂一些，也会对性能产生一定影响。 文件 vs 小文件 GlusterFS适合大文件还是小文件存储？弹性哈希算法和Stripe数据分布策略，移除了元数据依赖，优化了数据分布，提高数据访问并行性，能够大幅提高大文件存储的性能。对于小文件，无元数据服务设计解决了元数据的问题。但GlusterFS并没有在I/O方面作优化，在存储服务器底层文件系统上仍然是大量小文件，本地文件系统元数据访问是一个瓶颈，数据分布和并行性也无法充分发挥作用。因此，GlusterFS适合存储大文件，小文件性能较差，还存在很大优化空间。 高可用性 vs 存储利用率 GlusterFS使用复制技术来提供数据高可用性，复制数量没有限制，自动修复功能基于复制来实现。可用性与存储利用率是一个矛盾体，可用性高存储利用率就低，反之亦然。采用复制技术，存储利用率为1/复制数，镜像是50%，三路复制则只有33%。其实，可以有方法来同时提高可用性和存储利用率，比如RAID5的利用率是(n-1)/n，RAID6是(n-2)/n，而纠删码技术可以提供更高的存储利用率。但是，鱼和熊掌不可得兼，它们都会对性能产生较大影响。 另外，GlusterFS目前的代码实现不够好，系统不够稳定，BUGS数量相对还比较多。从其官方网站的部署情况来看，测试用户非常多，但是真正在生产环境中的应用较少，存储部署容量几TB－几十TB的占很大比率，数百TB－PB级案例非常少。这也可以从另一个方面说明，GlusterFS目前还不够稳定，需要更长的时间来检验。然而不可否认，GlusterFS是一个有着光明前景的集群文件系统，线性横向扩展能力使它具有天生的优势，尤其是对于云存储系统。 8.参考文献[1] Gluster: http://www.gluster.com/products/gluster-file-system-architecture-white-paper/ [2] Gluster: http://www.gluster.com/products/performance-in-a-gluster-system-white-paper/ [3] Gluster: http://gluster.com/community/documentation/index.php/Main_Page [4] GlusterFS-Design: http://edwyseguru.wordpress.com/2010/06/11/glusterfs-design/ [5] GlusterFS users: http://www.gluster.org/gluster-users/ [6] GlusterFS sources: http://download.gluster.com/pub/gluster/glusterfs/3.1/","link":"/2020/02/25/linux/GlusterFS%20Topic%202(Basic%20principles)/"},{"title":"GlusterFS集群文件系统专题二(主要术语)","text":"1.Trusted Storage Pool 一堆存储节点的集合 通过一个节点“邀请”其他节点创建，这里叫probe 成员可以动态加入，动态删除添加命令如下：node1# gluster peer probe node2删除命令如下：node1# gluster peer detach node3 2.Bricks Brick是一个节点和一个导出目录的集合，e.g. node1:/brick1 Brick是底层的RAID或磁盘经XFS或ext4文件系统格式化而来，所以继承了文件系统的限制 每个节点上的brick数是不限的 理想的状况是，一个集群的所有Brick大小都一样 3.Volumes Volume是brick的逻辑组合 创建时命名来识别 Volume是一个可挂载的目录 每个节点上的brick数是不变的,e.g.mount –t glusterfs www.std.com:test /mnt/gls 一个节点上的不同brick可以属于不同的卷 支持如下种类：a) 分布式卷b) 条带卷c) 复制卷d) 分布式复制卷e) 条带复制卷f) 分布式条带复制卷 3.1 分布式卷 文件分布存在不同的brick里 目录在每个brick里都可见 单个brick失效会带来数据丢失 无需额外元数据服务器 gluster是没有元数据服务器的，它定位文件和寻址都是通过哈希算法，这里使用的叫Davies-Meyer hash algorithm，可寻址空间为2^32次方，即0-4294967296，例如这里有四个节点，那么0-1073741824为node1的可寻址空间，1073741825-214748348为node2的可寻址空间，以此类推。访问一个文件时，通过文件名计算出一个地址，例如2142011129，属于1073741825-214748348，则将它存在node2中。 分布式卷内部的hash分布如下： 分布式卷的读写如下图所示: 3.2 复制卷 同步复制所有的目录和文件 节点故障时保持数据高可用 事务性操作，保持一致性 有changelog 副本数任意定 复制卷的读写如下图所示： 3.3条带卷 文件切分成一个个的chunk，存放于不同的brick上 只建议在非常大的文件时使用（比硬盘大小还大） Brick故障会导致数据丢失，建议和复制卷同时使用 Chunks are files with holes – this helps in maintaining offset consistency 3.3条带复制卷 数据将进行切片，切片在复本卷内进行复制，在不同卷间进行分布 3.3分布式复制卷 最常见的一种模式 读操作可以做到负载均衡 复本卷的组成依赖于指定brick的顺序，brick必须为复本数K的N倍,brick列表将以K个为一组，形成N个复本卷 3.4分布式条带复制卷 bricks数量为stripe个数N，和repl个数M的积N*M的整数倍 exp1 exp2 exp3 exp4组成一个分布卷，exp1和exp2组成一个stripe卷，exp3和exp4组成另一个stripe卷，1和2，3和4互为复本卷，exp4-exp8组成另一个分布卷 4.其他术语 Client:挂载了GFS卷的设备 Extended Attributes:xattr:是一个文件系统的特性,其支持用户或程序关联文件/目录和元数据。 FUSE:Filesystem Userspace,是一个可加载的内核模块，其支持非特权用户创建自己的文件系统而不需要修改内核代码,通过在用户空间运行文件系统的代码通过FUSE代码与内核进行桥接 Geo-Replication：异地备份，提供了一种持续，异步，增量数据备份策略，可以通过局域网，广域网，英特网来进行 GFID:GFS卷中的每个文件或目录都有一个唯一的128位的数据相关联，其用于模拟inode Namespace:每个Gluster卷都导出单个ns作为POSIX的挂载点 Node:一个拥有若干brick的设备 RDMA:远程直接内存访问,支持不通过双方的OS进行直接内存访问 RRDNS:round robin DNS是一种通过DNS轮转返回不同的设备以进行负载均衡的方法 Self-heal:用于后台运行检测复本卷中文件和目录的不一致性并解决这些不一致 Split-brain:脑裂 Volfile:glusterfs进程的配置文件,通常位于/var/lib/glusterd/vols/volname Volume:一组bricks的逻辑集合","link":"/2020/02/28/linux/GlusterFS%20Topic%202(Main%20terms)/"},{"title":"ntp-server服务端部署和配置","text":"1.前言Atomic Clock: 现在计算时间最准确的是使用 原子震荡周期 所计算的物理时钟(Atomic Clock),因此也被定义为标准时间(International Atomic Time) UTC(coordinated Universal Time): 协和标准时间 就是利用 Atomic Clock 为基准定义出来的正确时间（又称世界统一时间，世界标准时间，国际协调时间）。 硬件时钟: 硬件时钟是指嵌在主板上的特殊的电路, 它的存在就是平时我们关机之后还可以计算时间的原因 系统时钟: 就是操作系统的kernel所用来计算时间的时钟. 它从1970年1月1日00:00:00 UTC时间到目前为止秒数总和的值 NTP（Network Time Protocol 网络时间协议）是一个用于同步计算机时钟的网络协议。它可以使计算机与其他服务器或时钟源进行时间同步，进行高精度的时间校正。 2.系统与软件版本2.1 系统版本CentOS6.5 x86_64 2.2 ntpd软件版本ntp-4.2.8p10.tar.gz 2.3 下载地址官方下载地址：http://support.ntp.org/bin/view/Main/SoftwareDownloads 3.编译安装3.1 安装依赖1yum install gcc gcc-c++ openssl-devel libstdc++* libcap* 3.2 备份相关目录123456789cp -ar /etc/ntp /etc/ntp.bakcp /etc/ntp.conf /etc/ntp.conf.bakcp /etc/init.d/ntpd /etc/init.d/ntpd.bakcp /etc/sysconfig/ntpd /etc/sysconfig/ntpd.bakcp /etc/sysconfig/ntpdate /etc/sysconfig/ntpdate.bak 3.3 卸载原来的ntp软件1yum erase ntp ntpdate 3.4 编译ntp1234567#创建/var/lib/ntp目录install -v -m710 -o ntp -g ntp -d /var/lib/ntp#解压tar -zxvf ntp-4.2.8p10.tar.gzcd ntp-4.2.8p10./configure --prefix=/usr --bindir=/usr/sbin --sysconfdir=/etc --enable-linuxcaps --with-lineeditlibs=readline --docdir=/usr/share/doc/ntp-4.2.8p10 --enable-all-clocks --enable-parse-clocks --enable-clockctlmake &amp;&amp; make install 编译参数解释： 1234567--bindir指定二进制文件的安装位置--enable-linuxcaps --enable-clockctl ntpd用ntp普通用户运行，使时钟控制功能不受root用户控制--enable-all-clocks --enable-parse-clocks 允许所有可解析的时钟--with-lineeditlibs=readline This switch enables Readline support for ntpdc and ntpq programs. （允许ntpdc和ntpq命令可以以readline模式运行） 3.5 创建配置文件123456789cp /etc/init.d/ntpd.bak /etc/init.d/ntpdcp /etc/sysconfig/ntpd.bak /etc/sysconfig/ntpdcp /etc/sysconfig/ntpdate.bak /etc/sysconfig/ntpdatemv /etc/ntp.bak /etc/ntpcp /etc/ntp.conf.bak /etc/ntp.conf 3.6 配置主配置文件/etc/ntp.confntp.conf主要参数详解: ntp.conf里主要可以使用如下几个命令：restrict，server，driftfile，keys 其中server是设定上级时间服务器用的，而restrict是设定哪台服务器可以和ntp server进行时间同步，具有什么样的权限。driftfile是用来指定记录时间差异的文件，keys是用来指定认证key文件的（这里不用）。 先来看restrict的格式为：restrict [客户端IP] mask [netmask_IP] [parameter] 客户端IP，就是都是哪几台服务器要和这台ntp server进行同步的ip地址最后的parameter可以有如下几个参数：ignore：拒绝连接到ntp servernomodiy：可以连接到ntp server，但是不能对ntp server进行时间上的修改noquery：不提供对ntp server查询时间，也就是拒绝和ntp server进行时间同步notrust：对没有认证的客户端不提供服务 notrap ：不提供trap远端登陆：拒绝为匹配的主机提供模式 6 控制消息陷阱服务。陷阱服务是 ntpdq 控制消息协议的子系统，用于远程事件日志记录程序。 nopeer ：用于阻止主机尝试与服务器对等，并允许欺诈性服务器控制时钟 kod ： 访问违规时发送 KoD 包。kod技术可以阻止“Kiss of Death”包（一种Dos攻击）对服务器的破坏，使用此参数开启该功能。 restrict default nomodify notrap noquery此项设置的含义是不允许其他计算机修改或查询配置在本机linux系统上的NTP服务。其中default表示所有IP. restrict 127.0.0.1开放本机内部回环网络接口，以便于在本地对NTP服务进行监控及配置。 #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap允许192.168.1.0网络段的NTP客户端使用本NTP服务器进行网络校进，但不允许它们修改本机的NTP服务配置。如想开放此项功能需要根据实际网络情况修改网络段及掩码，并把注释符”#”去掉。 server 0.pool.ntp.org 指定上层的NTP服务器。linux系统中默认指定Internet上的时间服务器池中的时间服务器作为上层NTP服务器。NTP服务器池pool.ntp.org中拥有三百多台自愿加入其中的公共NTP服务器。如果想更准确的校时也可以自己指定上层NTP服务器。 ‚server命令的格式是：server [IP or hostname] [prefer]其中[IP or hostname]为上级时间服务器的IP或者域名，主机名：可以是192.168.12.177形式，或者time.nist.gov再或者ntpserver这样的形式。后面的[prefer]参数是可选的，加上prefer后，ntp server和上级时间服务器同步时会优先先访问加了prefer这行的进行同步。 iburst：当一个运程NTP服务器不可用时，向它发送一系列的并发包进行检测。 server 127.127.1.0fudge 127.127.1.0 stratum 10127.127.1.0是一个特殊的地址，代表本机的系统时钟。fudge是指定本地时间源的层号，数字越大，优先级越低。所以当有外部时间源时会优先使用外部时间源。 driftfile /var/lib/ntp/drift指定记录与上层NTP服务器联系时所花费时间的文件，指定了用来保存系统时钟频率偏差的文件, ntpd程序使用它来自动地补偿时钟的自然漂移， 从而使时钟即使在切断了外来时源的情况下，仍能保持相当的准确度。 还可以在/etc/ntp.conf文件中还可以进行如下设置peer 192.168.16.100 #设置IP地址为192.168.16.100的NTP服务器可以与本机的NTP服务器相互进行网络校时broadcast 224.0.1.1 #224.0.1.1是多播网址,设置该NTP服务器可对所有能访问到的网段进行多播broadcast 192.168.1.255 #设定该NTP服务器可对子网192.168.1.0/24中的所有计算机定期广播正确的时间 为了让本机的NTP服务器能够到指定的时间源那进行同步，还必须修改/etc/ntp/step-tickers文件,在该文件中把所用的上层NTP服务器的IP地址或域名加入即可。 为了每天与NTP服务器保持时间同步，可以将命令写入到cron中。 附一份ntp server的主配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).#我们每一个system clock的频率都有小小的误差,这个就是为什么机器运行一段时间后会不精确. NTP会自动来监测我们时钟的误差值并予以调整.#但问题是这是一个冗长的过程,所以它会把记录下来的误差先写入driftfile.这样即使你重新开机以后之前的计算结果也就不会丢失了driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.# 允许所有客户端使用这个服务restrict default nomodify#restrict default kod nomodify notrap nopeer noquery#restrict -6 default kod nomodify notrap nopeer noquery# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.#restrict 127.0.0.1 #restrict -6 ::1server pool.ntp.org iburstserver 0.pool.ntp.org iburstserver 1.pool.ntp.org iburst# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# 本机同步远程的ntp的服务器域名# perfer:表示优先级最高# burst ：当一个运程NTP服务器可用时，向它发送一系列的并发包进行检测。# iburst ：当一个运程NTP服务器不可用时，向它发送一系列的并发包进行检测#NTP Server和其自身保持同步，如果在/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端。fudge 127.127.1.1 stratum 10#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography.keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats#设置ntp日志的path statsdir /var/log/ntp/ ##设置ntp日志文件 logfile /var/log/ntp/ntp.log##watch ntpq -p##参数说明：#remote: 它指的就是本地机器所连接的远程NTP服务器#refid: 它指的是给远程服务器(e.g. 193.60.199.75)提供时间同步的服务器#st: 远程服务器的层级别（stratum）. 由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端. 所以服务器从高到低级别可以设定为1-16. 为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的.#t: 未知#when: 我个人把它理解为一个计时器用来告诉我们还有多久本地机器就需要和远程服务器进行一次时间同步#poll: 本地机和远程服务器多少时间进行一次同步(单位为秒). 在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围.之后poll值会逐渐增大,同步的频率也就会相应减小#reach: 这是一个八进制值,用来测试能否和服务器连接.每成功连接一次它的值就会增加#delay: 从本地机发送同步要求到服务器的round trip time#offset: 这是个最关键的值, 它告诉了我们本地机和服务器之间的时间差别. offset越接近于0,我们就和服务器的时间越接近#jitter: 这是一个用来做统计的值. 它统计了在特定个连续的连接数里offset的分布情况. 简单地说这个数值的绝对值越小我们和服务器的时间就越精确##tinker panic 0#server 0.pool.ntp.org prefer#server 1.pool.ntp.org iburst 4.启动ntp服务1234567service ntpd startservice ntpd statuschkconfig --level 345 ntpd onchkconfig --list ntpd 5.查看ntp与上层ntp的状态 参数说明： 目前正在使用的上层NTP。+已连线,可提供时间更新的候补服务器 remote：响应这个请求的NTP服务器的主机名或IP。 refid：remote端NTP服务器的上级NTP的IP。 st：remote远程服务器的级别.由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端.所以服务器从高到低级别可以设定为1-16.为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的. t：类型。“u”表示单播。其他值有：本地、多播和广播。 when：几秒钟前曾做过时间同步更新。 poll：本地主机和远程服务器多少时间进行一次同步(单位为秒).在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围，之后poll值会逐渐增大,同步的频率也就会相应减小 reach：这是一个八进制值,用来测试能否和服务器连接.每成功连接一次它的值就会增加。377表示100%成功。 delay：从本地发送同步要求到服务器的往返时间。 offset：本地主机通过NTP时钟同步与所同步时间源的时间偏移量，单位为毫秒（ms）。offset越接近于0,主机和ntp服务器的时间越接近。 jitter 这是一个用来做统计的值.它统计了在特定个连续的连接数里offset的分布情况.简单地说这个数值的绝对值越小，主机的时间就越精确，以ms为单位. 6.配置/etc/sysconfig/ntpd文件ntp服务，默认只会同步系统时间。如果想要让ntp同时同步硬件时间，可以设置/etc/sysconfig/ntpd文件，在/etc/sysconfig/ntpd文件中，添加 SYNC_HWCLOCK=yes 这样，就可以让硬件时间与系统时间一起同步。 允许BIOS与系统时间同步，也可以通过hwclock -w 命令 SYNC_HWCLOCK=yes 7.客户端配置123crontab -e#设置定时同步ntp 服务的时间00 01 * * * root /usr/sbin/ntpdate 10.201.3.90; /sbin/hwclock -w","link":"/2020/02/23/linux/NTP-SERVER/"},{"title":"YUM源部署和使用","text":"1.前言为什么需要内部yum源呢，有可能是业务内部的服务器对外是不通了，居于一些安全方面的考虑。内部yum源又有什么好处呢，第一，速度快；第二，内网可控，外网有问题也不影响内网包的下载和安装等。 2.部署2.1 创建yum仓库目录1234mkdir -p /data/yum_data/centos/6/os/x86_64/mkdir -p /data/yum_data/centos/6/extras/x86_64/mkdir -p /data/yum_data/centos/6/updates/x86_64/mkdir -p /data/yum_data/epel/6/x86_64/ 2.2 镜像同步公网yum源上游yum源必须要支持rsync协议，否则不能使用rsync进行同步CentOS官方标准源：rsync://mirrors.ustc.edu.cn/centos/epel源：rsync://mirrors.ustc.edu.cn/epel/同步命令： 12345rsync -auvzP --bwlimit=1000 rsync://rsync.mirrors.ustc.edu.cn/centos/6/os/x86_64/ /data/yum_data/centos/6/os/x86_64/rsync -auvzP --bwlimit=1000 rsync://rsync.mirrors.ustc.edu.cn/centos/6/extras/x86_64/ /data/yum_data/centos/6/extras/x86_64/rsync -auvzP --bwlimit=1000 rsync://rsync.mirrors.ustc.edu.cn/centos/6/updates/x86_64/ /data/yum_data/centos/6/updates/x86_64/# epel源 rsync -auvzP --bwlimit=1000 --exclude=debug rsync://rsync.mirrors.ustc.edu.cn/epel/6/x86_64/ /data/yum_data/epel/6/x86_64/ 2.3 提供yum服务部署tengine，server的配置如下： 1234567891011121314#/usr/local/nginx/conf.d/iso.confserver {listen 80;server_name localhost;access_log logs/iso.log main;location / {autoindex on;root /data/yum_data/;}error_page 500 502 503 504 /50x.html;location = /50x.html {root html; }} 2.4 客户端配置配置客户端的yum源文件，配置在/etc/yum.repos.d/下面：/etc/yum.repos.d/CentOS-Base.repo 配置： 12345678910111213141516171819[base107]name=CentOS-$releasever - Basebaseurl=http://192.168.31.107/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6#released updates[updates107]name=CentOS-$releasever - Updatesbaseurl=http://192.168.31.107/centos/$releasever/updates/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6#additional packages that may be useful[extras107]name=CentOS-$releasever - Extrasbaseurl=http://192.168.31.107/centos/$releasever/extras/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 /etc/yum.repos.d/epel.repo 配置： 123456[epel107]name=Extra Packages for Enterprise Linux 6 - $basearchbaseurl=http://192.168.31.107/epel/6/$basearchfailovermethod=prioritygpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 2.5 客户端的命令使用12yum clean allyum install telnet -y 3.参考文档 http://www.zyops.com/autodeploy-yum","link":"/2020/01/14/linux/YUM/"},{"title":"Tengine 基础原理和基本使用","text":"1.Tengine简介2.Tengine特性3.Tengine工作原理和用途 - 3.1 Tengine处理HTTP请求流程 - 3.2 Tengine的模块化 - 3.3 Tengine进程模型 - 3.4 Tengine事件模型 - 3.5 Tengine信号 - 3.6 Tengine定时器4.Tengine安装部署 - 4.1 安装依赖 - 4.2 编译5.Tengine 文件目录结构6.Tengine 配置文件详解 - 6.1 tengine主配置文件 - 6.2 状态检测配置文件 - 6.3 tengine常用配置7.Tengine 安全防护 - 7.1 禁止web服务IP直接访问 - 7.2 连接和请求数限制 - 7.3 访问全站IP黑名单限制 - 7.4 下载防盗链配置8.Tengine 重要模块 - 8.1 upstream负载均衡模块 - 8.2 rewrite重写模块9.location 在匹配中的优先级10.Tengine root 和 alias 的区别11.Tengine TCP转发配置12.Tengine 常用维护脚本或命令13.参考文档 1.Tengine简介Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。从2011年12月开始，Tengine成为一个开源项目，Tengine团队在积极地开发和维护着它。Tengine团队的核心成员来自于淘宝、搜狗等互联网企业。Tengine是社区合作的成果，我们欢迎大家参与其中，贡献自己的力量。 2.Tengine特性 继承Nginx-1.8.1的所有特性，兼容Nginx的配置 动态模块加载（DSO）支持。加入一个模块不再需要重新编译整个Tengine 支持HTTP/2协议，HTTP/2模块替代SPDY模块 流式上传到HTTP后端服务器或FastCGI服务器，大量减少机器的I/O压力 更加强大的负载均衡能力，包括一致性hash模块、会话保持模块，还可以对后端的服务器进行主动健康检查，根据服务器状态自动上线下线，以及动态解析upstream中出现的域名 输入过滤器机制支持，通过使用这种机制Web应用防火墙的编写更为方便 支持设置proxy、memcached、fastcgi、scgi、uwsgi在后端失败时的重试次数 动态脚本语言Lua支持。扩展功能非常高效简单 支持按指定关键字(域名，url等)收集Tengine运行状态 组合多个CSS、JavaScript文件的访问请求变成一个请求 自动去除空白字符和注释从而减小页面的体积 自动根据CPU数目设置进程个数和绑定CPU亲缘性 监控系统的负载和资源占用从而对系统进行保护 显示对运维人员更友好的出错信息，便于定位出错机器 更强大的防攻击（访问速度限制）模块 更方便的命令行参数，如列出编译的模块列表、支持的指令等 可以根据访问文件类型设置过期时间 3.Tengine工作原理和用途3.1 Tengine处理HTTP请求流程http请求是典型的请求-响应类型的的网络协议。http是文件协议，所以我们在分析请求行与请求头，以及输出响应行与响应头，往往是一行一行的进行处理。通常在一个连接建立好后，读取一行数据，分析出请求行中包含的method、uri、http_version信息。然后再一行一行处理请求头，并根据请求method与请求头的信息来决定是否有请求体以及请求体的长度，然后再去读取请求体。得到请求后，我们处理请求产生需要输出的数据，然后再生成响应行，响应头以及响应体。在将响应发送给客户端之后，一个完整的请求就处理完了。处理流程图： 3.2 Tengine的模块化Nginx由内核和模块组成。Nginx的模块从结构上分为核心模块、基础模块和第三方模块： 核心模块：HTTP模块、EVENT模块和MAIL模块 基础模块：HTTP Access模块、HTTP FastCGI模块、HTTP Proxy模块和HTTP Rewrite模块， 第三方模块：HTTP Upstream Request Hash模块、Notice模块和HTTP Access Key模块。用户根据自己的需要开发的模块都属于第三方模块。正是有了这么多模块的支撑，Nginx的功能才会如此强大，Nginx的模块从功能上分为如下三类。 Handlers（处理器模块）：此类模块直接处理请求，并进行输出内容和修改headers信息等操作。Handlers处理器模块一般只能有一个。 Filters （过滤器模块）：此类模块主要对其他处理器模块输出的内容进行修改操作，最后由Nginx输出。 Proxies （代理类模块）：此类模块是Nginx的HTTP Upstream之类的模块，这些模块主要与后端一些服务比如FastCGI等进行交互，实现服务代理和负载均衡等功能。Nginx模块常规的HTTP请求和响应的过程：Nginx本身做的工作实际很少，当它接到一个HTTP请求时，它仅仅是通过查找配置文件将此次请求映射到一个location block，而此location中所配置的各个指令则会启动不同的模块去完成工作，因此模块可以看做Nginx真正的劳动工作者。通常一个location中的指令会涉及一个handler模块和多个filter模块（当然，多个location可以复用同一个模块）。handler模块负责处理请求，完成响应内容的生成，而filter模块对响应内容进行处理。 3.3 Tengine进程模型Nginx默认采用多进程工作方式，Nginx启动后，会运行一个master进程和多个worker进程。其中master充当整个进程组与用户的交互接口，同时对进程进行监护，管理worker进程来实现重启服务、平滑升级、更换日志文件、配置文件实时生效等功能。worker用来处理基本的网络事件，worker之间是平等的，他们共同竞争来处理来自客户端的请求。nginx的进程模型如图所示： 请求到来时，如何分配均分worker进程来处理他们？ 在创建master进程时，先建立需要监听的socket（listenfd），然后从master进程中fork()出多个worker进程，如此一来每个worker进程多可以监听用户请求的socket。一般来说，当一个连接进来后，所有在Worker都会收到通知，但是只有一个进程可以接受这个连接请求，其它的都失败，这是所谓的惊群现象。nginx提供了一个accept_mutex（互斥锁），有了这把锁之后，同一时刻，就只会有一个进程在accpet连接，这样就不会有惊群问题了。 先打开accept_mutex选项，只有获得了accept_mutex的进程才会去添加accept事件。nginx使用一个叫ngx_accept_disabled的变量来控制是否去竞争accept_mutex锁。ngx_accept_disabled = nginx单进程的所有连接总数 / 8 -空闲连接数量，当ngx_accept_disabled大于0时，不会去尝试获取accept_mutex锁，ngx_accept_disable越大，于是让出的机会就越多，这样其它进程获取锁的机会也就越大。不去accept，每个worker进程的连接数就控制下来了，其它进程的连接池就会得到利用，这样，nginx就控制了多进程间连接的平衡。 每个worker进程都有一个独立的连接池，连接池的大小是worker_connections。这里的连接池里面保存的其实不是真实的连接，它只是一个worker_connections大小的一个ngx_connection_t结构的数组。并且，nginx会通过一个链表free_connections来保存所有的空闲ngx_connection_t，每次获取一个连接时，就从空闲连接链表中获取一个，用完后，再放回空闲连接链表里面。一个nginx能建立的最大连接数，应该是worker_connections 乘以 worker_processes。当然，这里说的是最大连接数，对于HTTP请求本地资源来说，能够支持的最大并发数量是worker_connections 乘以 worker_processes，而如果是HTTP作为反向代理来说，最大并发数量应该是worker_connections 乘以 worker_processes/2。因为作为反向代理服务器，每个并发会建立与客户端的连接和与后端服务的连接，会占用两个连接。 3.4 Tengine事件模型对于一个基本的web服务器来说，事件通常有三种类型，网络事件、信号的处理。Nginx每个worker里面只有一个主线程，多少个worker就能处理多少个并发，何来高并发呢？请求流程：首先，请求到来，要建立连接，然后再接收数据，接收数据后，再发送数据。具体到系统底层，就是读写事件，而当读写事件没有准备好时，不可操作。apache的常用工作方式：每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。这对操作系统来说，是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的cpu开销很大，自然性能就上不去了，而这些开销完全是没有意义的。 Nginx采用异步非阻塞的方式来支持用户请求。Nginx支持select/poll/epoll/kqueue等事件模型。拿epoll为例，当事件没准备好时，放到epoll里面，事件准备好了，我们就去读写，当读写返回EAGAIN时，我们将它再次加入到epoll里面。这样，只要有事件准备好了，我们就去处理它，当事件都没有完全准备好时，就在epoll里面等着。这样，我们就可以并发处理大量的并发了，当然，这里的并发请求，是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（上下文切换），更多的并发数，只是会占用更多的内存而已。 现在的网络服务器基本都采用这种方式，这也是nginx性能高效的主要原因。 推荐设置worker的个数为cpu的核数，因为更多的worker数，只会导致进程来竞争cpu资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了cpu亲缘性的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来cache的失效。 3.5 Tengine信号对nginx来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重新进行一次。如果nginx正在等待事件（epoll_wait时），如果程序收到信号，在信号处理函数处理完后，epoll_wait会返回错误，然后程序可再次进入epoll_wait调用。 3.6 Tengine定时器由于epoll_wait等函数在调用的时候是可以设置一个超时时间的，所以nginx借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入epoll_wait前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出epoll_wait的超时时间后进入epoll_wait。所以，当没有事件产生，也没有中断信号时，epoll_wait会超时，也就是说，定时器事件到了。这时，nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。由此可以看出，当我们写nginx代码时，在处理网络事件的回调函数时，通常做的第一个事情就是判断超时，然后再去处理网络事件 4.Tengine安装部署4.1 安装依赖1yum -y install zlib zlib-devel openssl openssl-devel pcre pcre-devel gcc gcc-c++ autoconf automake jemalloc jemalloc-devel 4.2 编译123456789101112tar -zxvf tengine-2.2.0.tar.gzcd tengine-2.2.0./configure --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-http_concat_module --with-jemalloc --with-http_v2_module --with-http_secure_link_modulemake &amp;&amp; make install#编译选项解析./configure --with-http_stub_status_module #添加监控状态配置 --with-http_ssl_module #支持https --with-http_gzip_static_module #支持静态文件预压缩 --with-http_concat_module #用于合并多个文件在一个响应报文中 --with-jemalloc #支持jemalloc内存管理 --with-http_v2_module #支持http_v2 --with-http_secure_link_module #防下载盗链支持 5.Tengine 文件目录结构 12345678910111213|---certs ：存放域名证书位置，自建的规范目录|---client_body_temp：如果客户端POST一个比较大的文件，长度超过了nginx缓冲区的大小，需要把这个文件的部分或者全部内容暂存到client_body_temp目录下的临时文件|---conf：主配置文件存放目录|---conf.d：存放虚拟主机的配置文件，自建规范目录|---fastcgi_temp：对于来自 FastCGI Server 的 Response，Nginx 将其缓冲到内存中，然后依次发送到客户端浏览器。缓冲区的大小由 fastcgi_buffers 和 fastcgi_buffer_size 两个值控制，fastcgi_buffers 控制 nginx 最多创建 8 个大小为 4K 的缓冲区，而 fastcgi_buffer_size 则是处理 Response 时第一个缓冲区的大小，不包含在前者中，超出部分存在这个目录|---html：静态文件默认存放位置|---include：存放编译代码头文件目录|---logs：默认存放日志目录|---modules:存放模块目录|---proxy_temp:后端返回数据的临时存放目录|---sbin:nginx二进制文件存放目录|---scgi_temp:客户端可能会向服务器端请求大量的数据,服务器端收到的请求报文中的body中可能会有很多的数据,而这些数据都会存放内存中,倘若有很多的用户并发发出请求,服务器端内存无法存放，因此就会把数据临时存放在磁盘上的这些临时文件内|---uwsgi_temp:代理服务器时缓存文件的存放路径 6.Tengine 配置文件详解6.1 tengine主配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108#/usr/local/nginx/conf/nginx.confuser nobody nobody; # 指定运行的用户和用户组 worker_processes auto; # 指定要开启的进程数，一般为CPU的核心数或两倍 worker_cpu_affinity auto; # cpu亲和性 worker_rlimit_nofile 102400; # 这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数(ulimit -n)与nginx进程数相除，但是nginx 分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致 pid logs/nginx.pid; #进程号存放位置events { use epoll; # 指定nginx的工作模式 worker_connections 65535; # 定义每个进程的最大连接数 }http { req_status_zone server \"$server_addr:$server_port\" 10M; # 根据变量值分别统计Tengine的运行状况 include mime.types; # 服务器识别的文件类型 default_type application/octet-stream; # 文件类型未定义时默认识别为二进制流 check_shm_size 20M; #后端检测内存区大小 sendfile on; # 高速传输文件 tcp_nopush on; # 防止网络阻塞 tcp_nodelay on; # 防止网络阻塞 server_tokens off; # 不返回nginx的版本信息 server_info off; #返回错误页面是不返回服务器信息 keepalive_timeout 20s; # 一个keepalive 连接被闲置以后还能保持多久打开状态 keepalive_requests 1000; # 这是一个客户端可以通过一个keepalive连接的请求次数。缺省值是100，但是也可以调得很高，而且这对于测试负载生成工具从哪里使用一个客户端发送这么多请求非常有用 gzip on; # 开启gzip压缩输出 gzip_min_length 1024; # 最小压缩文件大小 gzip_buffers 16 8k; # 压缩缓冲区 gzip_comp_level 6; # 压缩等级 gzip_proxied any; # Nginx作为反向代理的时候启用,根据某些请求和应答来决定是否在对代理请求的应答启用gzip压缩，是否压缩取决于请求头中的“Via”字段启用压缩,这里是如果header头中包含 \"Expires\" 头信息 gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript application/javascript; #压缩类型，默认就已经包含text/html，所以下面就不用再写了 # 写上去也不会有问题，但是会有一个warn gzip_vary on; # 让前端的缓存服务器缓存经过gzip压缩的页面，和http头有关系，加个vary头，给代理服务器用的，有的浏览器支持压缩，有的不支持，所以避免浪费不支持的也压缩，所以根据客户端的HTTP头来判断，是否需要压缩 fastcgi_intercept_errors on; # 这个指令指定是否传递4xx和5xx错误信息到客户端，或者允许nginx使用error_page处理错误信息 fastcgi_connect_timeout 75s; # 指定连接到后端FastCGI的超时时间 fastcgi_send_timeout 300s; # 指定向FastCGI传送请求的超时时间，这个值是已经完成两次握手后向FastCGI传送请求的超时时间。 fastcgi_read_timeout 300s; # 指定接收FastCGI应答的超时时间，这个值是已经完成两次握手后接收FastCGI应答的超时时间。 fastcgi_buffer_size 16k; # 用于指定读取FastCGI应答第一部分需要用多大的缓冲区，这个值表示将使用1个64KB的缓冲区读取应答的第一部分（应答头），可以设置为fastcgi_buffers选项指定的缓冲区大小 fastcgi_buffers 4 16k; # 指定本地需要用多少和多大的缓冲区来缓冲FastCGI的应答请求。如果一个PHP脚本所产生的页面大小为256KB,那么会为其分配4个64KB的缓冲区来缓存,如果页面大小大于256KB，那么大于256K # B的部分会缓存到 fastcgi_busy_buffers_size 32k; # 繁忙模式下的缓冲区大小，默认值是fastcgi_buffers的两倍 #open_file_cache max=10240 inactive=20s; # 最多缓存多少个文件，缓存多少时间 #open_file_cache_valid 30s; # 多少时间检查一次，如果发现20s内没有用过一次的删除 #open_file_cache_min_uses 1; # 在20S中没有使用到这个配置的次数的话就删除 client_body_timeout 90s; # 设置客户端请求头读取超时的时间 client_max_body_size 20m; # 用来设置允许客户端请求的最大的单个文件字节数 client_body_buffer_size 1m; # 设置缓存区的最大值 client_header_buffer_size 128k; # 用于指定来自客户端请求头的header buffer大小 large_client_header_buffers 256 16k; # 用来指定客户端请求中较大的消息头和缓存最大数量和大小 proxy_buffer_size 16k; # 设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 8 32k; # proxy_buffers缓冲区，网页平均在32k以下的设置 proxy_busy_buffers_size 64k; # 高负荷下缓冲大小（proxy_buffers*2） proxy_connect_timeout 300s; # nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 300s; # 后端服务器数据回传时间(代理发送超时) proxy_read_timeout 300s; # 连接成功后，后端服务器响应时间(代理接收超时) #proxy_temp_path /usr/local/nginx/proxy_temp; # 缓存临时目录。后端的响应并不直接返回客户端，而是先写到一个临时文件中，然后被rename一下当做缓存放在 proxy_cache_path #proxy_cache_path ... ： 设置缓存目录，目录里的文件名是 cache_key 的MD5值。 #levels=1:2 keys_zone=cache_one:50m表示采用2级目录结构，Web缓存区名称为cache_one，内存缓存空间大小为100MB，这个缓冲zone可以被多次使用。文件系统上看到的缓存文件名类似于 /usr/local/nginx-1.6/proxy_cache/c/29/b7f54b2df7773722d382f4809d65029c inactive=2d max_size=2g表示2天没有被访问的内容自动清除，硬盘最大缓存空间为2GB，超过这个大学将清除最近最少使用的数据。 #proxy_cache_path /usr/local/nginx/proxy_cache levels=1:2 keys_zone=cache_one:100m inactive=2d max_size=2g;这两句需要搭配location里面的指令使用，默认不开启 map $upstream_addr $short_address { ~^\\d+\\.\\d+\\.(.*) ''; } add_header X-from $short_address$1; proxy_set_header Host $host; proxy_set_header X-User-IP $clientRealIp; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass_header User-Agent; proxy_set_header X-Forwarded-Proto $scheme; map $http_x_forwarded_for $clientRealIp { \"\" $remote_addr; ~^(?P&lt;firstAddr&gt;[0-9\\.]+),?.*$ $firstAddr; } log_format main '{ \"@timestamp\": \"$time_iso8601\", ' '\"remote_addr\": \"$remote_addr\", ' '\"upstream_addr\": \"$upstream_addr\", ' '\"server_addr\": \"$server_addr\", ' '\"http_host\": \"$http_host\",' '\"request_time\": $request_time, ' '\"upstream_response_time\": $upstream_response_time, ' '\"request_uri\": \"$request_uri\", ' '\"status\": \"$status\", ' '\"request\": \"$request\", ' '\"request_method\": \"$request_method\", ' '\"http_referer\": \"$http_referer\", ' '\"body_bytes_sent\": $body_bytes_sent, ' '\"http_x_forwarded_for\": \"$http_x_forwarded_for\", ' '\"request_length\": $request_length, ' '\"http_user_agent\": \"$http_user_agent\", ' '\"scheme\": \"$scheme\",' '\"uri\": \"$uri\",' '\"clientRealIp\": \"$clientRealIp\"}'; include /usr/local/nginx/conf.d/*.conf;} 6.2 状态检测配置文件12345678910111213141516171819202122#/usr/local/nginx/conf.d/admin.confserver { listen 8000; location ~ ^/(phpfpm_status|ping)$ { fastcgi_pass 127.0.0.1:9000; include fastcgi.conf; } location = /nginx_status { stub_status on; access_log off; } location = /nginx_status_detail { req_status_show; } location = /nstatus { check_status; access_log off; #allow IP; #deny all; }} 6.3 tengine常用配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#/usr/local/nginx/conf.d/default.conf(一些常用的配置功能，供参考)server { listen 80 ; listen 443 ssl; server_name www.test1.com www.test2.com; server_tag off; req_status server; # 开启https的配置,证书存放位置需要规范 # ssl on; ssl_certificate /usr/local/nginx/certs/test.crt; ssl_certificate_key /usr/local/nginx/certs/test.key; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; #配置本监听端口下的日志 access_log logs/host.access.log main; location /example { root html; index index.html index.htm; # 最多 5 个排队， 由于每秒处理 10 个请求 + 5个排队，你一秒最多发送 15 个请求过来，再多就直接返回 503 错误给你了 # limit_req zone=ConnLimitZone burst=5 nodelay; # 用于获取用户真实IP，传给后端的服务器获取 proxy_set_header Host $host; proxy_set_header X-real-ip $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 开启与后端tomcat长连接需下面两个指令,tomcat也需要配置长连接参数(tomcat在server.xml配置标签 &lt;Connector keepAliveTimeout=\"20000\" maxKeepAliveRequests=\"3000\" /&gt;) proxy_http_version 1.1; proxy_set_header Connection \"\"; # 反向代理指令，记得proxy_pass 带uri和不带uri的区别 #proxy_pass http://bakend/ ; #与负载均衡结合使用 proxy_pass http://192.168.31.11:7111/ ; # 用户访问局部限制指令,匹配上的立马匹配，不继续下面的匹配动作 allow 192.168.31.69; deny all; deny 192.168.31.70; allow all; # 传输数据限速到20k,为了测试很明显就把此值调的很低 limit_rate 20k; # 数据合并发送 # 合并js css,需要开发合并文件，暂时使用不了 concat on; concat_max_files 20; concat_unique off; concat_delimiter \"\\r\\n\"; concat_ignore_file_error on; #配置web缓存,与主配置文件配置的缓存目录结合使用。缓存也就是将js、css、image等静态文件从tomcat缓存到nginx指定的缓存目录下，既可以减轻tomcat负担，也可以加快访问速度， #但这样缓存及时清理成为了一个问题，所以需要 ngx_cache_purge 这个模块来在过期时间未到之前，手动清理缓存。（有篇文章http://quenlang.blog.51cto.com/4813803/1570671， #对比使用缓存、不使用缓存、使用动静分离三种情况下，高并发性能比较。使用代理缓存功能性能会高出很多倍） #反向代理到哪台机器 proxy_pass http://192.168.31.11; #不显示重定向信息 proxy_redirect off; #设置请求头,获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #引用前面定义的缓存区 cache_one proxy_cache cache_one; #增加头部记录缓存命中率 add_header Nginx-Cache $upstream_cache_status; #proxy_cache_valid ： 为不同的响应状态码设置不同的缓存时间，比如200、302等正常结果可以缓存的时间长点，而404、500等缓存时间设置短一些，这个时间到了文件就会过期，而不论是否刚被访问过。 proxy_cache_valid 200 304 301 302 8h; proxy_cache_valid 404 1m; proxy_cache_valid any 2d; #定义cache_key proxy_cache_key $host$uri$is_args$args; #返回给客户端的浏览器缓存失效时间,指的是静态资源的缓存 expires 30d; } #清除缓存。下面配置的proxy_cache_purge指令用于方便的清除缓存，但必须按装第三方的 ngx_cache_purge 模块才能使用，项目地址：https://github.com/FRiCKLE/ngx_cache_purge/ 。 location ~ /purge(/.*) { #设置只允许指定的IP或IP段才可以清除URL缓存。 allow 127.0.0.1; allow 192.168.31.0/24; deny all; proxy_cache_purge cache_one $host$1$is_args$args; error_page 405 =200 /purge$1; } # 图片防盗链配置 location ~* \\.(gif|jpg|png|bmp)$ { root html; valid_referers none blocked *.feidee.com server_names ~\\.google\\. ~\\.baidu\\.; if ($invalid_referer) { return 403; #rewrite ^/ http://xxxx/403.jpg; } } location /realip { #如果是作为代理，下面三句才会把客户端的IP带到真正的服务器上面 proxy_set_header Host $host; proxy_set_header X-real-ip $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://192.168.31.11/realip/; } error_page 404 /404.html; # redirect server error pages to the static page /50x.html error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } 7.Tengine 安全防护7.1 禁止web服务IP直接访问123456789101112131415161718#/usr/local/nginx/conf.d/defend.conf(禁止IP直接访问配置，默认开启)#Nginx配置禁止IP直接访问，如果是直接使用IP访问，直接返回客户端403错误#注意:对于https，必须指定证书，否则连域名访问也会一起禁止server { listen 80 default; #server_name _; return 403; }server { listen 443 default; #server_name _; ssl_certificate /usr/local/nginx/certs/test.crt; ssl_certificate_key /usr/local/nginx/certs/test.key; return 403; } 7.2 连接和请求数限制123456789101112131415161718192021222324252627282930313233343536#/usr/local/nginx/conf.d/limit_rate.conf(连接和请求数限制脚本，默认关闭)# 这里取得原始用户的IP地址map $http_x_forwarded_for $clientRealIp { \"\" $remote_addr; ~^(?P&lt;firstAddr&gt;[0-9\\.]+),?.*$ $firstAddr;}# 设置白名单geo $clientRealIp $whiteiplist { default 1; 192.168.31.241 1; 192.168.31.251 0; 192.168.31.236 0; 192.168.31.0/24 0; }map $whiteiplist $limit { 1 $clientRealIp; 0 \"\"; }# server 段配置# if ( $whiteiplist = 0 ){# return 403;# }# if ( $http_user_agent ~ Dalvik.* ){# return 403;# }# 针对原始用户 IP 地址做限制limit_conn_zone $limit zone=TotalConnLimitZone:20m ;limit_conn TotalConnLimitZone 50; # 每个IP最大连接数limit_conn_log_level notice;# 针对原始用户 IP 地址做限制limit_req_zone $limit zone=ConnLimitZone:20m rate=20r/s; #每个地址每秒只能请求同一个URL20次limit_req zone=ConnLimitZone burst=10 nodelay; # 如果开启此条规则，burst=10的限制将会在nginx全局生效 一共有10块令牌,并且每秒钟只新增1块令牌,10块令牌发完后 多出来的那些请求就会返回503.limit_req_log_level notice; 7.3 访问全站IP黑名单限制1234567#/usr/local/nginx/conf.d/globle_blacklistip.conf(访问全站限制IP)#配置全局的对网站访问的限制，对特定的url进行访问限制可以在location里面进行配置#############globle setting ##########deny 192.168.31.69;deny 192.168.31.70;#deny 192.168.31.0/24 ;allow all; 7.4 下载防盗链配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#/usr/local/nginx/conf.d/secure_link.conf(下载防盗链配置)nginx 配置如下:server { listen 80; server_name www.testchao.com #本机域名 access_log ../logs/ www.testchao.com.access.log main; #日志 location / { root html; #md5_hash[,expiration_time] MD5哈希值和过期时间 secure_link $arg_st,$arg_e; #md5值对比结果,使用上面提供的uri、密钥、过期时间生成md5哈希值.如果它生成的md5哈希值与用户提交过来的哈希值一致，那么这个变量的值为1，否则为0 secure_link_md5 ttlsa.com$uri$arg_e; if ($secure_link = \"\") { return 403; } if ($secure_link = \"0\") { return 403; } } #php解析 location ~ \\.php$ { root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root/$fastcgi_script_name; include fastcgi_params; }}#下载页面php如下:#download.php&lt;html&gt;&lt;br&gt;nginx下载页面&lt;br&gt;&lt;a href=\"/test.php\" class=\"btn btn-danger go-link\" role=\"button\" target=\"_blank\" rel=\"nofollow\" _hover-ignore=\"1\"&gt;下载地址&lt;/a&gt;&lt;/html&gt;#test.php 生成连接页面&lt;?php # 作用：生成nginx secure link链接 # 站点：www.qingye.info # 作者：青叶 # 时间：2020-01-12$secret = 'qingye'; # 密钥 $path = '/web/nginx-1.4.2.tar.gz.jpg'; # 下载文件 # 下载到期时间,time是当前时间,300表示300秒,也就是说从现在到300秒之内文件不过期 $expire = time()+10;# 用文件路径、密钥、过期时间生成加密串 $md5 = base64_encode(md5($secret . $path . $expire .$_SERVER['REMOTE_ADDR'], true)); $md5 = strtr($md5, '+/', '-_'); $md5 = str_replace('=', '', $md5);# 加密后的下载地址$url = http://www.qingye.info/web/nginx-1.4.2.tar.gz.jpg?st='.$md5.'&amp;e='.$expire;#echo '&lt;a href=http://www.qingye.info/web/nginx-1.4.2.tar.gz.jpg?st='.$md5.'&amp;e='.$expire.'&gt;nginx-1.4.2&lt;/a&gt;';#echo '&lt;br&gt;http://www.qingye.info/web/nginx-1.4.2.tar.gz.jpg?st='.$md5.'&amp;e='.$expire;header(\"Location: $url\"); ?&gt; 8.Tengine 重要模块8.1 upstream负载均衡模块1234567891011121314151617181920#upstream模块详细例子upstream backend {#负载均衡算法，这个ip_hash根据IP来辨别，有时候因为访问都是从cdn或其他同一个IP过来，导致负载均衡发挥不好ip_hash;server 192.168.31.225:8080 weight 2;server 192.168.31.226:8080 weight=1 max_fails=2 fail_timeout=30s ;server 192.168.31.227:8080 backup; }server {location / {proxy_pass http://backend;proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; }#weight ： 轮询权值也是可以用在ip_hash的，默认值为1#max_fails ： 允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。#fail_timeout ： 有两层含义，一是在 30s 时间内最多容许 2 次失败；二是在经历了 2 次失败以后，30s时间内不分配请求到这台服务器。#backup ： 预留的备份机器。当其他所有的非backup机器出现故障的时候，才会请求backup机器，因此这台机器的压力最轻。（为什么我的1.6.3版本里配置backup启动nginx时说invalid parameter \"backup\"？）#max_conns： 限制同时连接到某台后端服务器的连接数，默认为0即无限制。因为queue指令是commercial，所以还是保持默认吧。#proxy_next_upstream ： 这个指令属于 http_proxy 模块的，指定后端返回什么样的异常响应时，使用另一个realserver 关于nginx使用负载均衡会话跟踪问题(ngx_http_upstream_session_sticky_module 模块，tenginx默认已经安装) 这个模块的作用是通过cookie黏贴的方式将来自同一个客户端（浏览器）的请求发送到同一个后端服务器上处理，这样一定程度上可以解决多个backend servers的session同步的问题 —— 因为不再需要同步， 而RR轮询模式(nginx的默认后端调度模式)必须要运维人员自己考虑session同步的实现。 语法：session_sticky [cookie=name] [domain=your_domain] [path=your_path] [maxage=time] [mode=insert|rewrite|prefix] [option=indirect] [maxidle=time] [maxlife=time] [fallback=on|off] [hash=plain|md5]默认值：session_sticky cookie=route mode=insert fallback=on上下文：upstream 说明: 本指令可以打开会话保持的功能，下面是具体的参数： cookie设置用来记录会话的cookie名称 domain设置cookie作用的域名，默认不设置 path设置cookie作用的URL路径，默认不设置 maxage设置cookie的生存期，默认不设置，即为session cookie，浏览器关闭即失效 mode设置cookie的模式: insert: 在回复中本模块通过Set-Cookie头直接插入相应名称的cookie。 prefix: 不会生成新的cookie，但会在响应的cookie值前面加上特定的前缀，当浏览器带着这个有特定标识的cookie再次请求时，模块在传给后端服务前先删除加入的前缀，后端服务拿到的还是原来的cookie值，这些动作对后端透明。如：”Cookie: NAME=SRV~VALUE”。 rewrite: 使用服务端标识覆盖后端设置的用于session sticky的cookie。如果后端服务在响应头中没有设置该cookie，则认为该请求不需要进行session sticky，使用这种模式，后端服务可以控制哪些请求需要sesstion sticky，哪些请求不需要。 option 设置用于session sticky的cookie的选项，可设置成indirect或direct。indirect不会将session sticky的cookie传送给后端服务，该cookie对后端应用完全透明。direct则与indirect相反。 maxidle设置session cookie的最长空闲的超时时间 maxlife设置session cookie的最长生存期 fallback设置是否重试其他机器，当sticky的后端机器挂了以后，是否需要尝试其他机器 hash 设置cookie中server标识是用明文还是使用md5值，默认使用md5maxage是cookie的生存期。不设置时，浏览器或App关闭后就失效。下次启动时，又会随机分配后端服务器。所以如果希望该客户端的请求长期落在同一台后端服务器上，可以设置maxage。 hash不论是明文还是hash值，都有固定的数目。因为hash是server的标识，所以有多少个server，就有等同数量的hash值。 一些例外： 同一客户端的请求，有可能落在不同的后端服务器上## 如果客户端启动时同时发起多个请求。由于这些请求都没带cookie，所以服务器会随机选择后端服务器，返回不同的cookie。当这些请求中的最后一个请求返回时，客户端的cookie才会稳定下来，值以最后返回的cookie为准。 cookie不一定生效## 由于cookie最初由服务器端下发，如果客户端禁用cookie，则cookie不会生效。 客户端可能不带cookie## Android客户端发送请求时，一般不会带上所有的cookie，需要明确指定哪些cookie会带上。如果希望用sticky做负载均衡，请对Android开发说加上cookie。 注意事项： cookie名称不要和业务使用的cookie重名。Sticky默认的cookie名称是route，可以改成任何值。但切记，不可以与业务中使用的cookie重名。客户端发的第一个请求是不带cookie的。服务器下发的cookie，在客户端下一次请求时才能生效另外内置的 ip_hash 也可以实现根据客户端IP来分发请求，但它很容易造成负载不均衡的情况，而如果nginx前面有CDN网络或者来自同一局域网的访问，它接收的客户端IP是一样的，容易造成负载不均衡现象。 这个模块并不合适不支持 Cookie 或手动禁用了cookie的浏览器，此时默认session_sticky就会切换成RR。它不能与ip_hash同时使用。 123456789101112upstream backend { check interval=3000 rise=2 fall=5 timeout=1000 type=http; check_http_send \"HEAD / HTTP/1.0\\r\\n\\r\\n\"; check_http_expect_alive http_2xx http_3xx; server 192.168.31.226:8080 weight=1; server 192.168.31.227:8080 weight=1; session_sticky;#在insert + indirect模式或者prefix模式下需要配置session_sticky_hide_cookie#这种模式不会将保持会话使用的cookie传给后端服务，让保持会话的cookie对后端透明#session_sticky cookie=uid fallback=on mode=insert option=indirect hash=plain;#配置起来超级简单，一般来说一个session_sticky指令就够了。} 负载均衡其它调度方案 轮询（默认） ： 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某台服务器宕机，故障系统被自动剔除，使用户访问不受影响。Weight 指定轮询权值，Weight值越大，分配到的访问机率越高，主要用于后端每个服务器性能不均的情况下。 ip_hash ： 每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题。当然如果这个节点不可用了，会发到下个节点，而此时没有session同步的话就注销掉了。 least_conn ： 请求被发送到当前活跃连接最少的realserver上。会考虑weight的值。 url_hash ： 此方法按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身是不支持url_hash的，如果需要使用这种调度算法，必须安装Nginx 的hash软件包 nginx_upstream_hash 。 fair ： 这是比上面两个更加智能的负载均衡算法。此种算法可以依据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身是不支持fair的，如果需要使用这种调度算法，必须下载Nginx的 upstream_fair 模块 8.2 rewrite重写模块rewrite模块rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在server{},location{},if{}中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如 http://seanlook.com/a/we/index.php?id=1&amp;u=str 只对/a/we/index.php重写。语法rewrite regex replacement [flag]; flag标志位 1234last : 相当于Apache的[L]标记，表示完成rewritebreak : 停止执行当前虚拟主机的后续rewrite指令集redirect : 返回302临时重定向，地址栏会显示跳转后的地址permanent : 返回301永久重定向，地址栏会显示跳转后的地址 if指令与全局变量,if判断指令: 12345678语法为if(condition){...}，对给定的条件condition进行判断。如果为真，大括号内的rewrite指令将被执行，if条件(conditon)可以是如下任何内容：当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false直接比较变量和内容时，使用=或!=~正则表达式匹配，~*不区分大小写的匹配，!~区分大小写的不匹配-f和!-f用来判断是否存在文件-d和!-d用来判断是否存在目录-e和!-e用来判断是否存在文件或目录-x和!-x用来判断文件是否可执行 12345678910111213141516171819202122232425if ($http_user_agent ~ MSIE) { rewrite ^(.*)$ /msie/$1 break;} //如果UA包含\"MSIE\"，rewrite请求到/msid/目录下if ($http_cookie ~* \"id=([^;]+)(?:;|$)\") { set $id $1; } //如果cookie匹配正则，设置变量$id等于正则引用部分if ($request_method = POST) { return 405;} //如果提交方法为POST，则返回状态405（Method not allowed）。return不能返回301,302if ($slow) { limit_rate 10k;} //限速，$slow可以通过 set 指令设置if (!-f $request_filename){ break; proxy_pass http://127.0.0.1;} //如果请求的文件名不存在，则反向代理到localhost 。这里的break也是停止rewrite检查if ($args ~ post=140){ rewrite ^ http://example.com/ permanent;} //如果query string中包含\"post=140\"，永久重定向到example.comlocation ~* \\.(gif|jpg|png|swf|flv)$ { valid_referers none blocked www.jefflei.com www.leizhenfang.com; if ($invalid_referer) { return 404; } //防盗链} 下面是可以用作if判断的全局变量: 123456789101112131415161718192021$args ： #这个变量等于请求行中的参数，同$query_string$content_length ： 请求头中的Content-length字段。$content_type ： 请求头中的Content-Type字段。$document_root ： 当前请求在root指令中指定的值。$host ： 请求主机头字段，否则为服务器名称。$http_user_agent ： 客户端agent信息$http_cookie ： 客户端cookie信息$limit_rate ： 这个变量可以限制连接速率。$request_method ： 客户端请求的动作，通常为GET或POST。$remote_addr ： 客户端的IP地址。$remote_port ： 客户端的端口。$remote_user ： 已经经过Auth Basic Module验证的用户名。$request_filename ： 当前请求的文件路径，由root或alias指令与URI请求生成。$scheme ： HTTP方法（如http，https）。$server_protocol ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。$server_addr ： 服务器地址，在完成一次系统调用后可以确定这个值。$server_name ： 服务器名称。$server_port ： 请求到达服务器的端口号。$request_uri ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。$uri ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。$document_uri ： 与$uri相同。 常用正则: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253. ： 匹配除换行符以外的任意字符? ： 重复0次或1次+ ： 重复1次或更多次* ： 重复0次或更多次\\d ：匹配数字^ ： 匹配字符串的开始$ ： 匹配字符串的介绍{n} ： 重复n次{n,} ： 重复n次或更多次[c] ： 匹配单个字符c[a-z] ： 匹配a-z小写字母的任意一个#小括号()之间匹配的内容，可以在后面通过$1来引用，$2表示的是前面第二个()里的内容。正则里面容易让人困惑的是\\转义特殊字符。break语法：break默认值：none使用字段：server, location, if完成当前设置的重写规则，停止执行其他的重写规则。return语法：return code默认值：none使用字段：server, location, if停止处理并为客户端返回状态码。非标准的444状态码将关闭连接，不发送任何响应头。可以使用的状态码有：204，400，402-406，408，410, 411, 413, 416与500-504。如果状态码附带文字段落，该文本将被放置在响应主体。相反，如果状态码后面是一个URL，该URL将成为location头补值。没有状态码的URL将被视为一个302状态码。rewrite语法：rewrite regex replacement flag默认值：none使用字段：server, location, if按照相关的正则表达式与字符串修改URI，指令按照在配置文件中出现的顺序执行。可以在重写指令后面添加标记。注意：如果替换的字符串以http://开头，请求将被重定向，并且不再执行多余的rewrite指令。尾部的标记(flag)可以是以下的值：last - 停止处理重写模块指令，之后搜索location与更改后的URI匹配。break - 完成重写指令。redirect - 返回302临时重定向，如果替换字段用http://开头则被使用。permanent - 返回301永久重定向。rewrite_log语法：rewrite_log on | off默认值：rewrite_log off使用字段：server, location, if变量：无启用时将在error log中记录notice级别的重写日志。set语法：set variable value默认值：none使用字段：server, location, if为给定的变量设置一个特定值。uninitialized_variable_warn语法：uninitialized_variable_warn on|off默认值：uninitialized_variable_warn on使用字段：http, server, location, if控制是否记录未初始化变量的警告信息。 例子: 1234567891011121314151617#last和break实现URL重写，浏览器地址栏URL地址不变location ~ ^/best/ {rewrite ^/best/(.*) /test/$1 break;proxy_pass http://www.taob.com}#更换域名server{server_name www.taob.com;rewrite ^/(.*)$ http://www.tb.com/$1 permanent;}#或者server {server_name www.tb.com www.taob.com;if ($host != 'www.tb.com')rewrite ^/(.*)$ http://www.tb.com/$1 permanent;} 9.location 在匹配中的优先级配置文件示例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051location = / {# 仅仅匹配请求 /[ configuration A ]}location / {# 匹配所有以 / 开头的请求。# 但是如果有更长的同类型的表达式，则选择更长的表达式。# 如果有正则表达式可以匹配，则优先匹配正则表达式。[ configuration B ]}location /documents/ {# 匹配所有以 /documents/ 开头的请求。# 但是如果有更长的同类型的表达式，则选择更长的表达式。# 如果有正则表达式可以匹配，则优先匹配正则表达式。[ configuration C ]}location ^~ /images/ {# 匹配所有以 /images/ 开头的表达式，如果匹配成功，则停止匹配查找。# 所以，即便有符合的正则表达式location，也不会被使用[ configuration D ]}location ~* \\.(gif|jpg|jpeg)$ {# 匹配所有以 gif jpg jpeg结尾的请求。# 但是 以 /images/开头的请求，将使用 Configuration D[ configuration E ]} 匹配结果： URL 匹配结果 原因 / configuration A =优先级最高，匹配到结束 /index.html configuration B 路径匹配 /documents/document.html configuration C 第二个匹配到，往后继续匹配，发现第三个匹配最确 /images/1.gif configuration D 同时匹配第二个，第四个，和第五个。但是由于优先级问题 /documents/1.jpg configuration E 同时匹配第二个，第三个，和第五个。第五个是正则表达式 优先级如下: (location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ,* 正则顺序) &gt; (location 部分起始路径) &gt; (/) 注： ~ 和 ~都是正则匹配 其中 ~ 不区分大小写 ~ 区分大小写 10.Tengine root 和 alias 的区别nginx配置下有两个指定目录的执行，root和alias 例如有以下配置文件： 123location /img/ { alias /var/www/image/;} 若按照上述配置的话，则访问/img/目录里面的文件时，ningx会自动去/var/www/image/目录找文件 123location /img/ { root /var/www/image;} 若按照这种配置的话，则访问/img/目录下的文件时，nginx会去/var/www/image/img/目录下找文件。alias是一个目录别名的定义，root则是最上层目录的定义还有一个重要的区别是alias后面必须要用“/”结束，否则会找不到文件的。。。而root则可有可无 11.Tengine TCP转发配置123456789101112131415#nginx tcp 配置，这一段必须放在http上下文tcp {timeout 1d;proxy_read_timeout 10d;proxy_send_timeout 10d;proxy_connect_timeout 30;upstream api_server {server 192.168.31.212:389 weight=5 max_fails=1 fail_timeout=10s;}server {listen 1389;proxy_connect_timeout 1s;proxy_pass api_server; }} 12.Tengine 常用维护脚本或命令123456789101112131415161718192021222324252627282930313233343536373839404142#1、查看安装的模块/usr/local/nginx/sbin/nginx -m#2、检测配置文件语法/usr/local/nginx/sbin/nginx -t#3、启动tenginx/usr/local/nginx/sbin/nginx#4、配置文件重新加载/usr/local/nginx/sbin/nginx -s reload#5、关闭nginx/usr/local/nginx/sbin/nginx -s stop#6、日志备份脚本# /bin/bashlogs_path=\"/usr/local/nginx/logs/\"pid_path=\"/usr/local/nginx/logs/nginx.pid\"cut_path=\"/usr/local/nginx/logs/bak/\"[ -e $cut_path ] || mkdir -p $cut_path cd $logs_pathfor log_name in `ls *.log`;do mv ${logs_path}${log_name} ${cut_path}${log_name}_$(date +\"%Y-%m-%d\").logdoneif [[ -s $pid_path ]]; then kill -USR1 `cat ${pid_path}`fifind ${cut_path} -type f -name \"*.log\" -mtime +7 | xargs rm -f 13.参考文档 http://tengine.taobao.org/documentation_cn.html","link":"/2020/01/12/linux/Tengine/"},{"title":"rkhunter安装和使用","text":"1.前言 rkhunter 是一个内核级别的rootkit检测工具,下面以rkhunter-1.4.2介绍一下安装和使用 2.安装 123456789101112131415161718192021222324252627#!/bin/bash#program:# this program init server#history:# 2016/09/06 qingye first releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHrkhunter(){tar -zxvf rkhunter-1.4.2.tar.gz cd rkhunter-1.4.2 mkdir -p /opt/tools/rkhunter ./installer.sh --layout custom /opt/tools/rkhunter --install k=$? mv /bin/rkhunter /tmp ln -s /opt/tools/rkhunter/bin/rkhunter /bin/rkhunter if [ $? = 0 ];then echo -e \"rkhunter_install \\033[32m [ ok ] \\033[0m\" else echo -e \"rkhunter_install \\033[31m [ fail ] \\033[0m\"fi }main(){rkhunter}main 3.rkhunter产生的日志结合zabbix进行报警3.1 制定定时任务12#每天定时扫描一遍，会生成一个日志文件:/var/log/rkhunter.log08 3 * * * /bin/rkhunter --check --cronjob 产生的日志有个病毒汇总信息： 3.2 编写脚本，添加zabbix自定义监控就可以了，当有发现病毒时报警出来12345678#!/bin/bashresult=\"`cat /var/log/rkhunter.log | grep \"Possible rootkits\" | awk '{print $4}'`\"if [[ $result &gt; 0 ]]; then echo 1else echo 0fi`","link":"/2020/01/14/linux/rkhunter/"},{"title":"深刻理解Docker镜像大小","text":"1.docker镜像分析是否还记得第一个接触Docker的时候，你从Docker Hub下拉的那个镜像呢？在那个处女镜像的基础上，你运行了容器生涯的处女容器。镜像的基石作用已经很明显，在Docker的世界里，可以说是：No Image，No Container。 再进一步思考Docker镜像，大家可能很快就会联想到以下几类镜像： 1.系统级镜像:如Ubuntu镜像，CentOS镜像以及Debian容器等； 2.工具栈镜像:如Golang镜像，Flask镜像，Tomcat镜像等； 3.服务级镜像:如MySQL镜像，MongoDB镜像，RabbitMQ镜像等； 4.应用级镜像:如WordPress镜像，DockerRegistry镜像等。 镜像林林总总，想要运行Docker容器，必须要有Docker镜像；想要有Docker镜像，必须要先下载Docker镜像；既然涉及到下载Docker镜像，自然会存在Docker镜像存储。谈到Docker镜像存储，那我们首先来聊聊Docker镜像大小方面的知识。 以下将从三个角度来分析Docker镜像的大小问题：Dockerfile与镜像、联合文件系统以及镜像共享关系。 Dockerfile与镜像Dockerfile由多条指令构成，随着深入研究Dockerfile与镜像的关系，很快大家就会发现，Dockerfile中的每一条指令都会对应于Docker镜像中的一层。 继续以如下Dockerfile为例： 1234FROM ubuntu:14.04ADD run.sh /VOLUME /dataCMD [\"./run.sh\"] 通过docker build以上Dockerfile的时候，会在Ubuntu:14.04镜像基础上，添加三层独立的镜像，依次对应于三条不同的命令。镜像示意图如下： 有了Dockerfile与镜像关系的初步认识之后，我们再进一步联系到每一层镜像的大小。 不得不说，在层级化管理的Docker镜像中，有不少层大小都为0。那些镜像层大小不为0的情况，归根结底的原因是：构建Docker镜像时，对当前的文件系统造成了修改更新。而修改更新的情况主要有两种： 1.ADD或COPY命令:ADD或者COPY的作用是在docker build构建镜像时向容器中添加内容，只要内容添加成功，当前构建的那层镜像就是添加内容的大小，如以上命令ADD run.sh /，新构建的那层镜像大小为文件run.sh的大小。 2.RUN命令:RUN命令的作用是在当前空的镜像层内运行一条命令，倘若运行的命令需要更新磁盘文件，那么所有的更新内容都在存储在当前镜像层中。举例说明：RUN echo DaoCloud命令不涉及文件系统内容的修改，故命令运行完之后当前镜像层的大小为0；RUN wget http://abc.com/def.tar命令会将压缩包下载至当前目录下，因此当前这一层镜像的大小为:对文件系统内容的增量修改部分，即def.tar文件的大小。 2.联合文件系统Dockerfile中命令与镜像层一一对应，那么是否意味着docker build完毕之后，镜像的总大小＝每一层镜像的大小总和呢？答案是肯定的。依然以上图为例：如果ubuntu:14.04镜像的大小为200MB，而run.sh的大小为5MB，那么以上三层镜像从上到下，每层大小依次为0、0以及5MB，那么最终构建出的镜像大小的确为0+0+5+200=205MB。 虽然最终镜像的大小是每层镜像的累加，但是需要额外注意的是：Docker镜像的大小并不等于容器中文件系统内容的大小（不包括挂载文件，/proc、/sys等虚拟文件）。个中缘由，就和联合文件系统有很大的关系了。 首先来看一下这个简单的Dockerfile例子(假如在Dockerfile当前目录下有一个100MB的压缩文件compressed.tar): 1234FROM ubuntu:14.04ADD compressed.tar /RUN rm /compressed.tarADD compressed.tar / 1.FROM ubuntu:镜像ubuntu:14.04的大小为200MB； 2.ADD compressed.tar /: compressed.tar文件为100MB，因此当前镜像层的大小为100MB，镜像总大小为300MB； 3.RUN rm /compressed.tar:删除文件compressed.tar,此时的删除并不会删除下一层的compressed.tar文件，只会在当前层产生一个compressed.tar的删除标记，确保通过该层将看不到compressed.tar,因此当前镜像层的大小也为0，镜像总大小为300MB； 4.ADD compressed.tar /:compressed.tar文件为100MB，因此当前镜像层的大小为300MB+100MB，镜像总大小为400MB； 分析完毕之后，我们发现镜像的总大小为400MB，但是如果运行该镜像的话，我们很快可以发现在容器根目录下执行du -sh之后，显示的数值并非400MB，而是300MB左右。主要的原因还是：联合文件系统的性质保证了两个拥有compressed.tar文件的镜像层，仅仅会容器看到一个。同时这也说明了一个现状，当用户基于一个非常大，甚至好几个GB的镜像运行容器时，在容器内部查看根目录大小，发现竟然只有500MB不到，设置更小。 分析至此，有一点大家需要非常注意：镜像大小和容器大小有着本质的区别。 3.镜像共享关系Docker镜像说大不大，说小不小，但是一旦镜像的总数上来之后，岂不是对本地磁盘造成很大的存储压力？平均每个镜像500MB，岂不是100个镜像就需要准备50GB的存储空间？ 结果往往不是我们想象的那样，Docker在镜像复用方面设计得非常出色，大大节省镜像占用的磁盘空间。Docker镜像的复用主要体现在：多个不同的Docker镜像可以共享相同的镜像层。 假设本地镜像存储中只有一个ubuntu:14.04的镜像，我们以两个Dockerfile来说明镜像复用： 1234FROM ubuntu:14.04RUN apt-get updateFROM ubuntu:14.04ADD compressed.tar / 假设最终docker build构建出来的镜像名分别为image1和image2，由于两个Dockerfile均基于ubuntu:14.04，因此，image1和image2这两个镜像均复用了镜像ubuntu:14.04。 假设RUN apt-get update修改的文件系统内容为20MB，最终本地三个镜像的大小关系应该如下： ubuntu:14.04: 200MB image1:200MB(ubuntu:14.04)+20MB=220MB image2:200MB(ubuntu:14.04)+100MB=300MB 如果仅仅是单纯的累加三个镜像的大小，那结果应该是：200+220+300=720MB，但是由于镜像复用的存在，实际占用的磁盘空间大小是：200＋20+100=320MB，足足节省了400MB的磁盘空间。在此，足以证明镜像复用的巨大好处。 4.总结学习Docker的同时，往往有三部分内容是分不开的，那就是Dockerfile，Docker镜像与Docker容器，分析Docker镜像大小也是如此。Docker镜像的大小，貌似平淡无奇，却是优化镜像，容器磁盘限额必须要涉及的内容。 本系列将通过以下多篇文章来分析Docker镜像： 1.深刻理解 Docker 镜像大小 2.其实 docker commit 很简单 3.不得不说的 docker save 与 docker export 区别 4.为什么有些容器文件动不得 5.打破 MNT Namespace 的容器 VOLUME 5.参考文章作者：孙宏亮 来源：CSDN 原文：https://blog.csdn.net/shlazww/article/details/47375009?utm_source=copy 版权声明：本文为博主原创文章，转载请附上博文链接！","link":"/2020/03/28/k8s/Docker-Image/"},{"title":"RESTful API 了解","text":"1.REST概念REST全称是Representational State Transfer。要理解RESTful架构，需要理解Representational State Transfer这个词组到底是什么意思，它的每一个词都有些什么涵义。下面我们结合REST原则，围绕资源展开讨论，从资源的定义、获取、表述、关联、状态变迁等角度，列举一些关键概念并加以解释。 资源与URI 统一资源接口 资源的表述 资源的链接 状态的转移 2.RESTful API概念在开发的过程中，我们经常会听到前后端分离这个技术名词，顾名思义，就是前台的开发和后台的开发分离开。这个技术方案的实现就是要借助API，API简单说就是开发人员提供编程接口被其他人调用，他们调用之后会返回数据供其使用。API的类型有多种，但是现在比较主流且实用的就是RESTful API。 RESTful API 的总结：1.每一个URL代表一种资源2.客户端和服务器之间，传递这种资源的某种表现层3.客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。具体为：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 2.1 RESTful API方法-GET 安全且幂等 获取表示 变更时获取表示（缓存） 200（OK） - 表示已在响应中发出 204（无内容） - 资源有空表示 301（Moved Permanently） - 资源的URI已被更新 303（See Other） - 其他（如，负载均衡） 304（not modified）- 资源未更改（缓存） 400 （bad request）- 指代坏请求（如，参数错误） 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务端当前无法处理请求 2.2 RESTful API方法-POST 不安全且不幂等 使用服务端管理的（自动产生）的实例号创建资源 创建子资源 部分更新资源 如果没有被修改，则不过更新资源（乐观锁） 200（OK）- 如果现有资源已被更改 201（created）- 如果新资源被创建 202（accepted）- 已接受处理请求但尚未完成（异步处理） 301（Moved Permanently）- 资源的URI被更新 303（See Other）- 其他（如，负载均衡） 400（bad request）- 指代坏请求 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 409 （conflict）- 通用冲突 412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突） 415 （unsupported media type）- 接受到的表示不受支持 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务当前无法处理请求 2.3 RESTful API方法-PUT 不安全但幂等 用客户端管理的实例号创建一个资源 通过替换的方式更新资源 如果未被修改，则更新资源（乐观锁） 200 （OK）- 如果已存在资源被更改 201 （created）- 如果新资源被创建 301（Moved Permanently）- 资源的URI已更改 303 （See Other）- 其他（如，负载均衡） 400 （bad request）- 指代坏请求 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 409 （conflict）- 通用冲突 412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突） 415 （unsupported media type）- 接受到的表示不受支持 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务当前无法处理请求 2.4 RESTful API方法-DELETE 不安全但幂等 删除资源 200 （OK）- 资源已被删除 301 （Moved Permanently）- 资源的URI已更改 303 （See Other）- 其他，如负载均衡 400 （bad request）- 指代坏请求 404 （not found）- 资源不存在 409 （conflict）- 通用冲突 500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务端当前无法处理请求 3.常见问题 POST和PUT用于创建资源时有什么区别?POST和PUT在创建资源的区别在于，所创建的资源的名称(URI)是否由客户端决定。 例如为我的博文增加一个java的分类，生成的路径就是分类名/categories/java，那么就可以采用PUT方法。不过很多人直接把POST、GET、PUT、DELETE直接对应上CRUD，例如在一个典型的rails实现的RESTful应用中就是这么做的。 我认为，这是因为rails默认使用服务端生成的ID作为URI的缘故，而不少人就是通过rails实践REST的，所以很容易造成这种误解。 客户端不一定都支持这些HTTP方法吧?的确有这种情况，特别是一些比较古老的基于浏览器的客户端，只能支持GET和POST两种方法。 在实践上，客户端和服务端都可能需要做一些妥协。例如rails框架就支持通过隐藏参数_method=DELETE来传递真实的请求方法， 而像Backbone这样的客户端MVC框架则允许传递_method传输和设置X-HTTP-Method-Override头来规避这个问题。 统一接口是否意味着不能扩展带特殊语义的方法?统一接口并不阻止你扩展方法，只要方法对资源的操作有着具体的、可识别的语义即可，并能够保持整个接口的统一性。 像WebDAV就对HTTP方法进行了扩展，增加了LOCK、UPLOCK等方法。而github的API则支持使用PATCH方法来进行issue的更新，例如: PATCH /repos/:owner/:repo/issues/:number不过，需要注意的是，像PATCH这种不是HTTP标准方法的，服务端需要考虑客户端是否能够支持的问题。 统一资源接口对URI有什么指导意义?统一资源接口要求使用标准的HTTP方法对资源进行操作，所以URI只应该来表示资源的名称，而不应该包括资源的操作。 通俗来说，URI不应该使用动作来描述。例如，下面是一些不符合统一接口要求的URI: GET /getUser/1POST /createUserPUT /updateUser/1DELETE /deleteUser/1 4.RESTful API 应用 Django Rest framework 1.建立 Models2.依靠 Serialiers 将数据库取出的数据 Parse 为 API 的数据（可用于返回给客户端，也可用于浏览器显示）3.ViewSet 是一个 views 的集合，根据客户端的请求（GET、POST等），返回 Serialiers 处理的数据，权限 Premissions 也在这一步做处理4.ViewSet 可在 Routers 进行注册，注册后会显示在 Api Root 页上5.在 urls 里注册 ViewSet 生成的 view，指定监听的 url","link":"/2020/03/29/linux/restful-api/"},{"title":"k8s专题[1.k8s基础概念]","text":"1.概念Kubernetes是一个容器编排系统，也就是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。 Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。 Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。 2.特点 可移植: 支持公有云，私有云，混合云，多重云（multi-cloud） 可扩展: 模块化, 插件化, 可挂载, 可组合 自动化: 自动部署，自动重启，自动复制，自动伸缩/扩展 3.Kubernetes 架构 4.分层架构Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示： 核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等） 接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS 应用、ChatOps 等Kubernetes 内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等 5.生态系统 6.参考文章 https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md http://queue.acm.org/detail.cfm?id=2898444 http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf http://thenewstack.io/kubernetes-an-overview https://github.com/kubernetes/community/tree/master/sig-architecture","link":"/2020/03/22/k8s/K8S-1-Basic-concepts/"},{"title":"k8s专题[10.使用Spinnaker持续发布应用]","text":"本节介绍如何使用spinnaker的持续发布k8s的应用1.首先使用spinnaker创建一个nginxdemo 的应用，再在nginxdemo应用里面创建 Pipeline。进入nginxdemo 详情页面，点击 “PIPELINES”，目前是没有任何信息的，点击 “+ Create”，弹框中选择类型为 Pipeline，输入流程名称，这里我命名为 nginxdemo-pipe。因为第一次创建，下边 “Copy From” 选择没出来，后续在创建时，我们也可以通过 “Copy From” 方式选择已存在的 Pipeline，非常方便就复制了一个一样配置的流程。创建完毕后，就会出现详细配置 Pipeline State 的页面了 2.配置 Configuration 项 刚开始这里只有一个 Configuration 选项，可以配置 Automated Triggers、Parameters、Notifications 等，这里说下 Automated Triggers 和 Parameters 这两个非常有用，我们可以将此视为 Pipeline 启动前的一些初始化配置，比如启动需要的参数配置、自动触发配置等，为后续各阶段提供必要的信息。Automated Triggers 自动触发，它提供 7 种类型的触发方式： CRON：调度触发，可以执行一个 cron 调度任务来定时任务触发该流程。 Git：当执行 Git push 操作时，触发该流程 Jenkins：监听 Jenkins 的某一个 Job Travis：监听 Travis 的某一个 Job Pipeline：监听另一个 Pipeline 执行 Pub/Sub：当接受到 pubsub 消息时触发 Docker Registry：当 image 更新时触发。 基本能满足我们日常持续集成或交付的需求，当然每一个类型都需要配置相应的参数，比如 Cron 类型，需要配置执行频率、启动时间等。下图我们选择Docker Registry作为触发类型 3.下拉框可以选择私有仓库，选择ops/nginx的镜像 4.增加阶段，我们直接增加一个发布的阶段。 5.选择阶段的类型，我们选择deploy 6.增加需要发布的服务器组，这里我们先增加之前定义的nginxdemo-test 服务器组 7.选择容器：选择ops/nginx的镜像，阶段类型：选择红/黑部署，新版本的服务器组起来后service会把请求转到新版本的后端，旧版本的disable，但是旧版本的服务器组还存在集群中，方便后续快速回滚的需求 8.填写探针开发环境，测试环境，生产环境按照下面格式填写，更改容器端口和检测页面即可。 9.手动触发一个发布流程 10.选择发布容器镜像的版本 11.点击run，开始发布新的版本 12.部署完成 13.查看服务器组，看到选择的镜像版本的服务已经启动","link":"/2020/03/26/k8s/K8S-10-Use-Spinnaker-CICD/"},{"title":"k8s专题[2.k8s设计原则]","text":"1.API设计原则对于云计算系统，系统API实际上处于系统设计的统领地位。Kubernetes集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。理解掌握的API，就好比抓住了K8s系统的牛鼻子。Kubernetes系统API的设计有以下几条原则： 所有API应该是声明式的。声明式的操作，相对于命令式操作，对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。另外，声明式操作更容易被用户使用，可以使系统向用户隐藏实现的细节，同时也保留了系统未来持续优化的可能性。此外，声明式的API还隐含了所有的API对象都是名词性质的，例如Service、Volume这些API都是名词，这些名词描述了用户所期望得到的一个目标对象。 API对象是彼此互补而且可组合的。这实际上鼓励API对象尽量实现面向对象设计时的要求，即“高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。 高层API以操作意图为基础设计。如何能够设计好API，跟如何能用面向对象的方法设计好应用系统有相通的地方，高层设计一定是从业务出发，而不是过早的从技术实现出发。因此，针对Kubernetes的高层API设计，一定是以K8s的业务为基础出发，也就是以系统调度管理容器的操作意图为基础设计。 低层API根据高层API的控制需要设计。设计实现低层API的目的，是为了被高层API使用，考虑减少冗余、提高重用性的目的，低层API的设计也要以需求为基础，要尽量抵抗受技术实现影响的诱惑。 尽量避免简单封装，不要有在外部API无法显式知道的内部隐藏的机制。简单的封装，实际没有提供新的功能，反而增加了对所封装API的依赖性。内部隐藏的机制也是非常不利于系统维护的设计方式，例如StatefulSet和ReplicaSet，本来就是两种Pod集合，那么Kubernetes就用不同API对象来定义它们，而不会说只用同一个ReplicaSet，内部通过特殊的算法再来区分这个ReplicaSet是有状态的还是无状态。 API操作复杂度与对象数量成正比。这一条主要是从系统性能角度考虑，要保证整个系统随着系统规模的扩大，性能不会迅速变慢到无法使用，那么最低的限定就是API的操作复杂度不能超过O(N)，N是对象的数量，否则系统就不具备水平伸缩性了。 API对象状态不能依赖于网络连接状态。由于众所周知，在分布式环境下，网络连接断开是经常发生的事情，因此要保证API对象状态能应对网络的不稳定，API对象的状态就不能依赖于网络连接状态。尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的。 Kubernetes 核心API图示: 2.控制机制设计原则 控制逻辑应该只依赖于当前状态。这是为了保证分布式系统的稳定可靠，对于经常出现局部错误的分布式系统，如果控制逻辑只依赖当前状态，那么就非常容易将一个暂时出现故障的系统恢复到正常状态，因为你只要将该系统重置到某个稳定状态，就可以自信的知道系统的所有控制逻辑会开始按照正常方式运行。 假设任何错误的可能，并做容错处理。在一个分布式系统中出现局部和临时错误是大概率事件。错误可能来自于物理系统故障，外部系统故障也可能来自于系统自身的代码错误，依靠自己实现的代码不会出错来保证系统稳定其实也是难以实现的，因此要设计对任何可能错误的容错处理。 尽量避免复杂状态机，控制逻辑不要依赖无法监控的内部状态。因为分布式系统各个子系统都是不能严格通过程序内部保持同步的，所以如果两个子系统的控制逻辑如果互相有影响，那么子系统就一定要能互相访问到影响控制逻辑的状态，否则，就等同于系统里存在不确定的控制逻辑。 假设任何操作都可能被任何操作对象拒绝，甚至被错误解析。由于分布式系统的复杂性以及各子系统的相对独立性，不同子系统经常来自不同的开发团队，所以不能奢望任何操作被另一个子系统以正确的方式处理，要保证出现错误的时候，操作级别的错误不会影响到系统稳定性。 每个模块都可以在出错后自动恢复。由于分布式系统中无法保证系统各个模块是始终连接的，因此每个模块要有自我修复的能力，保证不会因为连接不到其他模块而自我崩溃。 每个模块都可以在必要时优雅地降级服务。所谓优雅地降级服务，是对系统鲁棒性的要求，即要求在设计实现模块时划分清楚基本功能和高级功能，保证基本功能不会依赖高级功能，这样同时就保证了不会因为高级功能出现故障而导致整个模块崩溃。根据这种理念实现的系统，也更容易快速地增加新的高级功能，因为不必担心引入高级功能影响原有的基本功能。 3.架构设计原则 Self-hosting 是目标 减少依赖，特别是稳态运行的依赖 通过分层的原则管理依赖 循环依赖问题的原则 同时还接受其他方式的数据输入（比如本地文件等），这样在其他服务不可用时还可以手动配置引导服务 状态应该是可恢复或可重新发现的 支持简单的启动临时实例来创建稳态运行所需要的状态；使用分布式锁或文件锁等来协调不同状态的切换（通常称为pivoting技术） 自动重启异常退出的服务，比如副本或者进程管理器等 4.参考文章 https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md Kubernetes Design Principles Kubernetes与云原生应用","link":"/2020/03/24/k8s/K8S-2-Design-Principles/"},{"title":"k8s专题[3.k8s基础组件]","text":"1.核心组件1.1 etcd 保存了整个集群的状态,etcd是Kubernetes提供默认的存储系统，保存所有集群数据，使用时需要为etcd数据提供备份计划 1.2 kube-apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制 1.3 kube-controller-manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等,kube-controller-manager运行管理控制器，它们是集群中处理常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成单个二进制文件，并在单个进程中运行.这些控制器包括： 节点（Node）控制器 副本（Replication）控制器：负责维护系统中每个副本中的pod 端点（Endpoints）控制器：填充Endpoints对象（即连接Services＆Pods） Service Account和Token控制器：为新的Namespace 创建默认帐户访问API Token 1.4 kube-scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上 1.5 kubelet 负责维持容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理 1.6 Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI），默认的容器运行时为 Docker. 1.7 kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡. 1.8 docker服务，用于运行容器. 2.常用插件除了核心组件，还有一些推荐的 Add-ons： coredns 负责为整个集群提供 DNS 服务 CNI网络插件，常用calico，Flannel Ingress Controller 为服务提供外网入口 Heapster 提供资源监控 Dashboard 提供 GUI Federation 提供跨可用区的集群 Fluentd-elasticsearch 提供集群日志采集、存储与查询 3.组件高可用性3.1 k8s 高可用2个核心 apiserver master and etcd 3.2 apiserver master：（需高可用）集群核心，集群API接口、集群各个组件通信的中枢；集群安全控制； 3.3 etcd ：（需高可用）集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复；因此高可用集群部署首先就是etcd是高可用集群； 3.4 kube-scheduler：调度器 （内部自选举）集群Pod的调度中心；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态； 3.5 kube-controller-manager： 控制器 （内部自选举）集群状态管理器，当集群状态与期望不同时，kcm会努力让集群恢复期望状态，比如：当一个pod死掉，kcm会努力新建一个pod来恢复对应replicas set期望的状态；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态； 3.6 kubelet: agent node注册apiserver 3.7 kube-proxy: 每个node上一个，负责service vip到endpoint pod的流量转发，老版本主要通过设置iptables规则实现，新版1.9基于kube-proxy-lvs 实现 4.参考文章 https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md Kubernetes Design Principles Kubernetes与云原生应用","link":"/2020/03/24/k8s/K8S-3-Basic-components/"},{"title":"k8s专题[4.k8s资源对象]","text":"一、API对象资源API对象是K8s集群中的管理操作单元。K8s集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set对应的API对象是RS。 每个API对象都有3大类属性：元数据metadata、规范spec和状态status。元数据是用来标识API对象的，每个对象都至少有3个元数据：namespace，name和uid；除此以外还有各种各样的标签labels用来标识和匹配不同的对象，例如用户可以用标签env来标识区分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、测试、生产的不同服务。规范描述了用户期望K8s集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器Replication Controller设置期望的Pod副本数为3；status描述了系统实际当前达到的状态（Status），例如系统当前实际的Pod副本数为2；那么复本控制器当前的程序逻辑就是自动启动新的Pod，争取达到副本数为3。 K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果，而给副本数加1的操作就不是声明式的，运行多次结果就错了。 1.PodK8s有很多技术概念，同时对应很多API对象，最重要的也是最基础的是微服务Pod。Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod对多容器的支持是K8s最基础的设计理念。比如你运行一个操作系统发行版的软件仓库，一个Nginx容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像不太可能是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。 Pod是K8s集群中所有业务类型的基础，可以看作运行在K8s集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为Deployment、Job、DaemonSet和StatefulSet，本文后面会一一介绍。 2.复制控制器（Replication Controller，RC）RC是K8s集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有1个Pod在运行。RC是K8s较早期的技术概念，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。 3.副本集（Replica Set，RS）RS是新一代RC，提供同样的高可用能力，区别主要在于RS后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数使用。 4.部署(Deployment)部署表示用户对K8s集群的一次更新操作。部署是一个比RS应用模式更广的API对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新RS中副本数增加到理想状态，将旧RS中的副本数减小到0的复合操作；这样一个复合操作用一个RS是不太好描述的，所以用一个更通用的Deployment来描述。以K8s的发展方向，未来对所有长期伺服型的的业务的管理，都会通过Deployment来管理。 5.服务（Service）RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在K8s集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。在K8s集群中微服务的负载均衡是由Kube-proxy实现的。Kube-proxy是K8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8s的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端使用反向代理作负载均衡，还要进一步解决反向代理的高可用问题。 6.任务（Job）Job是K8s用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。 7.后台支撑服务集（DaemonSet）长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的Pod，有些节点上又没有这类Pod运行；而后台支撑型服务的核心关注点在K8s集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类Pod运行。节点可能是所有集群节点也可能是通过nodeSelector选定的一些特定节点。典型的后台支撑型服务包括，存储，日志和监控等在每个节点上支撑K8s集群运行的服务。 8.有状态服务集（StatefulSet）K8s在1.3版本里发布了Alpha版的PetSet以支持有状态服务，并从1.5版本开始重命名为StatefulSet。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数；而StatefulSet是用来控制有状态服务，StatefulSet中的每个Pod的名字都是事先确定的，不能更改。StatefulSet中Pod的名字的作用，并不是《千与千寻》的人性原因，而是关联与该Pod对应的状态。 对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别（这似乎也确实意味着失去了人性特征）；对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。 适合于StatefulSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。StatefulSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用StatefulSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，StatefulSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。StatefulSet还只在Alpha阶段，后面的设计如何演变，我们还要继续观察。 9.集群联邦（Federation）K8s在1.3版本里发布了beta版的Federation功能。在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。K8s的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足K8s的调度和计算存储连接要求。而联合集群服务就是为提供跨Region跨服务商K8s集群服务而设计的。 每个K8s Federation有自己的分布式存储、API Server和Controller Manager。用户可以通过Federation的API Server注册该Federation的成员K8s Cluster。当用户通过Federation的API Server创建、更改API对象时，Federation API Server会在自己所有注册的子K8s Cluster都创建一份对应的API对象。在提供业务请求服务时，K8s Federation会先在自己的各个子Cluster之间做负载均衡，而对于发送到某个具体K8s Cluster的业务请求，会依照这个K8s Cluster独立提供服务时一样的调度模式去做K8s Cluster内部的负载均衡。而Cluster之间的负载均衡是通过域名服务的负载均衡来实现的。 所有的设计都尽量不影响K8s Cluster现有的工作机制，这样对于每个子K8s集群来说，并不需要更外层的有一个K8s Federation，也就是意味着所有现有的K8s代码和机制不需要因为Federation功能有任何变化。 10.存储卷（Volume）K8s集群中的存储卷跟Docker的存储卷有些类似，只不过Docker的存储卷作用范围为一个容器，而K8s的存储卷的生命周期和作用范围是一个Pod。每个Pod中声明的存储卷由Pod中的所有容器共享。K8s支持非常多的存储卷类型，特别的，支持多种公有云平台的存储，包括AWS，Google和Azure云；支持多种分布式存储包括GlusterFS和Ceph；也支持较容易使用的主机本地目录hostPath和NFS。K8s还支持使用Persistent Volume Claim即PVC这种逻辑存储，使用这种存储，使得存储的使用者可以忽略后台的实际存储技术（例如AWS，Google或GlusterFS和Ceph），而将有关存储实际技术的配置交给存储管理员通过Persistent Volume来配置。 11.持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC）PV和PVC使得K8s集群具备了存储的逻辑抽象能力，使得在配置Pod的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。存储的PV和PVC的这种关系，跟计算的Node和Pod的关系是非常类似的；PV和Node是资源的提供者，根据集群的基础设施变化而变化，由K8s集群管理员配置；而PVC和Pod是资源的使用者，根据业务服务的需求变化而变化，由K8s集群的使用者即服务的管理员来配置。 12.节点（Node）K8s集群中的计算能力由Node提供，最初Node称为服务节点Minion，后来改名为Node。K8s集群中的Node也就等同于Mesos集群中的Slave节点，是所有Pod运行所在的工作主机，可以是物理机也可以是虚拟机。不论是物理机还是虚拟机，工作主机的统一特征是上面要运行kubelet管理节点上运行的容器。 13.密钥对象（Secret）Secret是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。使用Secret的好处是可以避免把敏感信息明文写在配置文件里。在K8s集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问AWS存储的用户名密码。为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个Secret对象，而在配置文件中通过Secret对象引用这些敏感信息。这种方式的好处包括：意图明确，避免重复，减少暴露机会。 14.用户帐户（User Account）和服务帐户（Service Account）顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和K8s集群中运行的Pod提供账户标识。用户帐户和服务帐户的一个区别是作用范围；用户帐户对应的是人的身份，人的身份与服务的namespace无关，所以用户账户是跨namespace的；而服务帐户对应的是一个运行中程序的身份，与特定namespace是相关的。 15.名字空间（Namespace）名字空间为K8s集群提供虚拟的隔离作用，K8s集群初始有两个名字空间，分别是默认名字空间default和系统名字空间kube-system，除此以外，管理员可以创建新的名字空间满足需要。 16.RBAC访问授权K8s在1.3版本中发布了alpha版的基于角色的访问控制（Role-based Access Control，RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control，ABAC），RBAC主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。在ABAC中，K8s集群中的访问策略只能跟用户直接关联；而在RBAC中，访问策略可以跟某个角色关联，具体的用户在跟一个或多个角色相关联。显然，RBAC像其他新功能一样，每次引入新功能，都会引入新的API对象，从而引入新的概念抽象，而这一新的概念抽象一定会使集群服务管理和使用更容易扩展和重用。 二、总结从K8s的系统架构、技术概念和设计理念，我们可以看到K8s系统最核心的两个设计理念：一个是容错性，一个是易扩展性。容错性实际是保证K8s系统稳定性和安全性的基础，易扩展性是保证K8s对变更友好，可以快速迭代增加新功能的基础。 三、参考文档 https://github.com/feiskyer/kubernetes-handbook/edit/master/architecture/concepts.md Kubernetes Design Principles Kubernetes与云原生应用","link":"/2020/03/25/k8s/K8S-4-Resource-object/"},{"title":"k8s专题[5.k8s高可用集群部署]","text":"1.高可用集群概述部署k8s的高可用集群，要做到无单电，主要是依靠负载均衡和k8s集群自身的高可用性。对于k8s的api，它本身是无状态的，自身不具备高可用，但是如果要它提供高可用的服务，做到其他节点和组件能无时无刻都可以访问它，需要在k8s api前面加一层tcp负载均衡器。一般k8s集群至少三个master节点，所以tcp转发层需要对这三台k8s api节点做负载均衡。这里有人可能问了，为什么需要tcp负载均衡，不做http负载均衡？原因就是k8s api的通信是https加密的，负载均衡器做https解析会比较麻烦，而且做tcp负载均衡效率也会高一些，也会比较方便，高版本的nginx就直接支持tcp转发。还有就是这三台master节点的服务器怎么放呢？这个要看个人的资源环境，比如你只是单机房的，那只能把master节点部署在一个机房里面了，然后三台服务器之间加个vip，使用keepalived做vip漂移，达到k8s api的高可用，如果你是同城三机房的，可以在每个机房部署一个master节点，然后每个机房部署一个高可用的负载均衡器，负载均衡器把请求转发到三个机房的k8s api上面去。然后对于etcd数据库，他本身自带高可用功能，部署三个以上节点的节点就好了，对于kube-scheduler组件，它们内部会自动选择一个leader节点，默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态，对于kube-controller-manager和kube-scheduler也是类似，默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态。对于k8s核心组件的功能，下面再说明一下： kube-apiserver：集群核心,集群API接口、集群各个组件通信的中枢；集群安全控制。 etcd：集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复；因此高可用集群部署首先就是etcd是高可用集群； kube-scheduler：集群Pod的调度中心；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态； kube-controller-manager：集群状态管理器，当集群状态与期望不同时，kcm会努力让集群恢复期望状态，比如：当一个pod死掉，kcm会努力新建一个pod来恢复对应replicas set期望的状态；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态； kubelet：kubernetes node agent，负责与node上的docker engine打交道； kube-proxy：每个node上一个，负责service vip到endpoint pod的流量转发，当前主要通过设置iptables规则实现。 2.k8s高可用架构图这里单机房和多机房的区别就在于单机房整个机房就一个vip，多机房就是每个机房都有一个vip，每个机房都有一个api访问入口。 3.部署过程 本次单机房v1.9.2版本，版本比较老了，因为文档比较老了，不过新版本大同小异 3.1：资源清单1234567891011121314151617181920212223242526272829303132333435363738394041424344主机节点清单 主机名 IP地址 说明 组件 k8s-master1 192.168.3.148 master keepalived、nginx、etcd、kubelet、kube-apiserver、kube-scheduler、kube-proxy、kube-dashboard、heapster、calico k8s-master2 192.168.3.149 master keepalived、nginx、etcd、kubelet、kube-apiserver、kube-scheduler、kube-proxy、kube-dashboard、heapster、calico k8s-master3 192.168.3.150 master keepalived、nginx、etcd、kubelet、kube-apiserver、kube-scheduler、kube-proxy、kube-dashboard、heapster、calico 无 192.168.3.157 虚拟IP 无 k8s-node1 192.168.3.154 node kubelet、kube-proxy k8s-node2 192.168.3.155 node kubelet、kube-proxykubernetes对应docker版本 Kubernetes 1.9 &lt;--Docker 1.11.2 to 1.13.1 and 17.03.x Kubernetes 1.8 &lt;--Docker 1.11.2 to 1.13.1 and 17.03.x Kubernetes 1.7 &lt;--Docker 1.10.3, 1.11.2, 1.12.6 Kubernetes 1.6 &lt;--Docker 1.10.3, 1.11.2, 1.12.6 Kubernetes 1.5 &lt;--Docker 1.10.3, 1.11.2, 1.12.3docker镜像准备 master节点： gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.3 gcr.io/google_containers/kube-proxy-amd64:v1.9.2 gcr.io/google_containers/kube-controller-manager-amd64:v1.9.2 gcr.io/google_containers/kube-apiserver-amd64:v1.9.2 gcr.io/google_containers/kube-scheduler-amd64:v1.9.2 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.7 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.7 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.7 gcr.io/google_containers/etcd-amd64:3.1.10 gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3 gcr.io/google_containers/heapster-grafana-amd64:v4.4.3 gcr.io/google_containers/heapster-amd64:v1.4.2 gcr.io/google_containers/pause-amd64:3.0 quay.io/calico/node:v3.0.3 quay.io/calico/kube-controllers:v2.0.1 quay.io/calico/cni:v2.0.1 quay.io/coreos/flannel:v0.9.1-amd64 nginx:latest node节点: gcr.io/google_containers/pause-amd64:3.0 gcr.io/google_containers/kube-proxy-amd64:v1.9.2 quay.io/calico/node:v3.0.3 quay.io/calico/kube-controllers:v2.0.1 quay.io/calico/cni:v2.0.1 quay.io/coreos/flannel:v0.9.1-amd64 3.2：软件安装,所有节点都同样处理1234567891011121314151617181920212223242526 mkdir -p /data/soft/docker yum -y install epel-release yum install -y git wget lrzsz vim net-tools yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=0 EOF cd /data/soft/docker yumdownloader docker-ce-17.03.0.ce yumdownloader docker-ce-selinux-17.03.0.ce yum remove docker docker-common docker-selinux docker-engine yum localinstall * yum -y install kubelet-1.9.2 kubectl-1.9.2 kubeadm-1.9.2 systemctl stop firewalld &amp;&amp; systemctl disable firewalld systemctl start docker &amp;&amp; systemctl enable docker systemctl start kubelet &amp;&amp; systemctl enable kubelet master节点： yum install -y keepalived systemctl enable keepalived &amp;&amp; systemctl start keepalived 3.3：系统设置，所有节点都同样处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#selinux setenforce 0 cp /etc/selinux/config /etc/selinux/config.bak sed -i '/SELINUX=enforcing/s/enforcing/disabled/' /etc/selinux/config#配置系统路由参数,防止kubeadm报路由警告、禁用swap echo \" vm.swappiness = 0 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 \" &gt;&gt; /etc/sysctl.conf sysctl -p swapoff -a#修改kubelet环境变量,修改KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs cp /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.bak sed -i '/KUBELET_CGROUP_ARGS/s/systemd/cgroupfs/' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf cat &gt; /etc/systemd/system/kubelet.service.d/20-pod-infra-image.conf &lt;&lt;EOF [Service] Environment=\"KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.test.sui.internal/test/pause-amd64:3.0\" EOF#修改docker配置 cp -p /usr/lib/systemd/system/docker.service /usr/lib/systemd/system/docker.service.bak sed -i '/ExecStart=/s/$/&amp; --insecure-registry registry.test.sui.internal/' /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl restart kubelet systemctl restart docker#防火墙设置 master:在所有master节点上开放相关firewalld端口（因为以上服务基于docker部署，如果docker版本为17.x，可以不进行以下设置，因为docker会自动修改iptables添加相关端口） 协议 方向 端口 说明 TCP Inbound 16443* Load balancer Kubernetes API server port TCP Inbound 6443* Kubernetes API server TCP Inbound 4001 etcd listen client port TCP Inbound 2379-2380 etcd server client API TCP Inbound 10250 Kubelet API TCP Inbound 10251 kube-scheduler TCP Inbound 10252 kube-controller-manager TCP Inbound 10255 Read-only Kubelet API TCP Inbound 30000-32767 NodePort Services $ systemctl status firewalld $ firewall-cmd --zone=public --add-port=16443/tcp --permanent $ firewall-cmd --zone=public --add-port=6443/tcp --permanent $ firewall-cmd --zone=public --add-port=4001/tcp --permanent $ firewall-cmd --zone=public --add-port=2379-2380/tcp --permanent $ firewall-cmd --zone=public --add-port=10250/tcp --permanent $ firewall-cmd --zone=public --add-port=10251/tcp --permanent $ firewall-cmd --zone=public --add-port=10252/tcp --permanent $ firewall-cmd --zone=public --add-port=10255/tcp --permanent $ firewall-cmd --zone=public --add-port=30000-32767/tcp --permanent $ firewall-cmd --reload $ firewall-cmd --list-all --zone=public public (active) target: default icmp-block-inversion: no interfaces: ens2f1 ens1f0 nm-bond sources: services: ssh dhcpv6-client ports: 4001/tcp 6443/tcp 2379-2380/tcp 10250/tcp 10251/tcp 10252/tcp 10255/tcp 30000-32767/tcp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules:#node:在所有worker节点上开放相关firewalld端口（因为以上服务基于docker部署，如果docker版本为17.x，可以不进行以下设置，因为docker会自动修改iptables添加相关端口） 协议 方向 端口 说明 TCP Inbound 10250 Kubelet API TCP Inbound 10255 Read-only Kubelet API TCP Inbound 30000-32767 NodePort Services $ systemctl status firewalld $ firewall-cmd --zone=public --add-port=10250/tcp --permanent $ firewall-cmd --zone=public --add-port=10255/tcp --permanent $ firewall-cmd --zone=public --add-port=30000-32767/tcp --permanent $ firewall-cmd --reload $ firewall-cmd --list-all --zone=public public (active) target: default icmp-block-inversion: no interfaces: ens2f1 ens1f0 nm-bond sources: services: ssh dhcpv6-client ports: 10250/tcp 10255/tcp 30000-32767/tcp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules:所有节点,在所有kubernetes节点上允许kube-proxy的forward $ firewall-cmd --permanent --direct --add-rule ipv4 filter INPUT 1 -i docker0 -j ACCEPT -m comment --comment \"kube-proxy redirects\" $ firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD 1 -o docker0 -j ACCEPT -m comment --comment \"docker subnet\" $ firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD 1 -i flannel.1 -j ACCEPT -m comment --comment \"flannel subnet\" $ firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD 1 -o flannel.1 -j ACCEPT -m comment --comment \"flannel subnet\" $ firewall-cmd --reload $ firewall-cmd --direct --get-all-rules ipv4 filter INPUT 1 -i docker0 -j ACCEPT -m comment --comment 'kube-proxy redirects' ipv4 filter FORWARD 1 -o docker0 -j ACCEPT -m comment --comment 'docker subnet' ipv4 filter FORWARD 1 -i flannel.1 -j ACCEPT -m comment --comment 'flannel subnet' ipv4 filter FORWARD 1 -o flannel.1 -j ACCEPT -m comment --comment 'flannel subnet' #在所有kubernetes节点上，删除iptables的设置，解决kube-proxy无法启用nodePort。（注意：每次重启firewalld必须执行以下命令） iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited 3.3：配置文件初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128#在所有master节点上获取代码，并进入代码目录 git clone https://github.com/cookeem/kubeadm-ha cd kubeadm-ha #在所有master节点上设置初始化脚本配置，每一项配置参见脚本中的配置说明，请务必正确配置。该脚本用于生成相关重要的配置文件 [root@k8s-master1 kubeadm-ha]# cat create-config.sh #!/bin/bash # local machine ip address export K8SHA_IPLOCAL=192.168.3.148 # local machine etcd name, options: etcd1, etcd2, etcd3 export K8SHA_ETCDNAME=etcd1 # local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP export K8SHA_KA_STATE=MASTER # local machine keepalived priority config, options: 102, 101, 100. MASTER must 102 export K8SHA_KA_PRIO=102 # local machine keepalived network interface name config, for example: eth0 export K8SHA_KA_INTF=ens3 ####################################### # all masters settings below must be same ####################################### # master keepalived virtual ip address export K8SHA_IPVIRTUAL=192.168.3.157 # master01 ip address export K8SHA_IP1=192.168.3.148 # master02 ip address export K8SHA_IP2=192.168.3.149 # master03 ip address export K8SHA_IP3=192.168.3.150 # master01 hostname export K8SHA_HOSTNAME1=k8s-master1 # master02 hostname export K8SHA_HOSTNAME2=k8s-master2 # master03 hostname export K8SHA_HOSTNAME3=k8s-master3 # keepalived auth_pass config, all masters must be same export K8SHA_KA_AUTH=4cdf7dc3b4c90194d1600c483e10ad1d # kubernetes cluster token, you can use 'kubeadm token generate' to get a new one export K8SHA_TOKEN=7f276c.0741d82a5337f526 # kubernetes CIDR pod subnet, if CIDR pod subnet is \"10.244.0.0/16\" please set to \"10.244.0.0\\\\/16\" export K8SHA_CIDR=10.244.0.0\\\\/16 # kubernetes CIDR service subnet, if CIDR service subnet is \"10.96.0.0/12\" please set to \"10.96.0.0\\\\/12\" export K8SHA_SVC_CIDR=10.96.0.0\\\\/16 # calico network settings, set a reachable ip address for the cluster network interface, for example you can use the gateway ip address export K8SHA_CALICO_REACHABLE_IP=192.168.3.1 ############################## # please do not modify anything below ############################## # set etcd cluster docker-compose.yaml file sed \\ -e \"s/K8SHA_ETCDNAME/$K8SHA_ETCDNAME/g\" \\ -e \"s/K8SHA_IPLOCAL/$K8SHA_IPLOCAL/g\" \\ -e \"s/K8SHA_IP1/$K8SHA_IP1/g\" \\ -e \"s/K8SHA_IP2/$K8SHA_IP2/g\" \\ -e \"s/K8SHA_IP3/$K8SHA_IP3/g\" \\ etcd/docker-compose.yaml.tpl &gt; etcd/docker-compose.yaml echo 'set etcd cluster docker-compose.yaml file success: etcd/docker-compose.yaml' # set keepalived config file mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak cp keepalived/check_apiserver.sh /etc/keepalived/ sed \\ -e \"s/K8SHA_KA_STATE/$K8SHA_KA_STATE/g\" \\ -e \"s/K8SHA_KA_INTF/$K8SHA_KA_INTF/g\" \\ -e \"s/K8SHA_IPLOCAL/$K8SHA_IPLOCAL/g\" \\ -e \"s/K8SHA_KA_PRIO/$K8SHA_KA_PRIO/g\" \\ -e \"s/K8SHA_IPVIRTUAL/$K8SHA_IPVIRTUAL/g\" \\ -e \"s/K8SHA_KA_AUTH/$K8SHA_KA_AUTH/g\" \\ keepalived/keepalived.conf.tpl &gt; /etc/keepalived/keepalived.conf echo 'set keepalived config file success: /etc/keepalived/keepalived.conf' # set nginx load balancer config file sed \\ -e \"s/K8SHA_IP1/$K8SHA_IP1/g\" \\ -e \"s/K8SHA_IP2/$K8SHA_IP2/g\" \\ -e \"s/K8SHA_IP3/$K8SHA_IP3/g\" \\ nginx-lb/nginx-lb.conf.tpl &gt; nginx-lb/nginx-lb.conf echo 'set nginx load balancer config file success: nginx-lb/nginx-lb.conf' # set kubeadm init config file sed \\ -e \"s/K8SHA_HOSTNAME1/$K8SHA_HOSTNAME1/g\" \\ -e \"s/K8SHA_HOSTNAME2/$K8SHA_HOSTNAME2/g\" \\ -e \"s/K8SHA_HOSTNAME3/$K8SHA_HOSTNAME3/g\" \\ -e \"s/K8SHA_IP1/$K8SHA_IP1/g\" \\ -e \"s/K8SHA_IP2/$K8SHA_IP2/g\" \\ -e \"s/K8SHA_IP3/$K8SHA_IP3/g\" \\ -e \"s/K8SHA_IPVIRTUAL/$K8SHA_IPVIRTUAL/g\" \\ -e \"s/K8SHA_TOKEN/$K8SHA_TOKEN/g\" \\ -e \"s/K8SHA_CIDR/$K8SHA_CIDR/g\" \\ -e \"s/K8SHA_SVC_CIDR/$K8SHA_SVC_CIDR/g\" \\ kubeadm-init.yaml.tpl &gt; kubeadm-init.yaml echo 'set kubeadm init config file success: kubeadm-init.yaml' # set canal deployment config file sed \\ -e \"s/K8SHA_CIDR/$K8SHA_CIDR/g\" \\ -e \"s/K8SHA_CALICO_REACHABLE_IP/$K8SHA_CALICO_REACHABLE_IP/g\" \\ kube-canal/canal.yaml.tpl &gt; kube-canal/canal.yaml echo 'set canal deployment config file success: kube-canal/canal.yaml' 3.4：在所有master节点上运行配置脚本，创建对应的配置文件，配置文件包括1.etcd集群docker-compose.yaml文件2.keepalived配置文件3.nginx负载均衡集群docker-compose.yaml文件4.kubeadm init 配置文件5.canal配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125 $ ./create-config.sh set etcd cluster docker-compose.yaml file success: etcd/docker-compose.yaml set keepalived config file success: /etc/keepalived/keepalived.conf set nginx load balancer config file success: nginx-lb/nginx-lb.conf set kubeadm init config file success: kubeadm-init.yaml set canal deployment config file success: kube-canal/canal.yaml [root@k8s-master1 kubeadm-ha]# cat etcd/docker-compose.yaml version: '2' services: etcd: image: gcr.io/google_containers/etcd-amd64:3.1.10 container_name: etcd hostname: etcd volumes: - /etc/ssl/certs:/etc/ssl/certs - /var/lib/etcd-cluster:/var/lib/etcd ports: - 4001:4001 - 2380:2380 - 2379:2379 restart: always command: [\"sh\", \"-c\", \"etcd --name=etcd1 \\ --advertise-client-urls=http://192.168.3.148:2379,http://192.168.3.148:4001 \\ --listen-client-urls=http://0.0.0.0:2379,http://0.0.0.0:4001 \\ --initial-advertise-peer-urls=http://192.168.3.148:2380 \\ --listen-peer-urls=http://0.0.0.0:2380 \\ --initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \\ --initial-cluster=etcd1=http://192.168.3.148:2380,etcd2=http://192.168.3.149:2380,etcd3=http://192.168.3.150:2380 \\ --initial-cluster-state=new \\ --auto-tls \\ --peer-auto-tls \\ --data-dir=/var/lib/etcd\"] [root@k8s-master1 kubeadm-ha]# ls keepalived/ check_apiserver.sh keepalived.conf.tpl [root@k8s-master1 kubeadm-ha]# cat /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script chk_apiserver { script \"/etc/keepalived/check_apiserver.sh\" interval 2 weight -5 fall 3 rise 2 } vrrp_instance VI_1 { state MASTER interface ens3 mcast_src_ip 192.168.3.148 virtual_router_id 157 priority 102 advert_int 2 authentication { auth_type PASS auth_pass 4cdf7dc3b4c90194d1600c483e10ad1d } virtual_ipaddress { 192.168.3.157 } track_script { chk_apiserver } } [root@k8s-master1 kubeadm-ha]# cat nginx-lb/docker-compose.yaml version: '2' services: etcd: image: nginx:latest container_name: nginx-lb hostname: nginx-lb volumes: - ./nginx-lb.conf:/etc/nginx/nginx.conf ports: - 16443:16443 restart: always [root@k8s-master1 kubeadm-ha]# cat kubeadm-init.yaml apiVersion: kubeadm.k8s.io/v1alpha1 kind: MasterConfiguration kubernetesVersion: v1.9.2 networking: podSubnet: 10.244.0.0/16 serviceSubnet: 10.96.0.0/16 apiServerCertSANs: - k8s-master1 - k8s-master2 - k8s-master3 - 192.168.3.148 - 192.168.3.149 - 192.168.3.150 - 192.168.3.157 - 127.0.0.1 etcd: endpoints: - http://192.168.3.148:2379 - http://192.168.3.149:2379 - http://192.168.3.150:2379 token: 7f276c.0741d82a5337f526 tokenTTL: \"0\"#独立etcd集群部署,如已安装过，需清除历史数据 rm -rf /var/lib/etcd docker-compose --file etcd/docker-compose.yaml stop docker-compose --file etcd/docker-compose.yaml rm -f#启动etcd集群 docker-compose --file etcd/docker-compose.yaml up -d#验证etcd集群状态是否正常 docker exec -ti etcd etcdctl cluster-health [root@k8s-master1 kubeadm-ha]# docker exec -ti etcd etcdctl cluster-health member 89e5a572671ddffc is healthy: got healthy result from http://192.168.3.148:2379 member bf48e493e3fe022d is healthy: got healthy result from http://192.168.3.149:2379 member d9c99f531d70dd69 is healthy: got healthy result from http://192.168.3.150:2379 cluster is healthy docker exec -ti etcd etcdctl member list [root@k8s-master1 kubeadm-ha]# docker exec -ti etcd etcdctl member list 89e5a572671ddffc: name=etcd1 peerURLs=http://192.168.3.148:2380 clientURLs=http://192.168.3.148:2379,http://192.168.3.148:4001 isLeader=false bf48e493e3fe022d: name=etcd2 peerURLs=http://192.168.3.149:2380 clientURLs=http://192.168.3.149:2379,http://192.168.3.149:4001 isLeader=true d9c99f531d70dd69: name=etcd3 peerURLs=http://192.168.3.150:2380 clientURLs=http://192.168.3.150:2379,http://192.168.3.150:4001 isLeader=false 3.5：开始进行master初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131#第一台master初始化kubeadm初始化： kubeadm init --config=kubeadm-init.yaml#在所有master节点上设置kubectl客户端连接 echo \"export KUBECONFIG=/etc/kubernetes/admin.conf\" &gt;&gt; ~/.bashrc source ~/.bashrc#安装基础组件#安装canal网络组件 kubectl apply -f kube-canal/ 等待所有pods正常 kubectl get pods --all-namespaces -o wide#安装dashboard kubectl apply -f kube-dashboard/ serviceaccount \"admin-user\" created clusterrolebinding \"admin-user\" created secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created 通过浏览器访问dashboard地址 https://k8s-master1:30000/#!/login 获取token，把token粘贴到login页面的token中，即可进入dashboard kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')#安装heapster kubectl apply -f kube-heapster/influxdb/ service \"monitoring-grafana\" created serviceaccount \"heapster\" created deployment \"heapster\" created service \"heapster\" created deployment \"monitoring-influxdb\" created service \"monitoring-influxdb\" created kubectl apply -f kube-heapster/rbac/ clusterrolebinding \"heapster\" created kubectl get pods --all-namespaces#master集群高可用设置#复制配置 在k8s-master1上复制目录/etc/kubernetes/pki到k8s-master2、k8s-master3，从v1.9.x开始，kubeadm会检测pki目录是否有证书，如果已经存在证书则跳过证书生成的步骤 scp -r /etc/kubernetes/pki k8s-master2:/etc/kubernetes/ scp -r /etc/kubernetes/pki k8s-master3:/etc/kubernetes/#其余master节点初始化#在k8s-master2进行初始化，等待所有pods正常启动后再进行下一个master初始化，特别要保证kube-apiserver-{current-node-name}处于running状态 kubeadm init --config=kubeadm-init.yaml#在k8s-master3进行初始化，等待所有pods正常启动后再进行下一个master初始化，特别要保证kube-apiserver-{current-node-name}处于running状态 kubeadm init --config=kubeadm-init.yaml#dns支持多节点 kubectl scale --replicas=2 -n kube-system deployment/kube-dns#检查集群状态 kubectl get pods --all-namespaces -o wide [root@k8s-master1 kubeadm-ha]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system canal-kcnfp 3/3 Running 0 3h kube-system canal-lzzf8 3/3 Running 17 2d kube-system canal-sf8l6 3/3 Running 0 2d kube-system canal-trdqt 3/3 Running 0 2d kube-system canal-znmxq 3/3 Running 1 3h kube-system heapster-698c5f45bd-7xqc9 1/1 Running 0 5h kube-system kube-apiserver-k8s-master1 1/1 Running 13 4h kube-system kube-apiserver-k8s-master2 1/1 Running 0 5h kube-system kube-apiserver-k8s-master3 1/1 Running 0 2d kube-system kube-controller-manager-k8s-master1 1/1 Running 2 4h kube-system kube-controller-manager-k8s-master2 1/1 Running 0 5h kube-system kube-controller-manager-k8s-master3 1/1 Running 0 2d kube-system kube-dns-6f4fd4bdf-bjq2m 3/3 Running 0 5h kube-system kube-dns-6f4fd4bdf-bqxmc 3/3 Running 0 2d kube-system kube-proxy-5g4q7 1/1 Running 0 4h kube-system kube-proxy-c2fwf 1/1 Running 1 3h kube-system kube-proxy-j4fcd 1/1 Running 0 4h kube-system kube-proxy-jqbgr 1/1 Running 0 4h kube-system kube-proxy-wsqlr 1/1 Running 1 3h kube-system kube-scheduler-k8s-master1 1/1 Running 1 4h kube-system kube-scheduler-k8s-master2 1/1 Running 0 5h kube-system kube-scheduler-k8s-master3 1/1 Running 0 2d kube-system kubernetes-dashboard-54cc6684f5-7t6nx 1/1 Running 0 5h kube-system monitoring-grafana-5ffb49ff84-cb7b5 1/1 Running 0 5h kube-system monitoring-influxdb-5b77d47fdd-lw76t 1/1 Running 0 5h#keepalived配置,在master上重启keepalived systemctl restart keepalived#检查虚拟IP是否正常 ping -c 2 192.168.3.157#nginx负载均衡配置,在master上安装并启动nginx作为负载均衡docker-compose -f nginx-lb/docker-compose.yaml up -d#在master上验证负载均衡和keepalived是否成功curl -k https://192.168.3.157:16443#kube-proxy配置,在k8s-master1上设置proxy高可用，设置server指向高可用虚拟IP以及负载均衡的16443端口 kubectl edit -n kube-system configmap/kube-proxy server: https://192.168.3.157:16443#在master上重启proxy kubectl delete pod -n kube-system $(kubectl get pods --all-namespaces -o wide | grep kube-proxy|awk '{print $2}')#node节点加入高可用集群设置,kubeadm加入高可用集群#在所有worker节点上进行加入kubernetes集群操作，这里统一使用k8s-master1的apiserver地址来加入集群 kubeadm join --token xxx 192.168.3.148:6443 --discovery-token-ca-cert-hash sha256:xxxx#在所有worker节点上修改kubernetes集群设置，更改server为高可用虚拟IP以及负载均衡的16443端口 sed -i '/server:/s#https:.*#https://192.168.3.157:16443#g' /etc/kubernetes/bootstrap-kubelet.conf sed -i '/server:/s#https:.*#https://192.168.3.157:16443#g' /etc/kubernetes/kubelet.conf grep 192.168.3.157 /etc/kubernetes/*.conf [root@k8s-node1 ~]# grep 192.168.3.157 /etc/kubernetes/*.conf /etc/kubernetes/bootstrap-kubelet.conf: server: https://192.168.3.157:16443 /etc/kubernetes/kubelet.conf: server: https://192.168.3.157:16443 systemctl restart docker kubelet#设置workers的节点标签 kubectl label nodes k8s-node1 role=worker kubectl label nodes k8s-node2 role=worker kubectl label nodes k8s-node3 role=worker 4.总结总体部署下来步骤太多，后面写了一个部署脚本，只要写上节点IP就能一件部署，不过前期要做好资源本地化，就是要把准备好的工具，组件等放到内网，保证下载资源各方面都没问题，减少外部网络的影响。后面整理好再放到github上面。","link":"/2020/03/25/k8s/K8S-5-%20High-availability-cluster-deployment/"},{"title":"k8s专题[6.harbor私有仓库部署]","text":"1.私有仓库版本镜像仓库版本： Harbor v1.7.5 Clair v2.0.7（漏洞库{+}https://github.com/coreos/clair+） 2.部署步骤 内网组件资源要事先准备好 2.1.安装docker 服务 yum -y install http://192.168.3.146/docker-ce-cli-18.09.0-3.el7.x86_64.rpm http://192.168.3.146/docker-ce-18.09.3-3.el7.x86_64.rpm http://192.168.3.146/containerd.io-1.2.4-3.1.el7.x86_64.rpm 2.2.安装docker-docker-compose wget http://192.168.3.146/docker-compose mv docker-compose /usr/local/bin/docker-compose &amp;&amp; chmod 755 /usr/local/bin/docker-compose 2.3.部署harbor cd /usr/local/src &amp;&amp; wget http://192.168.3.146/harbor-offline-installer-v1.7.5.tgz &amp;&amp; tar -zxvf harbor-offline-installer-v1.7.5.tgz &amp;&amp; mv harbor /usr/local/ 修改harbor 配置文件 /usr/local/harbor/harbor.cfg，一般修改hostname为harbor的访问域名，ui_url_protocol为访问协议，设置成http，harbor_admin_password 设置harbor的admin账号默认密码，其他邮箱，认证模式可以部署完系统之后在系统配置页面修改。 执行安装命令:cd /usr/local/harbor &amp;&amp; sh install.sh –with-clair harbor启动命令:docker-compose -f docker-compose.clair.yml -f docker-compose.yml start 2.4.配置复制管理(记得做主从复制harbor的机器仓库的域名都要指向主harbor那一台，要不做镜像复制同步就有问题) 3.总结 记得做主从复制harbor的机器仓库的域名都要指向主harbor那一台，要不做镜像复制同步就有问题","link":"/2020/03/25/k8s/K8S-6-harbor/"},{"title":"k8s专题[7.harbor从1.7.5升级到1.9.0]","text":"升级步骤前文：harbor1.9新功能众多，包括tag 保留和配额、可与 CI/CD 工具集成的 Webhook 通知、数据复制、Syslog 集成以及 CVE 例外策略等安全功能。harbor在1.8版本改变较大，因此需要分两步进行升级，升级到v1.8.0，再升级到v1.9.0。 准备工作：1.下载harbor1.8.0和1.9.0版本的离线安装包 wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz wget https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.0.tgz 2.创建文件备份的目录 mkdir /data_backup1 mkdir /data_backup2 3.从docker hub拉取镜像 docker pull goharbor/harbor-migrator:v1.8.0 docker pull goharbor/harbor-migrator:v1.9.0 一、harbor1.7.5升级到1.8.01.关闭harbor cd /usr/local/harbor docker-compose down 2.数据备份 mv /usr/local/harbor /data_backup1/ 3.解压离线安装包 tar xf harbor-offline-installer-v1.8.0.tgz -C /usr/local/ 4.升级harbor配置文件 docker run -it –rm -v /data_backup1/harbor/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg -v /usr/local/harbor/harbor.yml:/harbor-migration/harbor-cfg-out/harbor.yml goharbor/harbor-migrator:v1.8.0 –cfg up 由于1.8后的版本docker-compse已交由harbor.yml控制，因此需要转化为将cfg文件转化为yml文件 5.安装并启动 cd /usr/local/harbor sh install.sh –with-clair 看成执行成功就可以登录访问harbor了 6.测试 登录后看成版本是否为1.8.0，对应的镜像在不在 并尝试上传和拉取镜像，都成功则升级成功 二、harbor1.8.0升级到1.9.01.关闭harbor cd /usr/local/harbor docker-compose down 2.数据备份 mv /usr/local/harbor /data_backup2/ 3.解压离线安装包 tar xf harbor-offline-installer-v1.9.0.tgz -C /usr/local/ 4.升级harbor配置文件 ！！！由于从1.8版本开始后不再需要cfg文件，因此需要升级的配置文件是yml，而且在1.9版本中新加入了一个参数chart，所以需要在1.8的yml文件中添加该参数 网上升级到1.9以上分享的人不多，没有找到完全一致的解决方案，因此这个坑要特别注意 对应问题可见 https://github.com/goharbor/harbor/issues/9146 vim /data_backup2/harbor/harbor.yml ，在其中加入下面一段 chart: absolute_url: disabled docker run -it –rm -v /data_backup2/harbor/harbor.yml:/harbor-migration/harbor-cfg/harbor.yml -v /usr/local/harbor/harbor.yml:/harbor-migration/harbor-cfg-out/harbor.yml goharbor/harbor-migrator:v1.9.0 –cfg up 5.安装并启动 cd /usr/local/harbor sh install.sh –with-clair 看成执行成功就可以登录访问harbor了 6.测试 登录后看成版本是否为1.9.0，对应的镜像在不在 成功的版本界面如下图 总结 注意尝试上传和拉取镜像，都成功则升级成功 稳定运行一段时间，将旧版本的备份数据清理","link":"/2020/03/25/k8s/K8S-7-harbor-update-to-1.9.0/"},{"title":"k8s专题[8.spinnaker基本介绍]","text":"1.概念Spinnaker 是 Netflix 的开源项目，是一个持续交付平台，它定位于将产品快速且持续的部署到多种云平台上。Spinnaker 通过将发布和各个云平台解耦，来将部署流程流水线化，从而降低平台迁移或多云品台部署应用的复杂度，它本身内部支持 Google、AWS EC2、Microsoft Azure、Kubernetes和OpenStack 等云平台，并且它可以无缝集成其他持续集成（CI）流程，如 git、Jenkins、Travis CI、Docker registry、cron 调度器等。简而言之，Spinnaker是致力于提供在多种平台上实现开箱即用的集群管理和部署功能的平台。 2.功能2.1：集群管理主要用于管理云上的资源，它分为以下几个块 Server Group：服务组，是资源管理单位，识别可部署组件和基础配置设置，它并且关联了一个负载均衡器和安全组，当部署完毕后，服务组就相当于一组运行中的软件实例集合，如（VM 实例，Kubernetes pods）。 Cluster：集群，由用户定义的，对服务组的逻辑分组。 Applications：应用，是对集群的逻辑分组。 Load Balancer：负载均衡，用于将外部网络流量重定向到服务组中的机器实例，还可以指定一系列规则，用来对服务组中的机器实例做健康监测。 Security Group：安全组，定义了网络访问权限，由IP、端口和通信协议组成的防火墙 2.2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分 管道 部署管理的核心是管道，在Spinnaker的定义中，管道由一系列的阶段（stages）组成。管道可以人工触发，也可以配置为自动触发，比如由 Jenkins Job 完成时、Docker Images 上传到仓库时，CRON 定时器、其他管道中的某一阶段。同时，管道可以配置参数和通知，可以在管道一些阶段上执行时发送邮件消息。Spinnaker 已经内置了一些阶段，如执行自定义脚本、触发 Jenkins 任务等。 阶段 阶段在 Spinnaker 中，可以作为管道的一个自动构建模块的功能组成。我们可以随意在管道中定义各个阶段执行顺序。Spinnaker 提供了很多阶段供我们选择使用，比如执行发布（Deploy）、执行自定义脚本 (script)、触发 Jenkins 任务 (jenkins)等，功能很强大。 部署策略 Spinnaker 支持精细的部署策略，比如 红/黑（蓝/绿）部署，多阶段环境部署，滚动红/黑策略，canary 发布等。用户可以为每个环境使用不同部署策略，比如，测试环境可以使用红/黑策略，生产环境使用滚动红/黑策略，它封装好了必须的步骤，用户不需要复杂操作，就可以实现企业级上线。 3.Spinnaker 架构所依赖的各个组件 Deck：面向用户 UI 界面组件，提供直观简介的操作界面，可视化操作发布部署流程。 API： 面向调用 API 组件，我们可以不使用提供的 UI，直接调用 API 操作，由它后台帮我们执行发布等任务。 Gate：是 API 的网关组件，可以理解为代理，所有请求由其代理转发。 Rosco：是构建 beta 镜像的组件，需要配置 Packer 组件使用。 Orca：是核心流程引擎组件，用来管理流程。 Igor：是用来集成其他 CI 系统组件，如 Jenkins 等一个组件。 Echo：是通知系统组件，发送邮件等信息。 Front50：是存储管理组件，需要配置 Redis、Cassandra 等组件使用。 Cloud driver 是它用来适配不同的云平台的组件，比如 Kubernetes，Google、AWS EC2、Microsoft Azure 等。 Fiat 是鉴权的组件，配置权限管理，支持 OAuth、SAML、LDAP、GitHub teams、Azure groups、 Google Groups 等。 各组件监听端口: 组件 端口 依赖组件 端口 Clouddriver 7002 Redis 6379 Fiat 7003 Front50 8080 minio 9000 Orca 8083 Gate 8084 Rosco 8087 Igor 8088 Echo 8089 Deck 80 以上组件除了核心组件外，一些组价可选择配置是否启动，比如不做权限管理的话，Fiat 就可以不启动，不集成其他 CI 的话，那就可以不启动 Igor 组件等。这些都可以在配置文件中配置，各个组件独立服务运行，有各自的服务端口，且各个组件都有自己的独立的项目 GitHub 地址","link":"/2020/03/26/k8s/K8S-8-spinnaker/"},{"title":"k8s专题[9.基于Jenkins和Spinnaker的CI/CD流程]","text":"流程图 流程说明:1.用户向Gitlab提交代码，代码中包含Dockerfile2.将代码提交到远程仓库3.Gitlab提交触发Jenkins自动构建4.Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库5.更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息(不是新业务不用更改此配置)6.Jenkins构建完成可触发Spinnaker的自动发布流程，或者手动触发spinnaker发布。7.Spinnaker会根据发布流程更新kubernetes YAML配置文件8.Spinnaker调用kubernetes的API，部署应用","link":"/2020/03/26/k8s/K8S-9-Jenkins-Spinnaker-CICD/"},{"title":"k8s常见问题解决","text":"1.基础镜像制作规范和使用1.1 时区修改官网下载centos镜像默认时区是国外的时区，需要把它修改成上海时区。运维提供提一个基础的镜像给业务部门下载使用。 1.2 规范docker运行程序的用户基于安全和规范的考虑，docker统一使用一个uid为1001的，用户名为pub的普通账号运行程序，需要在镜像和容器的宿主机上面同时新建。这个也是运维在初始化基础镜像和容器宿主机上面统一创建。 1.3 规范每个容器的cpu和内存(每个容器分配1核2G内存,视情况而定) 2.镜像下载策略2.1 默认的镜像拉取策略是“IfNotPresent”，在镜像已经存在的情况下，kubelet将不在去拉取镜像。 如果总是想要拉取镜像，必须设置拉取策略为“Always”或者设置镜像标签为“:latest”。 如果没有指定镜像的标签，它会被假定为“:latest”,同时拉取策略为“Always”。 3.增加k8s各个命名空间的服务账号 default 的自定义权限(RABC)首先，了解下什么是RABC： 基于角色的访问控制（Role-Based Access Control, 即”RBAC”）使用”rbac.authorization.k8s.io” API Group实现授权决策，允许管理员通过Kubernetes API动态配置策略。 在RBAC API中，一个角色包含了一套表示一组权限的规则。 权限以纯粹的累加形式累积（没有”否定”的规则）。 角色可以由命名空间（namespace）内的Role对象定义，而整个Kubernetes集群范围内有效的角色则通过ClusterRole对象实现 比如授予default命名空间的default ServiceAccount账号能够”get”, “watch”, “list”, “patch” default 命名空间里面的pod资源 3.1 定义role，一个Role对象只能用于授予对某一单一命名空间中资源的访问权限。 以下示例描述了”default”命名空间中的一个Role对象的定义，用于授予对pod的读访问权限： 123456789kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: default name: pod-readerrules:- apiGroups: [\"\"] # 空字符串\"\"表明使用core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\", \"patch\"] 3.2 RoleBinding，角色绑定将一个角色中定义的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即subject, 包括用户——User、用户组——Group、或者服务账户——Service Account）以及对被授予角色的引用。 在命名空间中可以通过RoleBinding对象授予权限，而集群范围的权限授予则通过ClusterRoleBinding对象完成 12345678910111213kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: read-pods namespace: defaultsubjects:- kind: ServiceAccount name: default apiGroup: \"\"roleRef: kind: Role name: pod-reader apiGroup: \"\" 4.docker使用http代理上传或者下载镜像1.在里面系统shell里面设置的代理docker服务无法获取信息，需要在docker服务启动里面写代理信息。 默认情况下这个配置文件夹并不存在，我们要创建它。 123456789101112131415161718192021222324$ mkdir -p /etc/systemd/system/docker.service.d#创建一个文件 /etc/systemd/system/docker.service.d/http-proxy.conf#包含 HTTP_PROXY 环境变量:[Service]\"HTTP_PROXY=http://username:pwd@192.168.34.216:3128/\" \"HTTPS_PROXY=http://username:pwd@192.168.34.216:3128/\"#有个注意的地方，如果直接写密码，docker无法识别，需要把密码进行url编码，注意，只需要把密码进行编码#编码地址:http://tool.chinaz.com/tools/urlencode.aspx#如果有局域网或者国内的registry，我们还需要使用 NO_PROXY 变量声明一下，比如测试环境的:[Service]Environment=\"HTTP_PROXY=http://pub:pwd@192.168.34.216:3128/\" \"HTTPS_PROXY=http://username:pwd@192.168.34.216:3128/\" \"NO_PROXY=localhost,127.0.0.1,registry.test.internal\"#刷新systemd配置:$ sudo systemctl daemon-reload#用系统命令验证环境变量加上去没:$ systemctl show --property=Environment dockerEnvironment=HTTP_PROXY=http://username:pwd@192.168.34.216:3128/ HTTPS_PROXY=http://username:pwd@192.168.34.216:3128/ NO_PROXY=localhost,127.0.0.1,registry.test.internal$ sudo systemctl restart docker 5.docker忽略https通信有时我们内网搭建的容器私有仓库没有使用https，但是docker通信默认是使用https的，这是怎么办呢？经过查询，docker可以自定义对某个私有仓库不使用https，只需要在docker服务启动的脚本上面修改：修改/usr/lib/systemd/system/docker.service(centos7配置路径)文件的启动命令ExecStart=/usr/bin/dockerd 改为 ExecStart=/usr/bin/dockerd –insecure-registry registry.test.internal*，如果有多个域名需要使用http通信，可使用–insecure-registry registry.xxx.xxx* 多个标记，然后reload配置和重启docker服务，命令如下： 1234#刷新systemd配置:$ sudo systemctl daemon-reload#重启docker服务：$ sudo systemctl restart docker 6.master节点故障迁移master节点的故障迁移简单就四点 第一步，把故障的节点从集群里面删掉 第二步，另外找一台安装好master组件，使用kubeadm join xxx –experimental-control-plane 即可 第三步，nginx-lb的nginx配置需要修改IP，然后重启nginx-lb 容器 第四步，把keepalive的配置迁移到新节点，启动keepalive即可(/etc/keepalived/keepalived.conf里面对应的IP需要修改)","link":"/2020/03/27/k8s/K8S-Common-problem/"},{"title":"利用LXCFS提升容器资源可见性","text":"!!!你好呀，本博客已经迁移至[青叶の博客]持续更新，如需访问，请点击:青叶の博客由于默认情况下容器挂载的是宿主机的硬件配置信息，导致有些应用根据这些信息来决定启动内存等的大小，导致应用内存溢出等问题。 LXCFS简介 社区中常见的做法是利用 lxcfs来提供容器中的资源可见性。lxcfs 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器，它也可以支持Docker容器。 LXCFS通过用户态文件系统，在容器中提供下列 procfs 的文件。 123456/proc/cpuinfo/proc/diskstats/proc/meminfo/proc/stat/proc/swaps/proc/uptime LXCFS的示意图如下： 比如，把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup中读取正确的内存限制。从而使得应用获得正确的资源约束设定。 安装lxcfs ，先安装需要使用的依赖包： yum install http://mirror.centos.org/centos/7/os/x86_64/Packages/fuse-libs-2.9.2-10.el7.x86_64.rpm 用deamonset方式在每个节点启动一个lxcfs,lxcfs-daemonset.yaml配置如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344apiVersion: apps/v1beta2kind: DaemonSetmetadata: name: lxcfs labels: app: lxcfsspec: selector: matchLabels: app: lxcfs template: metadata: labels: app: lxcfs spec: hostPID: true tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: lxcfs image: reg.test.sui.internal/library/lxcfs:2.0.8-1 imagePullPolicy: Always securityContext: privileged: true volumeMounts: - name: cgroup mountPath: /sys/fs/cgroup - name: lxcfs mountPath: /var/lib/lxcfs mountPropagation: Bidirectional - name: usr-local mountPath: /usr/local volumes: - name: cgroup hostPath: path: /sys/fs/cgroup - name: usr-local hostPath: path: /usr/local - name: lxcfs hostPath: path: /var/lib/lxcfs type: DirectoryOrCreate 然后在发版平台deploy的模板配置资源限制的信息，主要信息如下： 1234567891011121314151617181920212223242526272829303132333435363738394041volumeMounts: - mountPath: /proc/cpuinfo name: lxcfs-proc-cpuinfo - mountPath: /proc/meminfo name: lxcfs-proc-meminfo - mountPath: /proc/diskstats name: lxcfs-proc-diskstats - mountPath: /proc/stat name: lxcfs-proc-stat - mountPath: /proc/swaps name: lxcfs-proc-swaps - mountPath: /proc/uptime name: lxcfs-proc-uptime restartPolicy: Always imagePullSecrets: - name: pull-registry-secret volumes: - hostPath: path: /var/lib/lxcfs/proc/cpuinfo type: \"\" name: lxcfs-proc-cpuinfo - hostPath: path: /var/lib/lxcfs/proc/diskstats type: \"\" name: lxcfs-proc-diskstats - hostPath: path: /var/lib/lxcfs/proc/meminfo type: \"\" name: lxcfs-proc-meminfo - hostPath: path: /var/lib/lxcfs/proc/stat type: \"\" name: lxcfs-proc-stat - hostPath: path: /var/lib/lxcfs/proc/swaps type: \"\" name: lxcfs-proc-swaps - hostPath: path: /var/lib/lxcfs/proc/uptime type: \"\" name: lxcfs-proc-uptime 启动应用之后即可看到内存大小就是cgroup分配的内存大小,注意不要使用alpine镜像，这个镜像挂载仍然有问题。","link":"/2020/03/30/k8s/K8S-LXCFS/"},{"title":"k8s+dubbo架构集群内外网络通讯解决方案","text":"1.问题k8s有自己的一套网络管理机制，集群内的容器和容器之间是可以相互通信的。 但是在容器化升级改造的过程中，不可能一步到位的将所有的服务全部迁移到k8s的容器当中来，毕竟新的技术在没有经过实践趟坑时，肯定不能轻易的全面铺开升级。 那么就涉及到集群外的服务访问集群内的服务，集群内容器中的ip都是k8s管理的IP，dubbo服务注册的也是获取的容器内分配的IP。 比如宿主机ip是10.201.7.xx,容器内的ip就是172.66.4.x。群外的和宿主主机同网段的服务通过拿到dubbo的注册的172.66.4.x也根本没法访问容器内的dubbo服务。 2.分析k8s是通过Service来暴露集群内的服务，假如dubbo服务注册的是Service暴露的端口和宿主的IP那么集群外的服务就可以直接访问集群内容器中的服务了。 该方案主要有两个难点： 1.如何获取Service暴露的端口和宿主机的IP注入到POD环境变量； 2.Dubbo服务如何从环境变量获取IP和端口注册到ZK； 关于难点1： 通过downward-api的方式向Pod内注入NodeIP的env； 通过给Pod注入env的方式将NodePort注入到Pod内；（要求先创建Service） 关于难点2： Dubbo在启动阶段提供两对系统属性，用于设置外部通信的IP和端口地址。 DUBBO_IP_TO_REGISTRY — 注册到注册中心的IP地址DUBBO_PORT_TO_REGISTRY — 注册到注册中心的端口DUBBO_IP_TO_BIND — 监听IP地址DUBBO_PORT_TO_BIND — 监听端口 详见链接：https://www.jianshu.com/p/b045dbdb8e12 因此，将NodeIP和NodePort的变量名分别设置为DUBBO_IP_TO_REGISTRY和DUBBO_PORT_TO_REGISTRY Dubbo服务启动自动获取这两个变量，并注册到ZK，不需要修改任何代码； （备注：验证过程发现dubbo2.5.3版本不生效，dubbo2.6.0版本验证成功；2.5.3到2.6.0版本之间未验证） 3.实现过程Helm Chart文件templates/deployment.yaml 配置说明 DUBBO_PORT_TO_BIND：Dubbo服务默认启动端口为20880，该变量指定Dubbo服务监听端口；端口要与Service中NodePort相同，否则dubbo服务调用报错； DUBBO_PORT_TO_REGISTRY ：注册到ZK的端口 DUBBO_IP_TO_REGISTRY：注册到ZK的IP地址 Helm Chart文件values.yaml 配置说明 1.指定Service的类型为NodePort； 2.指定dubbo服务默认启动端口为8080； 发版过程Pipeline文件 步骤1：连接k8s集群，更新helm仓库； 步骤2：查询服务是否部署，如果未部署则部署（保证能获取到Service的NodePort）； 步骤3：查询对应服务的NodePort，并赋值给变量； 步骤4：更新服务，将获取到的NodePort注入POD系统变量。","link":"/2020/03/29/k8s/k8s-dubbo/"},{"title":"nginx-ingress-controller部署","text":"1.打标签由于nginx要部署到指定节点上，所以需要对node打个label，通过nodeSelector调度到节点上，同时要确保部署节点主机的80、443、18080、10254端口没有被占用。kubectl label node ss-1-centos221 nginx-ingress=nginxkubectl label node ss-1-centos221-2 nginx-ingress=nginx 2.准备nginx-ingress-controller部署文件2.1 nginx-ingress-controller-rbac.yaml ：设置rabc权限123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126---apiVersion: v1kind: ServiceAccountmetadata: name: nginx-ingress-serviceaccount namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: nginx-ingress-clusterrolerules: - apiGroups: - \"\" resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - \"\" resources: - nodes verbs: - get - apiGroups: - \"\" resources: - services verbs: - get - list - watch - apiGroups: - \"extensions\" resources: - ingresses verbs: - get - list - watch - apiGroups: - \"\" resources: - events verbs: - create - patch - apiGroups: - \"extensions\" resources: - ingresses/status verbs: - update---apiVersion: rbac.authorization.k8s.io/v1beta1kind: Rolemetadata: name: nginx-ingress-role namespace: kube-systemrules: - apiGroups: - \"\" resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - \"\" resources: - configmaps resourceNames: # Defaults to \"&lt;election-id&gt;-&lt;ingress-class&gt;\" # Here: \"&lt;ingress-controller-leader&gt;-&lt;nginx&gt;\" # This has to be adapted if you change either parameter # when launching the nginx-ingress-controller. - \"ingress-controller-leader-nginx\" verbs: - get - update - apiGroups: - \"\" resources: - configmaps verbs: - create - apiGroups: - \"\" resources: - endpoints verbs: - get---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: nginx-ingress-role-nisa-binding namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: nginx-ingress-rolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: nginx-ingress-clusterrole-nisa-bindingroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-ingress-clusterrolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: kube-system 2.2 nginx-ingress-controller-cm.yaml ：设置配置文件1234567891011121314151617181920212223242526272829---kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: kube-system labels: app: ingress-nginxdata: enable-vts-status: \"true\" vts-default-filter-key: \"$server_name\" proxy-body-size: 20m upstream-keepalive-connections: \"300\" access-log-path: \"/var/log/nginx/access.log\" error-log-path: \"/var/log/nginx/error.log\"---kind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: kube-systemdata: 1068: \"default/example-service-nodeport:80\"---kind: ConfigMapapiVersion: v1metadata: name: udp-services namespace: kube-system 2.3 nginx-ingress-controller-ds.yaml ：设置nginx-ingress-controller启动配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: nginx-ingress-controller namespace: kube-system spec: selector: matchLabels: app: ingress-nginx template: metadata: labels: app: ingress-nginx annotations: prometheus.io/port: '10254' prometheus.io/scrape: 'true' spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true dnsPolicy: ClusterFirstWithHostNet volumes: - name: nginx-log hostPath: path: /var/log/nginx - name: host-time hostPath: path: /etc/localtime containers: - name: nginx-ingress-controller image:k8s.qingye.info/test/nginx-ingress-controller:0.15.0 args: - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --annotations-prefix=nginx.ingress.kubernetes.io env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 hostPort: 80 - name: https containerPort: 443 hostPort: 443 - name: tcp containerPort: 18080 hostPort: 18080 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 volumeMounts: - name: nginx-log mountPath: /var/log/nginx/ - name: host-time mountPath: /etc/localtime tolerations: - key: \"node-role.kubernetes.io/node\" operator: \"Equal\" value: \"\" effect: \"NoSchedule\" nodeSelector: nginx-ingress: \"nginx\"---kind: ServiceapiVersion: v1metadata: name: nginx-ingress-controller-service namespace: kube-systemspec: selector: app: ingress-nginx ports: - protocol: TCP port: 80 name: http - protocol: TCP port: 443 name: https 2.4 default-http-backend.yaml ：设置默认后端12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: default-http-backend labels: app: default-http-backend namespace: kube-systemspec: replicas: 1 selector: matchLabels: app: default-http-backend template: metadata: labels: app: default-http-backend spec: terminationGracePeriodSeconds: 60 containers: - name: default-http-backend # Any image is permissible as long as: # 1. It serves a 404 page at / # 2. It serves 200 on a /healthz endpoint image:k8s.qingye.info/test/defaultbackend:1.4 livenessProbe: httpGet: path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 30 timeoutSeconds: 5 ports: - containerPort: 8080 resources: limits: cpu: 10m memory: 20Mi requests: cpu: 10m memory: 20Mi---apiVersion: v1kind: Servicemetadata: name: default-http-backend namespace: kube-system labels: app: default-http-backendspec: ports: - port: 80 targetPort: 8080 selector: app: default-http-backend 3.开始安装kubectl create -f nginx-ingress-controller-rbac.yaml kubectl create -f nginx-ingress-controller-cm.yaml kubectl create -f nginx-ingress-controller-ds.yaml kubectl create -f default-http-backend.yaml 4.安装完毕后，看下有没成功启动kubectl get pod -n kube-system |egrep “backend|nginx” 启动成功。可以访问管理页面查看流量状态 http://ip:18080/nginx_status 或者http://ip:18080/nginx_status 5.配置ingres，以下是一个例子123456789101112131415161718apiVersion: extensions/v1beta1kind: Ingressmetadata: name: nginx-ingress-scope namespace: kube-system annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/vts-filter-key: $uri $server_name prometheus.io/probe: 'true'spec: rules: - host: k8s.scope.qingye.cn http: paths: - path: / backend: serviceName: weave-scope-app servicePort: 80","link":"/2020/03/30/k8s/nginx-ingress-controller/"},{"title":"k8s集群更新证书","text":"(版权声明：本文转载博客园「aguncn」的文章。原文链接) 1.问题起源kubeadm 是 kubernetes 提供的一个初始化集群的工具，使用起来非常方便。但是它创建的apiserver、controller-manager等证书默认只有一年的有效期，同时kubelet 证书也只有一年有效期，一年之后 kubernetes 将停止服务。官方推荐一年之内至少用 kubeadm upgrade 更新一次 kubernetes 系统，更新时也会自动更新证书。不过，在产线环境或者无法连接外网的环境频繁更新 kubernetes 不太现实。我们可以在过期之前或之后，使用kubeadm alpha phase里的certs和kubeconfig命令，同时配合kubelet证书自动轮换机制来解决这个问题。 2.k8s里面的证书详解 Kubernetes 集群根证书 /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.key 由此根证书签发的证书有: 1.kube-apiserver 组件持有的服务端证书 /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.key 2.kubelet 组件持有的客户端证书,kubelet 上一般不会明确指定服务端证书, 而是只指定 ca 根证书, 让 kubelet 根据本地主机信息自动生成服务端证书并保存到配置的cert-dir文件夹中。 /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.key 汇聚层(aggregator)证书 /etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/front-proxy-ca.key 由此根证书签发的证书只有一组: 1.代理端使用的客户端证书, 用作代用户与 kube-apiserver 认证 /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.key etcd 集群根证书 /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key 由此根证书签发机构签发的证书有: 1.etcd server 持有的服务端证书 /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/server.key 2.peer 集群中节点互相通信使用的客户端证书 /etc/kubernetes/pki/etcd/peer.crt /etc/kubernetes/pki/etcd/peer.key 3.pod 中定义 Liveness 探针使用的客户端证书 /etc/kubernetes/pki/etcd/healthcheck-client.crt /etc/kubernetes/pki/etcd/healthcheck-client.key 4.配置在 kube-apiserver 中用来与 etcd server 做双向认证的客户端证书 /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.key Serveice Account秘钥 这组的密钥对儿仅提供给 kube-controller-manager 使用. kube-controller-manager 通过 sa.key 对 token 进行签名, master 节点通过公钥 sa.pub 进行签名的验证. API Server的authenticating环节支持多种身份校验方式：client cert、bearer token、static password auth等，这些方式中有一种方式通过authenticating（Kubernetes API Server会逐个方式尝试），那么身份校验就会通过。一旦API Server发现client发起的request使用的是service account token的方式，API Server就会自动采用signed bearer token方式进行身份校验。而request就会使用携带的service account token参与验证。该token是API Server在创建service account时用API server启动参数：–service-account-key-file的值签署(sign)生成的。如果–service-account-key-file未传入任何值，那么将默认使用–tls-private-key-file的值，即API Server的私钥（server.key）。 通过authenticating后，API Server将根据Pod username所在的group：system:serviceaccounts和system:serviceaccounts:(NAMESPACE)的权限对其进行authority 和admission control两个环节的处理。在这两个环节中，cluster管理员可以对service account的权限进行细化设置。 /etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub kubeadm 创建的集群, kube-proxy ,flannel,coreDNS是以 pod 形式运行的, 在 pod 中, 直接使用 service account 与 kube-apiserver 进行认证, 此时就不需要再单独为 kube-proxy 创建证书. 3.Kubeadm本地读取集群配置正如默认的kubeadm 安装k8s集群时，会从外网拉取镜像。在kubeadm命令升级master证书时，它也会默认从网上读取一个stable.txt的文件。由于公司实际情况，这个问题得解决掉。解决这个问题的办法，就是生成一个集群配置的yaml文件，然后，在运行命令时指定这个Yaml文件即可。如何生居一个集群配置的yaml文件呢？命令如下：kubeadm config view &gt; cluster.yaml 4.重新生成master证书建议不要重新生成ca证书，因为更新了ca证书，集群节点就需要手工操作，才能让集群正常(会涉及重新join)。操作之前，先将/etc/kubernetes/pki下的证书文件，mv到其它文件夹，作个临时备份，不要删除。 12345678kubeadm alpha phase certs etcd-healthcheck-client --config cluster.yamlkubeadm alpha phase certs etcd-peer --config cluster.yamlkubeadm alpha phase certs etcd-server --config cluster.yamlkubeadm alpha phase certs front-proxy-client--config cluster.yamlkubeadm alpha phase certs apiserver-etcd-client --config cluster.yamlkubeadm alpha phase certs apiserver-kubelet-client --config cluster.yamlkubeadm alpha phase certs apiserver --config cluster.yamlkubeadm alpha phase certs sa --config cluster.yaml #千万别更换这个证书，有坑 5.重新生成kubeconfig配置文件在生成这些新的证书文件之后，再需要kubeadm alpha phase config命令，重新生成新的kubeconfig文件。操作之前，先将/etc/kubernetes/下的kubeconfig，mv到其它文件夹，作个临时备份，不要删除。kubeadm alpha phase kubeconfig all –config cluster.yaml所有的kubeconfig重新生成以后，替换到kubectl使用的config文件之后（默认位置为~.kube/config），即可正常操作kubectl命令了。 6.Kubelet证书自动轮换kubelet证书分为server和client两种， k8s 1.10默认启用了client证书的自动轮换，但server证书自动轮换需要用户开启。 7.Service Account密钥更新由于service account的密钥是以rsa密钥对形式生成，所以没有过期时间。如无必要，千万不要生成重新生成sa密钥。因为sa密钥关联到一切系统pod内的进程访问api server时的认证。如果更新了sa，则需要先重新生成这些pod加截的token，再删除这些pod之后，重新加载token文件。经过测试，这些系统级pod包括但不限于kube-proxy,flannel,kubenetes-dashboard, kube-stat-metricst等所有用到sa认证的pod(我也是因为重新生成了这个证书，导致kube-proxy无法访问k8s api ,好久才找出来是这个原因) 8.其他问题之前我用新的ca根证书重新生成一套证书，替换后，master节点都没问题了，但是node节点无法和master节点通讯，想接入新的节点也加入不了，也是一个坑。","link":"/2020/03/21/k8s/K8S-UPDATE-CERTS/"}],"tags":[{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"tengine","slug":"tengine","link":"/tags/tengine/"},{"name":"http2","slug":"http2","link":"/tags/http2/"},{"name":"openssl","slug":"openssl","link":"/tags/openssl/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":"http","slug":"http","link":"/tags/http/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"cross-domain","slug":"cross-domain","link":"/tags/cross-domain/"},{"name":"跨域","slug":"跨域","link":"/tags/%E8%B7%A8%E5%9F%9F/"},{"name":"glusterfs","slug":"glusterfs","link":"/tags/glusterfs/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"容器","slug":"容器","link":"/tags/%E5%AE%B9%E5%99%A8/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"ntp","slug":"ntp","link":"/tags/ntp/"},{"name":"yum源","slug":"yum源","link":"/tags/yum%E6%BA%90/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"docker镜像","slug":"docker镜像","link":"/tags/docker%E9%95%9C%E5%83%8F/"},{"name":"RESTful API","slug":"RESTful-API","link":"/tags/RESTful-API/"},{"name":"spinnaker","slug":"spinnaker","link":"/tags/spinnaker/"},{"name":"harbor","slug":"harbor","link":"/tags/harbor/"},{"name":"镜像仓库","slug":"镜像仓库","link":"/tags/%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93/"},{"name":"Jenkins","slug":"Jenkins","link":"/tags/Jenkins/"}],"categories":[{"name":"Tengine","slug":"Tengine","link":"/categories/Tengine/"},{"name":"http","slug":"http","link":"/categories/http/"},{"name":"glusterfs","slug":"glusterfs","link":"/categories/glusterfs/"},{"name":"k8s","slug":"k8s","link":"/categories/k8s/"},{"name":"ntp","slug":"ntp","link":"/categories/ntp/"},{"name":"yum","slug":"yum","link":"/categories/yum/"},{"name":"rkhunter","slug":"rkhunter","link":"/categories/rkhunter/"},{"name":"k8s专题","slug":"k8s专题","link":"/categories/k8s%E4%B8%93%E9%A2%98/"}]}