{"pages":[{"title":"","text":"个人信息: 青叶: Do Something… 职业: 运维工程师 目标: 人管代码，代码管机器 城市: 广东深圳 E-mail: qingyeinfo@gmail.com 关于本站: 本站搭建于2019-11-05，是青叶的个人博客。 本站主要总结记录个人技术上面的经验，也会借鉴技术大佬的文笔。 关于版权: 写作不易,若须转载本站文章请注明出处,多谢。 PS：若发现本站有任何侵犯你的利益与版权隐私的内容，请联系博主，我将会在第一时间作调整。","link":"/about/index.html"},{"title":"","text":"[{\"tag\":\"nginx\",\"url\":\"/categories/nginx/\",\"class\":\"is-success\"},{\"tag\":\"linux\",\"url\":\"/tags/linux/\",\"class\":\"is-danger\"},{\"tag\":\"查看更多\",\"url\":\"/tags/\",\"class\":\"is-white\"}]","link":"/json-data/hot-tags.json"},{"title":"","text":"[{\"img\":\"./json-data/banner/shell.jpg\",\"url\":\"http://www.qingye.info/tags/nginx/\",\"alt\":\"linux技术文档\"},{\"img\":\"./json-data/banner/linux-banner.jpg\",\"url\":\"http://www.qingye.info/tags/nginx/\",\"alt\":\"linux技术文档\"},{\"img\":\"./json-data/banner/nginx-banner.jpg\",\"url\":\"http://www.qingye.info/tags/nginx/\",\"alt\":\"nginx技术文档\"},{\"img\":\"./json-data/banner/cloud.png\",\"url\":\"http://www.qingye.info/tags/nginx/\",\"alt\":\"linux技术文档\"}]","link":"/json-data/main-banner.json"}],"posts":[{"title":"GlusterFS集群文件系统专题一(基础原理)","text":"(版权声明：本文为CSDN博主「刘爱贵」的原创文章。原文链接) 1.概述GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBand RDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。GlusterFS基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。 GlusterFS支持运行在任何标准IP网络上标准应用程序的标准客户端，如图2所示，用户可以在全局统一的命名空间中使用NFS/CIFS等标准协议来访问应用数据。GlusterFS使得用户可摆脱原有的独立、高成本的封闭存储系统，能够利用普通廉价的存储设备来部署可集中管理、横向扩展、虚拟化的存储池，存储容量可扩展至TB/PB级。GlusterFS主要特征如下： 扩展性和高性能 GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（Elastic Hash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。 高可用性 GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至是在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。GlusterFS没有设计自己的私有数据文件格式，而是采用操作系统中主流标准的磁盘文件系统（如EXT3、ZFS）来存储文件，因此数据可以使用各种标准工具进行复制和访问。 全局统一命名空间 全局统一命名空间将磁盘和内存资源聚集成一个单一的虚拟存储池，对上层用户和应用屏蔽了底层的物理硬件。存储资源可以根据需要在虚拟存储池中进行弹性扩展，比如扩容或收缩。当存储虚拟机映像时，存储的虚拟映像文件没有数量限制，成千虚拟机均通过单一挂载点进行数据共享。虚拟机I/O可在命名空间内的所有服务器上自动进行负载均衡，消除了SAN环境中经常发生的访问热点和性能瓶颈问题。 一致性哈希算法 GlusterFS采用弹性哈希算法在存储池中定位数据，而不是采用集中式或分布式元数据服务器索引。在其他的Scale-Out存储系统中，元数据服务器通常会导致I/O性能瓶颈和单点故障问题。GlusterFS中，所有在Scale-Out存储配置中的存储系统都可以智能地定位任意数据分片，不需要查看索引或者向其他服务器查询。这种设计机制完全并行化了数据访问，实现了真正的线性性能扩展。 弹性卷管理 数据储存在逻辑卷中，逻辑卷可以从虚拟化的物理存储池进行独立逻辑划分而得到。存储服务器可以在线进行增加和移除，不会导致应用中断。逻辑卷可以在所有配置服务器中增长和缩减，可以在不同服务器迁移进行容量均衡，或者增加和移除系统，这些操作都可在线进行。文件系统配置更改也可以实时在线进行并应用，从而可以适应工作负载条件变化或在线性能调优。 工作于标准协议 Gluster存储服务支持NFS, CIFS, HTTP, FTP以及Gluster原生协议，完全与POSIX标准兼容。现有应用程序不需要作任何修改或使用专用API，就可以对Gluster中的数据进行访问。这在公有云环境中部署Gluster时非常有用，Gluster对云服务提供商专用API进行抽象，然后提供标准POSIX接口。 2.设计目标GlusterFS的设计思想显著区别有现有并行/集群/分布式文件系统。如果GlusterFS在设计上没有本质性的突破，难以在与Lustre、PVFS2、Ceph等的竞争中占据优势，更别提与GPFS、StorNext、ISILON、IBRIX等具有多年技术沉淀和市场积累的商用文件系统竞争。其核心设计目标包括如下三个： 弹性存储系统（Elasticity） 存储系统具有弹性能力，意味着企业可以根据业务需要灵活地增加或缩减数据存储以及增删存储池中的资源，而不需要中断系统运行。GlusterFS设计目标之一就是弹性，允许动态增删数据卷、扩展或缩减数据卷、增删存储服务器等，不影响系统正常运行和业务服务。GlusterFS早期版本中弹性不足，部分管理工作需要中断服务，目前最新的3.1.X版本已经弹性十足，能够满足对存储系统弹性要求高的应用需求，尤其是对云存储服务系统而言意义更大。GlusterFS主要通过存储虚拟化技术和逻辑卷管理来实现这一设计目标。 线性横向扩展（Linear Scale-Out） 线性扩展对于存储系统而言是非常难以实现的，通常系统规模扩展与性能提升之间是LOG对数曲线关系，因为同时会产生相应负载而消耗了部分性能的提升。现在的很多并行/集群/分布式文件系统都具很高的扩展能力，Luster存储节点可以达到1000个以上，客户端数量能够达到25000以上，这个扩展能力是非常强大的，但是Lustre也不是线性扩展的。 纵向扩展（Scale-Up）旨在提高单个节点的存储容量或性能，往往存在理论上或物理上的各种限制，而无法满足存储需求。横向扩展（Scale-Out）通过增加存储节点来提升整个系统的容量或性能，这一扩展机制是目前的存储技术热点，能有效应对容量、性能等存储需求。目前的并行/集群/分布式文件系统大多都具备横向扩展能力。 GlusterFS是线性横向扩展架构，它通过横向扩展存储节点即可以获得线性的存储容量和性能的提升。因此，结合纵向扩展GlusterFS可以获得多维扩展能力，增加每个节点的磁盘可增加存储容量，增加存储节点可以提高性能，从而将更多磁盘、内存、I/O资源聚集成更大容量、更高性能的虚拟存储池。GlusterFS利用三种基本技术来获得线性横向扩展能力： 1)消除元数据服务 2)高效数据分布，获得扩展性和可靠性 3)通过完全分布式架构的并行化获得性能的最大化 可靠性（Reliability） 与GFS（Google File System）类似，GlusterFS可以构建在普通的服务器和存储设备之上，因此可靠性显得尤为关键。GlusterFS从设计之初就将可靠性纳入核心设计，采用了多种技术来实现这一设计目标。首先，它假设故障是正常事件，包括硬件、磁盘、网络故障以及管理员误操作造成的数据损坏等。GlusterFS设计支持自动复制和自动修复功能来保证数据可靠性，不需要管理员的干预。其次，GlusterFS利用了底层EXT3/ZFS等磁盘文件系统的日志功能来提供一定的数据可靠性，而没有自己重新发明轮子。再次，GlusterFS是无元数据服务器设计，不需要元数据的同步或者一致性维护，很大程度上降低了系统复杂性，不仅提高了性能，还大大提高了系统可靠性。 3.技术特点GlusterFS在技术实现上与传统存储系统或现有其他分布式文件系统有显著不同之处，主要体现在如下几个方面。 全软件实现（Software Only） GlusterFS认为存储是软件问题，不能够把用户局限于使用特定的供应商或硬件配置来解决。GlusterFS采用开放式设计，广泛支持工业标准的存储、网络和计算机设备，而非与定制化的专用硬件设备捆绑。对于商业客户，GlusterFS可以以虚拟装置的形式交付，也可以与虚拟机容器打包，或者是公有云中部署的映像。开源社区中，GlusterFS被大量部署在基于廉价闲置硬件的各种操作系统上，构成集中统一的虚拟存储资源池。简而言之，GlusterFS是开放的全软件实现，完全独立于硬件和操作系统。 完整的存储操作系统栈（Complete Storage Operating System Stack） GlusterFS不仅提供了一个分布式文件系统，而且还提供了许多其他重要的分布式功能，比如分布式内存管理、I/O调度、软RAID和自我修复等。GlusterFS汲取了微内核架构的经验教训，借鉴了GNU/Hurd操作系统的设计思想，在用户空间实现了完整的存储操作系统栈。 用户空间实现（User Space） 与传统的文件系统不同，GlusterFS在用户空间实现，这使得其安装和升级特别简便。另外，这也极大降低了普通用户基于源码修改GlusterFS的门槛，仅仅需要通用的C程序设计技能，而不需要特别的内核编程经验。 模块化堆栈式架构（Modular Stackable Architecture） GlusterFS采用模块化、堆栈式的架构，可通过灵活的配置支持高度定制化的应用环境，比如大文件存储、海量小文件存储、云存储、多传输协议应用等。每个功能以模块形式实现，然后以积木方式进行简单的组合，即可实现复杂的功能。比如，Replicate模块可实现RAID1，Stripe模块可实现RAID0，通过两者的组合可实现RAID10和RAID01，同时获得高性能和高可靠性。 原始数据格式存储（Data Stored in Native Formats） GlusterFS以原始数据格式（如EXT3、EXT4、XFS、ZFS）储存数据，并实现多种数据自动修复机制。因此，系统极具弹性，即使离线情形下文件也可以通过其他标准工具进行访问。如果用户需要从GlusterFS中迁移数据，不需要作任何修改仍然可以完全使用这些数据。 无元数据服务设计（No Metadata with the Elastic Hash Algorithm） 对Scale-Out存储系统而言，最大的挑战之一就是记录数据逻辑与物理位置的映像关系，即数据元数据，可能还包括诸如属性和访问权限等信息。传统分布式存储系统使用集中式或分布式元数据服务来维护元数据，集中式元数据服务会导致单点故障和性能瓶颈问题，而分布式元数据服务存在性能负载和元数据同步一致性问题。特别是对于海量小文件的应用，元数据问题是个非常大的挑战。 GlusterFS独特地采用无元数据服务的设计，取而代之使用算法来定位文件，元数据和数据没有分离而是一起存储。集群中的所有存储系统服务器都可以智能地对文件数据分片进行定位，仅仅根据文件名和路径并运用算法即可，而不需要查询索引或者其他服务器。这使得数据访问完全并行化，从而实现真正的线性性能扩展。无元数据服务器极大提高了GlusterFS的性能、可靠性和稳定性。 4.总体架构与设计 GlusterFS总体架构与组成部分如图2所示，它主要由存储服务器（Brick Server）、客户端以及NFS/Samba存储网关组成。不难发现，GlusterFS架构中没有元数据服务器组件，这是其最大的设计这点，对于提升整个系统的性能、可靠性和稳定性都有着决定性的意义。GlusterFS支持TCP/IP和InfiniBand RDMA高速网络互联，客户端可通过原生Glusterfs协议访问数据，其他没有运行GlusterFS客户端的终端可通过NFS/CIFS标准协议通过存储网关访问数据。 存储服务器主要提供基本的数据存储功能，最终的文件数据通过统一的调度策略分布在不同的存储服务器上。它们上面运行着Glusterfsd进行，负责处理来自其他组件的数据服务请求。如前所述，数据以原始格式直接存储在服务器的本地文件系统上，如EXT3、EXT4、XFS、ZFS等，运行服务时指定数据存储路径。多个存储服务器可以通过客户端或存储网关上的卷管理器组成集群，如Stripe（RAID0）、Replicate（RAID1）和DHT（分布式Hash）存储集群，也可利用嵌套组合构成更加复杂的集群，如RAID10。 由于没有了元数据服务器，客户端承担了更多的功能，包括数据卷管理、I/O调度、文件定位、数据缓存等功能。客户端上运行Glusterfs进程，它实际是Glusterfsd的符号链接，利用FUSE（File system in User Space）模块将GlusterFS挂载到本地文件系统之上，实现POSIX兼容的方式来访问系统数据。在最新的3.1.X版本中，客户端不再需要独立维护卷配置信息，改成自动从运行在网关上的glusterd弹性卷管理服务进行获取和更新，极大简化了卷管理。GlusterFS客户端负载相对传统分布式文件系统要高，包括CPU占用率和内存占用。 GlusterFS存储网关提供弹性卷管理和NFS/CIFS访问代理功能，其上运行Glusterd和Glusterfs进程，两者都是Glusterfsd符号链接。卷管理器负责逻辑卷的创建、删除、容量扩展与缩减、容量平滑等功能，并负责向客户端提供逻辑卷信息及主动更新通知功能等。GlusterFS 3.1.X实现了逻辑卷的弹性和自动化管理，不需要中断数据服务或上层应用业务。对于Windows客户端或没有安装GlusterFS的客户端，需要通过NFS/CIFS代理网关来访问，这时网关被配置成NFS或Samba服务器。相对原生客户端，网关在性能上要受到NFS/Samba的制约。 GlusterFS是模块化堆栈式的架构设计，如图3所示。模块称为Translator，是GlusterFS提供的一种强大机制，借助这种良好定义的接口可以高效简便地扩展文件系统的功能。服务端与客户端模块接口是兼容的，同一个translator可同时在两边加载。每个translator都是SO动态库，运行时根据配置动态加载。每个模块实现特定基本功能，GlusterFS中所有的功能都是通过translator实现，比如Cluster, Storage, Performance, Protocol, Features等，基本简单的模块可以通过堆栈式的组合来实现复杂的功能。这一设计思想借鉴了GNU/Hurd微内核的虚拟文件系统设计，可以把对外部系统的访问转换成目标系统的适当调用。大部分模块都运行在客户端，比如合成器、I/O调度器和性能优化等，服务端相对简单许多。客户端和存储服务器均有自己的存储栈，构成了一棵Translator功能树，应用了若干模块。模块化和堆栈式的架构设计，极大降低了系统设计复杂性，简化了系统的实现、升级以及系统维护。 5.弹性哈希算法对于分布式系统而言，元数据处理是决定系统扩展性、性能以及稳定性的关键。GlusterFS另辟蹊径，彻底摒弃了元数据服务，使用弹性哈希算法代替传统分布式文件系统中的集中或分布式元数据服务。这根本性解决了元数据这一难题，从而获得了接近线性的高扩展性，同时也提高了系统性能和可靠性。GlusterFS使用算法进行数据定位，集群中的任何服务器和客户端只需根据路径和文件名就可以对数据进行定位和读写访问。换句话说，GlusterFS不需要将元数据与数据进行分离，因为文件定位可独立并行化进行。GlusterFS中数据访问流程如下： 1、计算hash值，输入参数为文件路径和文件名； 2、根据hash值在集群中选择子卷（存储服务器），进行文件定位； 3、对所选择的子卷进行数据访问。 GlusterFS目前使用Davies-Meyer算法计算文件名hash值，获得一个32位整数。Davies-Meyer算法具有非常好的hash分布性，计算效率很高。假设逻辑卷中的存储服务器有N个，则32位整数空间被平均划分为N个连续子空间，每个空间分别映射到一个存储服务器。这样，计算得到的32位hash值就会被投射到一个存储服务器，即我们要选择的子卷。难道真是如此简单？现在让我们来考虑一下存储节点加入和删除、文件改名等情况，GlusterFS如何解决这些问题而具备弹性的呢？ 逻辑卷中加入一个新存储节点，如果不作其他任何处理，hash值映射空间将会发生变化，现有的文件目录可能会被重新定位到其他的存储服务器上，从而导致定位失败。解决问题的方法是对文件目录进行重新分布，把文件移动到正确的存储服务器上去，但这大大加重了系统负载，尤其是对于已经存储大量的数据的海量存储系统来说显然是不可行的。另一种方法是使用一致性哈希算法，修改新增节点及相邻节点的hash映射空间，仅需要移动相邻节点上的部分数据至新增节点，影响相对小了很多。然而，这又带来另外一个问题，即系统整体负载不均衡。GlusterFS没有采用上述两种方法，而是设计了更为弹性的算法。GlusterFS的哈希分布是以目录为基本单位的，文件的父目录利用扩展属性记录了子卷映射信息，其下面子文件目录在父目录所属存储服务器中进行分布。由于文件目录事先保存了分布信息，因此新增节点不会影响现有文件存储分布，它将从此后的新创建目录开始参与存储分布调度。这种设计，新增节点不需要移动任何文件，但是负载均衡没有平滑处理，老节点负载较重。GlusterFS在设计中考虑了这一问题，在新建文件时会优先考虑容量负载最轻的节点，在目标存储节点上创建文件链接直向真正存储文件的节点。另外，GlusterFS弹性卷管理工具可以在后台以人工方式来执行负载平滑，将进行文件移动和重新分布，此后所有存储服务器都会均会被调度。 GlusterFS目前对存储节点删除支持有限，还无法做到完全无人干预的程度。如果直接删除节点，那么所在存储服务器上的文件将无法浏览和访问，创建文件目录也会失败。当前人工解决方法有两个，一是将节点上的数据重新复制到GlusterFS中，二是使用新的节点来替换删除节点并保持原有数据。 如果一个文件被改名，显然hash算法将产生不同的值，非常可能会发生文件被定位到不同的存储服务器上，从而导致文件访问失败。采用数据移动的方法，对于大文件是很难在实时完成的。为了不影响性能和服务中断，GlusterFS采用了文件链接来解决文件重命名问题，在目标存储服务器上创建一个链接指向实际的存储服务器，访问时由系统解析并进行重定向。另外，后台同时进行文件迁移，成功后文件链接将被自动删除。对于文件移动也作类似处理，好处是前台操作可实时处理，物理数据迁移置于后台选择适当时机执行。 弹性哈希算法为文件分配逻辑卷，那么GlusterFS如何为逻辑卷分配物理卷呢？GlusterFS3.1.X实现了真正的弹性卷管理，如图4所示。存储卷是对底层硬件的抽象，可以根据需要进行扩容和缩减，以及在不同物理系统之间进行迁移。存储服务器可以在线增加和移除，并能在集群之间自动进行数据负载平衡，数据总是在线可用，没有应用中断。文件系统配置更新也可以在线执行，所作配置变动能够快速动态地在集群中传播，从而自动适应负载波动和性能调优。 弹性哈希算法本身并没有提供数据容错功能，GlusterFS使用镜像或复制来保证数据可用性，推荐使用镜像或3路复制。复制模式下，存储服务器使用同步写复制到其他的存储服务器，单个服务器故障完全对客户端透明。此外，GlusterFS没有对复制数量进行限制，读被分散到所有的镜像存储节点，可以提高读性能。弹性哈希算法分配文件到唯一的逻辑卷，而复制可以保证数据至少保存在两个不同存储节点，两者结合使得GlusterFS具备更高的弹性。 6.Translators如前所述，Translators是GlusterFS提供的一种强大文件系统功能扩展机制，这一设计思想借鉴于GNU/Hurd微内核操作系统。GlusterFS中所有的功能都通过Translator机制实现，运行时以动态库方式进行加载，服务端和客户端相互兼容。GlusterFS 3.1.X中，主要包括以下几类Translator： （1） Cluster：存储集群分布，目前有AFR, DHT, Stripe三种方式 （2） Debug：跟踪GlusterFS内部函数和系统调用 （3） Encryption：简单的数据加密实现 （4） Features：访问控制、锁、Mac兼容、静默、配额、只读、回收站等 （5） Mgmt：弹性卷管理 （6） Mount：FUSE接口实现 （7） Nfs：内部NFS服务器 （8） Performance：io-cache, io-threads, quick-read, read-ahead, stat-prefetch, sysmlink-cache, write-behind等性能优化 （9） Protocol：服务器和客户端协议实现 （10）Storage：底层文件系统POSIX接口实现 这里我们重点介绍一下Cluster Translators，它是实现GlusterFS集群存储的核心，它包括AFR（Automatic File Replication）、DHT（Distributed Hash Table）和Stripe三种类型。 AFR相当于RAID1，同一文件在多个存储节点上保留多份，主要用于实现高可用性以及数据自动修复。AFR所有子卷上具有相同的名字空间，查找文件时从第一个节点开始，直到搜索成功或最后节点搜索完毕。读数据时，AFR会把所有请求调度到所有存储节点，进行负载均衡以提高系统性能。写数据时，首先需要在所有锁服务器上对文件加锁，默认第一个节点为锁服务器，可以指定多个。然后，AFR以日志事件方式对所有服务器进行写数据操作，成功后删除日志并解锁。AFR会自动检测并修复同一文件的数据不一致性，它使用更改日志来确定好的数据副本。自动修复在文件目录首次访问时触发，如果是目录将在所有子卷上复制正确数据，如果文件不存则创建，文件信息不匹配则修复，日志指示更新则进行更新。 DHT即上面所介绍的弹性哈希算法，它采用hash方式进行数据分布，名字空间分布在所有节点上。查找文件时，通过弹性哈希算法进行，不依赖名字空间。但遍历文件目录时，则实现较为复杂和低效，需要搜索所有的存储节点。单一文件只会调度到唯一的存储节点，一旦文件被定位后，读写模式相对简单。DHT不具备容错能力，需要借助AFR实现高可用性, 如图5所示应用案例。 Stripe相当于RAID0，即分片存储，文件被划分成固定长度的数据分片以Round-Robin轮转方式存储在所有存储节点。Stripe所有存储节点组成完整的名字空间，查找文件时需要询问所有节点，这点非常低效。读写数据时，Stripe涉及全部分片存储节点，操作可以在多个节点之间并发执行，性能非常高。Stripe通常与AFR组合使用，构成RAID10/RAID01，同时获得高性能和高可用性，当然存储利用率会低于50%。 7. 设计讨论GlusterFS是一个具有高扩展性、高性能、高可用性、可横向扩展的弹性分布式文件系统，在架构设计上非常有特点，比如无元数据服务器设计、堆栈式架构等。然而，存储应用问题是很复杂的，GlusterFS也不可能满足所有的存储需求，设计实现上也一定有考虑不足之处，下面我们作简要分析。 元数据服务器 vs 元数据服务器 无元数据服务器设计的好处是没有单点故障和性能瓶颈问题，可提高系统扩展性、性能、可靠性和稳定性。对于海量小文件应用，这种设计能够有效解决元数据的难点问题。它的负面影响是，数据一致问题更加复杂，文件目录遍历操作效率低下，缺乏全局监控管理功能。同时也导致客户端承担了更多的职能，比如文件定位、名字空间缓存、逻辑卷视图维护等等，这些都增加了客户端的负载，占用相当的CPU和内存。 户空间 vs 内核空间 用户空间实现起来相对要简单许多，对开发者技能要求较低，运行相对安全。用户空间效率低，数据需要多次与内核空间交换，另外GlusterFS借助FUSE来实现标准文件系统接口，性能上又有所损耗。内核空间实现可以获得很高的数据吞吐量，缺点是实现和调试非常困难，程序出错经常会导致系统崩溃，安全性低。纵向扩展上，内核空间要优于用户空间，GlusterFS有横向扩展能力来弥补。 栈式 vs 非堆栈式 这有点像操作系统的微内核设计与单一内核设计之争。GlusterFS堆栈式设计思想源自GNU/Hurd微内核操作系统，具有很强的系统扩展能力，系统设计实现复杂性降低很多，基本功能模块的堆栈式组合就可以实现强大的功能。查看GlusterFS卷配置文件我们可以发现，translator功能树通常深达10层以上，一层一层进行调用，效率可见一斑。非堆栈式设计可看成类似Linux的单一内核设计，系统调用通过中断实现，非常高效。后者的问题是系统核心臃肿，实现和扩展复杂，出现问题调试困难。 始存储格式 vs 私有存储格式 usterFS使用原始格式存储文件或数据分片，可以直接使用各种标准的工具进行访问，数据互操作性好，迁移和数据管理非常方便。然而，数据安全成了问题，因为数据是以平凡的方式保存的，接触数据的人可以直接复制和查看。这对很多应用显然是不能接受的，比如云存储系统，用户特别关心数据安全，这也是影响公有云存储发展的一个重要原因。私有存储格式可以保证数据的安全性，即使泄露也是不可知的。GlusterFS要实现自己的私有格式，在设计实现和数据管理上相对复杂一些，也会对性能产生一定影响。 文件 vs 小文件 GlusterFS适合大文件还是小文件存储？弹性哈希算法和Stripe数据分布策略，移除了元数据依赖，优化了数据分布，提高数据访问并行性，能够大幅提高大文件存储的性能。对于小文件，无元数据服务设计解决了元数据的问题。但GlusterFS并没有在I/O方面作优化，在存储服务器底层文件系统上仍然是大量小文件，本地文件系统元数据访问是一个瓶颈，数据分布和并行性也无法充分发挥作用。因此，GlusterFS适合存储大文件，小文件性能较差，还存在很大优化空间。 高可用性 vs 存储利用率 GlusterFS使用复制技术来提供数据高可用性，复制数量没有限制，自动修复功能基于复制来实现。可用性与存储利用率是一个矛盾体，可用性高存储利用率就低，反之亦然。采用复制技术，存储利用率为1/复制数，镜像是50%，三路复制则只有33%。其实，可以有方法来同时提高可用性和存储利用率，比如RAID5的利用率是(n-1)/n，RAID6是(n-2)/n，而纠删码技术可以提供更高的存储利用率。但是，鱼和熊掌不可得兼，它们都会对性能产生较大影响。 另外，GlusterFS目前的代码实现不够好，系统不够稳定，BUGS数量相对还比较多。从其官方网站的部署情况来看，测试用户非常多，但是真正在生产环境中的应用较少，存储部署容量几TB－几十TB的占很大比率，数百TB－PB级案例非常少。这也可以从另一个方面说明，GlusterFS目前还不够稳定，需要更长的时间来检验。然而不可否认，GlusterFS是一个有着光明前景的集群文件系统，线性横向扩展能力使它具有天生的优势，尤其是对于云存储系统。 8.参考文献[1] Gluster: http://www.gluster.com/products/gluster-file-system-architecture-white-paper/ [2] Gluster: http://www.gluster.com/products/performance-in-a-gluster-system-white-paper/ [3] Gluster: http://gluster.com/community/documentation/index.php/Main_Page [4] GlusterFS-Design: http://edwyseguru.wordpress.com/2010/06/11/glusterfs-design/ [5] GlusterFS users: http://www.gluster.org/gluster-users/ [6] GlusterFS sources: http://download.gluster.com/pub/gluster/glusterfs/3.1/","link":"/2020/02/25/glusterfs-one/"},{"title":"GlusterFS集群文件系统专题二(主要术语)","text":"1.Trusted Storage Pool 一堆存储节点的集合 通过一个节点“邀请”其他节点创建，这里叫probe 成员可以动态加入，动态删除添加命令如下：node1# gluster peer probe node2删除命令如下：node1# gluster peer detach node3 2.Bricks Brick是一个节点和一个导出目录的集合，e.g. node1:/brick1 Brick是底层的RAID或磁盘经XFS或ext4文件系统格式化而来，所以继承了文件系统的限制 每个节点上的brick数是不限的 理想的状况是，一个集群的所有Brick大小都一样 3.Volumes Volume是brick的逻辑组合 创建时命名来识别 Volume是一个可挂载的目录 每个节点上的brick数是不变的,e.g.mount –t glusterfs www.std.com:test /mnt/gls 一个节点上的不同brick可以属于不同的卷 支持如下种类：a) 分布式卷b) 条带卷c) 复制卷d) 分布式复制卷e) 条带复制卷f) 分布式条带复制卷 3.1 分布式卷 文件分布存在不同的brick里 目录在每个brick里都可见 单个brick失效会带来数据丢失 无需额外元数据服务器 gluster是没有元数据服务器的，它定位文件和寻址都是通过哈希算法，这里使用的叫Davies-Meyer hash algorithm，可寻址空间为2^32次方，即0-4294967296，例如这里有四个节点，那么0-1073741824为node1的可寻址空间，1073741825-214748348为node2的可寻址空间，以此类推。访问一个文件时，通过文件名计算出一个地址，例如2142011129，属于1073741825-214748348，则将它存在node2中。 分布式卷内部的hash分布如下： 分布式卷的读写如下图所示: 3.2 复制卷 同步复制所有的目录和文件 节点故障时保持数据高可用 事务性操作，保持一致性 有changelog 副本数任意定 复制卷的读写如下图所示： 3.3条带卷 文件切分成一个个的chunk，存放于不同的brick上 只建议在非常大的文件时使用（比硬盘大小还大） Brick故障会导致数据丢失，建议和复制卷同时使用 Chunks are files with holes – this helps in maintaining offset consistency 3.3条带复制卷 数据将进行切片，切片在复本卷内进行复制，在不同卷间进行分布 3.3分布式复制卷 最常见的一种模式 读操作可以做到负载均衡 复本卷的组成依赖于指定brick的顺序，brick必须为复本数K的N倍,brick列表将以K个为一组，形成N个复本卷 3.4分布式条带复制卷 bricks数量为stripe个数N，和repl个数M的积N*M的整数倍 exp1 exp2 exp3 exp4组成一个分布卷，exp1和exp2组成一个stripe卷，exp3和exp4组成另一个stripe卷，1和2，3和4互为复本卷，exp4-exp8组成另一个分布卷 4.其他术语 Client:挂载了GFS卷的设备 Extended Attributes:xattr:是一个文件系统的特性,其支持用户或程序关联文件/目录和元数据。 FUSE:Filesystem Userspace,是一个可加载的内核模块，其支持非特权用户创建自己的文件系统而不需要修改内核代码,通过在用户空间运行文件系统的代码通过FUSE代码与内核进行桥接 Geo-Replication：异地备份，提供了一种持续，异步，增量数据备份策略，可以通过局域网，广域网，英特网来进行 GFID:GFS卷中的每个文件或目录都有一个唯一的128位的数据相关联，其用于模拟inode Namespace:每个Gluster卷都导出单个ns作为POSIX的挂载点 Node:一个拥有若干brick的设备 RDMA:远程直接内存访问,支持不通过双方的OS进行直接内存访问 RRDNS:round robin DNS是一种通过DNS轮转返回不同的设备以进行负载均衡的方法 Self-heal:用于后台运行检测复本卷中文件和目录的不一致性并解决这些不一致 Split-brain:脑裂 Volfile:glusterfs进程的配置文件,通常位于/var/lib/glusterd/vols/volname Volume:一组bricks的逻辑集合","link":"/2020/02/28/glusterfs-two/"},{"title":"ntp-server服务端部署和配置","text":"1.前言Atomic Clock: 现在计算时间最准确的是使用 原子震荡周期 所计算的物理时钟(Atomic Clock),因此也被定义为标准时间(International Atomic Time) UTC(coordinated Universal Time): 协和标准时间 就是利用 Atomic Clock 为基准定义出来的正确时间（又称世界统一时间，世界标准时间，国际协调时间）。 硬件时钟: 硬件时钟是指嵌在主板上的特殊的电路, 它的存在就是平时我们关机之后还可以计算时间的原因 系统时钟: 就是操作系统的kernel所用来计算时间的时钟. 它从1970年1月1日00:00:00 UTC时间到目前为止秒数总和的值 NTP（Network Time Protocol 网络时间协议）是一个用于同步计算机时钟的网络协议。它可以使计算机与其他服务器或时钟源进行时间同步，进行高精度的时间校正。 2.系统与软件版本2.1 系统版本CentOS6.5 x86_64 2.2 ntpd软件版本ntp-4.2.8p10.tar.gz 2.3 下载地址官方下载地址：http://support.ntp.org/bin/view/Main/SoftwareDownloads 3.编译安装3.1 安装依赖1yum install gcc gcc-c++ openssl-devel libstdc++* libcap* 3.2 备份相关目录123456789cp -ar /etc/ntp /etc/ntp.bakcp /etc/ntp.conf /etc/ntp.conf.bakcp /etc/init.d/ntpd /etc/init.d/ntpd.bakcp /etc/sysconfig/ntpd /etc/sysconfig/ntpd.bakcp /etc/sysconfig/ntpdate /etc/sysconfig/ntpdate.bak 3.3 卸载原来的ntp软件1yum erase ntp ntpdate 3.4 编译ntp1234567#创建/var/lib/ntp目录install -v -m710 -o ntp -g ntp -d /var/lib/ntp#解压tar -zxvf ntp-4.2.8p10.tar.gzcd ntp-4.2.8p10./configure --prefix=/usr --bindir=/usr/sbin --sysconfdir=/etc --enable-linuxcaps --with-lineeditlibs=readline --docdir=/usr/share/doc/ntp-4.2.8p10 --enable-all-clocks --enable-parse-clocks --enable-clockctlmake &amp;&amp; make install 编译参数解释： 1234567--bindir指定二进制文件的安装位置--enable-linuxcaps --enable-clockctl ntpd用ntp普通用户运行，使时钟控制功能不受root用户控制--enable-all-clocks --enable-parse-clocks 允许所有可解析的时钟--with-lineeditlibs=readline This switch enables Readline support for ntpdc and ntpq programs. （允许ntpdc和ntpq命令可以以readline模式运行） 3.5 创建配置文件123456789cp /etc/init.d/ntpd.bak /etc/init.d/ntpdcp /etc/sysconfig/ntpd.bak /etc/sysconfig/ntpdcp /etc/sysconfig/ntpdate.bak /etc/sysconfig/ntpdatemv /etc/ntp.bak /etc/ntpcp /etc/ntp.conf.bak /etc/ntp.conf 3.6 配置主配置文件/etc/ntp.confntp.conf主要参数详解: ntp.conf里主要可以使用如下几个命令：restrict，server，driftfile，keys 其中server是设定上级时间服务器用的，而restrict是设定哪台服务器可以和ntp server进行时间同步，具有什么样的权限。driftfile是用来指定记录时间差异的文件，keys是用来指定认证key文件的（这里不用）。 先来看restrict的格式为：restrict [客户端IP] mask [netmask_IP] [parameter] 客户端IP，就是都是哪几台服务器要和这台ntp server进行同步的ip地址最后的parameter可以有如下几个参数：ignore：拒绝连接到ntp servernomodiy：可以连接到ntp server，但是不能对ntp server进行时间上的修改noquery：不提供对ntp server查询时间，也就是拒绝和ntp server进行时间同步notrust：对没有认证的客户端不提供服务 notrap ：不提供trap远端登陆：拒绝为匹配的主机提供模式 6 控制消息陷阱服务。陷阱服务是 ntpdq 控制消息协议的子系统，用于远程事件日志记录程序。 nopeer ：用于阻止主机尝试与服务器对等，并允许欺诈性服务器控制时钟 kod ： 访问违规时发送 KoD 包。kod技术可以阻止“Kiss of Death”包（一种Dos攻击）对服务器的破坏，使用此参数开启该功能。 restrict default nomodify notrap noquery此项设置的含义是不允许其他计算机修改或查询配置在本机linux系统上的NTP服务。其中default表示所有IP. restrict 127.0.0.1开放本机内部回环网络接口，以便于在本地对NTP服务进行监控及配置。 #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap允许192.168.1.0网络段的NTP客户端使用本NTP服务器进行网络校进，但不允许它们修改本机的NTP服务配置。如想开放此项功能需要根据实际网络情况修改网络段及掩码，并把注释符”#”去掉。 server 0.pool.ntp.org 指定上层的NTP服务器。linux系统中默认指定Internet上的时间服务器池中的时间服务器作为上层NTP服务器。NTP服务器池pool.ntp.org中拥有三百多台自愿加入其中的公共NTP服务器。如果想更准确的校时也可以自己指定上层NTP服务器。 ‚server命令的格式是：server [IP or hostname] [prefer]其中[IP or hostname]为上级时间服务器的IP或者域名，主机名：可以是192.168.12.177形式，或者time.nist.gov再或者ntpserver这样的形式。后面的[prefer]参数是可选的，加上prefer后，ntp server和上级时间服务器同步时会优先先访问加了prefer这行的进行同步。 iburst：当一个运程NTP服务器不可用时，向它发送一系列的并发包进行检测。 server 127.127.1.0fudge 127.127.1.0 stratum 10127.127.1.0是一个特殊的地址，代表本机的系统时钟。fudge是指定本地时间源的层号，数字越大，优先级越低。所以当有外部时间源时会优先使用外部时间源。 driftfile /var/lib/ntp/drift指定记录与上层NTP服务器联系时所花费时间的文件，指定了用来保存系统时钟频率偏差的文件, ntpd程序使用它来自动地补偿时钟的自然漂移， 从而使时钟即使在切断了外来时源的情况下，仍能保持相当的准确度。 还可以在/etc/ntp.conf文件中还可以进行如下设置peer 192.168.16.100 #设置IP地址为192.168.16.100的NTP服务器可以与本机的NTP服务器相互进行网络校时broadcast 224.0.1.1 #224.0.1.1是多播网址,设置该NTP服务器可对所有能访问到的网段进行多播broadcast 192.168.1.255 #设定该NTP服务器可对子网192.168.1.0/24中的所有计算机定期广播正确的时间 为了让本机的NTP服务器能够到指定的时间源那进行同步，还必须修改/etc/ntp/step-tickers文件,在该文件中把所用的上层NTP服务器的IP地址或域名加入即可。 为了每天与NTP服务器保持时间同步，可以将命令写入到cron中。 附一份ntp server的主配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).#我们每一个system clock的频率都有小小的误差,这个就是为什么机器运行一段时间后会不精确. NTP会自动来监测我们时钟的误差值并予以调整.#但问题是这是一个冗长的过程,所以它会把记录下来的误差先写入driftfile.这样即使你重新开机以后之前的计算结果也就不会丢失了driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.# 允许所有客户端使用这个服务restrict default nomodify#restrict default kod nomodify notrap nopeer noquery#restrict -6 default kod nomodify notrap nopeer noquery# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.#restrict 127.0.0.1 #restrict -6 ::1server pool.ntp.org iburstserver 0.pool.ntp.org iburstserver 1.pool.ntp.org iburst# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# 本机同步远程的ntp的服务器域名# perfer:表示优先级最高# burst ：当一个运程NTP服务器可用时，向它发送一系列的并发包进行检测。# iburst ：当一个运程NTP服务器不可用时，向它发送一系列的并发包进行检测#NTP Server和其自身保持同步，如果在/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端。fudge 127.127.1.1 stratum 10#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography.keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats#设置ntp日志的path statsdir /var/log/ntp/ ##设置ntp日志文件 logfile /var/log/ntp/ntp.log##watch ntpq -p##参数说明：#remote: 它指的就是本地机器所连接的远程NTP服务器#refid: 它指的是给远程服务器(e.g. 193.60.199.75)提供时间同步的服务器#st: 远程服务器的层级别（stratum）. 由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端. 所以服务器从高到低级别可以设定为1-16. 为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的.#t: 未知#when: 我个人把它理解为一个计时器用来告诉我们还有多久本地机器就需要和远程服务器进行一次时间同步#poll: 本地机和远程服务器多少时间进行一次同步(单位为秒). 在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围.之后poll值会逐渐增大,同步的频率也就会相应减小#reach: 这是一个八进制值,用来测试能否和服务器连接.每成功连接一次它的值就会增加#delay: 从本地机发送同步要求到服务器的round trip time#offset: 这是个最关键的值, 它告诉了我们本地机和服务器之间的时间差别. offset越接近于0,我们就和服务器的时间越接近#jitter: 这是一个用来做统计的值. 它统计了在特定个连续的连接数里offset的分布情况. 简单地说这个数值的绝对值越小我们和服务器的时间就越精确##tinker panic 0#server 0.pool.ntp.org prefer#server 1.pool.ntp.org iburst 4.启动ntp服务1234567service ntpd startservice ntpd statuschkconfig --level 345 ntpd onchkconfig --list ntpd 5.查看ntp与上层ntp的状态 参数说明： 目前正在使用的上层NTP。+已连线,可提供时间更新的候补服务器 remote：响应这个请求的NTP服务器的主机名或IP。 refid：remote端NTP服务器的上级NTP的IP。 st：remote远程服务器的级别.由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端.所以服务器从高到低级别可以设定为1-16.为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的. t：类型。“u”表示单播。其他值有：本地、多播和广播。 when：几秒钟前曾做过时间同步更新。 poll：本地主机和远程服务器多少时间进行一次同步(单位为秒).在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围，之后poll值会逐渐增大,同步的频率也就会相应减小 reach：这是一个八进制值,用来测试能否和服务器连接.每成功连接一次它的值就会增加。377表示100%成功。 delay：从本地发送同步要求到服务器的往返时间。 offset：本地主机通过NTP时钟同步与所同步时间源的时间偏移量，单位为毫秒（ms）。offset越接近于0,主机和ntp服务器的时间越接近。 jitter 这是一个用来做统计的值.它统计了在特定个连续的连接数里offset的分布情况.简单地说这个数值的绝对值越小，主机的时间就越精确，以ms为单位. 6.配置/etc/sysconfig/ntpd文件ntp服务，默认只会同步系统时间。如果想要让ntp同时同步硬件时间，可以设置/etc/sysconfig/ntpd文件，在/etc/sysconfig/ntpd文件中，添加 SYNC_HWCLOCK=yes 这样，就可以让硬件时间与系统时间一起同步。 允许BIOS与系统时间同步，也可以通过hwclock -w 命令 SYNC_HWCLOCK=yes 7.客户端配置123crontab -e#设置定时同步ntp 服务的时间00 01 * * * root /usr/sbin/ntpdate 10.201.3.90; /sbin/hwclock -w","link":"/2020/02/23/ntp-server/"},{"title":"rkhunter安装和使用","text":"1.前言 rkhunter 是一个内核级别的rootkit检测工具,下面以rkhunter-1.4.2介绍一下安装和使用 2.安装 123456789101112131415161718192021222324252627#!/bin/bash#program:# this program init server#history:# 2016/09/06 qingye first releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHrkhunter(){tar -zxvf rkhunter-1.4.2.tar.gz cd rkhunter-1.4.2 mkdir -p /opt/tools/rkhunter ./installer.sh --layout custom /opt/tools/rkhunter --install k=$? mv /bin/rkhunter /tmp ln -s /opt/tools/rkhunter/bin/rkhunter /bin/rkhunter if [ $? = 0 ];then echo -e \"rkhunter_install \\033[32m [ ok ] \\033[0m\" else echo -e \"rkhunter_install \\033[31m [ fail ] \\033[0m\"fi }main(){rkhunter}main 3.rkhunter产生的日志结合zabbix进行报警3.1 制定定时任务12#每天定时扫描一遍，会生成一个日志文件:/var/log/rkhunter.log08 3 * * * /bin/rkhunter --check --cronjob 产生的日志有个病毒汇总信息： 3.2 编写脚本，添加zabbix自定义监控就可以了，当有发现病毒时报警出来12345678#!/bin/bashresult=\"`cat /var/log/rkhunter.log | grep \"Possible rootkits\" | awk '{print $4}'`\"if [[ $result &gt; 0 ]]; then echo 1else echo 0fi`","link":"/2020/01/14/rkhunter/"},{"title":"Centos6 Tengine开启http2传输协议","text":"1.前言最近在优化网站的访问速度，为网站开启http2协议，这个协议有什么优点呢？如下： http2是下一代的传输协议，以后都会普遍用它，是一个趋势。 http2有多路复用特性，意思是访问一个域名下面的资源，多个请求共用一条TCP链路，所以比http1.1要快得多。 2.准备工作 需要重新编译openssl1.0.2以上版本，因为我们系统的版本都是centos6的，不支持直接yum更新openssl，如果是centos7，直接yum update openssl -y 即可更新 编译完成openssl后，需要重新使用openssl的库文件重新编译tengine，我们使用的Tengine版本是Tengine/2.2.2。 3.操作步骤 安装 openssl-1.0.2t 123456789101112131415161718192021222324#进入/usr/local/src,一般软件包都放这里cd /usr/local/src#下载安装包wget https://www.openssl.org/source/openssl-1.0.2t.tar.gztar -zxvf openssl-1.0.2t.tar.gzcd openssl-1.0.2t./config shared zlib#默认安装找/usr/local/sslmake &amp;&amp; make install #先备份之前的老版本mv /usr/bin/openssl /usr/bin/openssl.oldmv /usr/include/openssl /usr/include/openssl.old#建立软连接ln -s /usr/local/ssl/bin/openssl /usr/bin/opensslln -s /usr/local/ssl/include/openssl /usr/include/openssl#把动态库加入系统配置路径echo \"/usr/local/ssl/lib\" &gt;&gt; /etc/ld.so.conf#查看动态库是否生效ldconfig -p#检查openssl版本openssl version 安装Tengine 12345678910111213141516171819202122232425262728293031323334cd /usr/local/srcwget http://tengine.taobao.org/download/tengine-2.2.2.tar.gztar tengine-2.2.2.tar.gzcd tengine-2.2.2#这里需要修改一下tengine的代码，因为我们是手动编译的openssl，依赖库路径和原来系统安装的不太一样，所以需要手动指定vim auto/lib/openssl/conf#在大概32行配置原来配置如下：CORE_INCS=\"$CORE_INCS $OPENSSL/.openssl/include\"CORE_DEPS=\"$CORE_DEPS $OPENSSL/.openssl/include/openssl/ssl.h\"CORE_LIBS=\"$CORE_LIBS $OPENSSL/.openssl/lib/libssl.a\"CORE_LIBS=\"$CORE_LIBS $OPENSSL/.openssl/lib/libcrypto.a\"#修改成如下，保存退出CORE_INCS=\"$CORE_INCS $OPENSSL/include\"CORE_DEPS=\"$CORE_DEPS $OPENSSL/include/openssl/ssl.h\"CORE_LIBS=\"$CORE_LIBS $OPENSSL/lib/libssl.a\"CORE_LIBS=\"$CORE_LIBS $OPENSSL/lib/libcrypto.a\"#先安装一些依赖(我是升级，其实不需要安装依赖了，如果是首次安装tengine，就需要安装依赖)yum -y install zlib zlib-devel openssl openssl-devel pcre pcre-devel gcc gcc-c++ autoconf automake jemalloc jemalloc-devel#开始编译tenginecd /usr/local/src/tengine-2.2.2 &amp;&amp; ./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-http_concat_module --with-jemalloc --with-http_v2_module --with-http_secure_link_module --with-openssl=/usr/local/sslmake##注意，如果是第一安装tegninx，只需要只需执行以下命令make install#但是我是安装过了，所以需要备份老的tenginecp -af /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bakcp -af /usr/local/nginx/sbin/dso_tool /usr/local/nginx/sbin/dso_tool_bak#拷贝编译好的tengine到对应目录cp /usr/local/src/tengine-2.2.2/objs/nginx /usr/local/nginx/sbin/cp /usr/local/src/tengine-2.2.2/objs/dso_tool /usr/local/nginx/sbin/#然后重启tengine，就算编译安装完成啦 tengine http2配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#配置http2很简单，如下：server { #http 不支持http2的传输协议，所以80端口不变 listen 80 # listen在原https配置文件基础上添加http2 listen 443 ssl http2; server_name www.oneq.work; .....}#另外附上一份完整的支持http2的tengine配置upstream server_backend { server ip:80 weight=10; server ip:80 weight=10; keepalive 800;#下面检测端口的配置需要tengine的才有效，不是tengine需要安装额外的插件或者直接注释即可 check interval=5000 rise=3 fall=3 timeout=5000 type=tcp;}server { listen 80; listen 443 ssl http2; server_name xxx.xxx.xxx; req_status server; ssl_certificate /usr/local/nginx/certs/xxx.xxx.xxx.crt; ssl_certificate_key /usr/local/nginx/certs/xxx.xxx.xxx.key; ssl_session_timeout 5m; ssl_protocols TLSv1.1 TLSv1.2 TLSv1; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass_header User-Agent; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Connection \"\"; proxy_http_version 1.1; access_log logs/access.log main;location / { proxy_pass http://server_backend/; access_log logs/server_backend.log main; }error_page 404 /404.html; location = /404.html { root html; }error_page 500 502 503 504 /50x.html; location = /50x.html { root html; }} 4.效果展示 5.总结1.第一次安装tengine和升级步骤有所区别，需要注意下 2.http不支持http2的传输协议，所以80端口还是使用http1.1的协议，https使用http2的传输协议","link":"/2019/11/05/tengine-http2/"},{"title":"YUM源部署和使用","text":"1.前言为什么需要内部yum源呢，有可能是业务内部的服务器对外是不通了，居于一些安全方面的考虑。内部yum源又有什么好处呢，第一，速度快；第二，内网可控，外网有问题也不影响内网包的下载和安装等。 2.部署2.1 创建yum仓库目录1234mkdir -p /data/yum_data/centos/6/os/x86_64/mkdir -p /data/yum_data/centos/6/extras/x86_64/mkdir -p /data/yum_data/centos/6/updates/x86_64/mkdir -p /data/yum_data/epel/6/x86_64/ 2.2 镜像同步公网yum源上游yum源必须要支持rsync协议，否则不能使用rsync进行同步CentOS官方标准源：rsync://mirrors.ustc.edu.cn/centos/epel源：rsync://mirrors.ustc.edu.cn/epel/同步命令： 12345rsync -auvzP --bwlimit=1000 rsync://rsync.mirrors.ustc.edu.cn/centos/6/os/x86_64/ /data/yum_data/centos/6/os/x86_64/rsync -auvzP --bwlimit=1000 rsync://rsync.mirrors.ustc.edu.cn/centos/6/extras/x86_64/ /data/yum_data/centos/6/extras/x86_64/rsync -auvzP --bwlimit=1000 rsync://rsync.mirrors.ustc.edu.cn/centos/6/updates/x86_64/ /data/yum_data/centos/6/updates/x86_64/# epel源 rsync -auvzP --bwlimit=1000 --exclude=debug rsync://rsync.mirrors.ustc.edu.cn/epel/6/x86_64/ /data/yum_data/epel/6/x86_64/ 2.3 提供yum服务部署tengine，server的配置如下： 1234567891011121314#/usr/local/nginx/conf.d/iso.confserver {listen 80;server_name localhost;access_log logs/iso.log main;location / {autoindex on;root /data/yum_data/;}error_page 500 502 503 504 /50x.html;location = /50x.html {root html; }} 2.4 客户端配置配置客户端的yum源文件，配置在/etc/yum.repos.d/下面：/etc/yum.repos.d/CentOS-Base.repo 配置： 12345678910111213141516171819[base107]name=CentOS-$releasever - Basebaseurl=http://192.168.31.107/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6#released updates[updates107]name=CentOS-$releasever - Updatesbaseurl=http://192.168.31.107/centos/$releasever/updates/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6#additional packages that may be useful[extras107]name=CentOS-$releasever - Extrasbaseurl=http://192.168.31.107/centos/$releasever/extras/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 /etc/yum.repos.d/epel.repo 配置： 123456[epel107]name=Extra Packages for Enterprise Linux 6 - $basearchbaseurl=http://192.168.31.107/epel/6/$basearchfailovermethod=prioritygpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 2.5 客户端的命令使用12yum clean allyum install telnet -y 3.参考文档 http://www.zyops.com/autodeploy-yum","link":"/2020/01/14/yum-source/"},{"title":"Tengine 基础原理和基本使用","text":"1.Tengine简介2.Tengine特性3.Tengine工作原理和用途 - 3.1 Tengine处理HTTP请求流程 - 3.2 Tengine的模块化 - 3.3 Tengine进程模型 - 3.4 Tengine事件模型 - 3.5 Tengine信号 - 3.6 Tengine定时器4.Tengine安装部署 - 4.1 安装依赖 - 4.2 编译5.Tengine 文件目录结构6.Tengine 配置文件详解 - 6.1 tengine主配置文件 - 6.2 状态检测配置文件 - 6.3 tengine常用配置7.Tengine 安全防护 - 7.1 禁止web服务IP直接访问 - 7.2 连接和请求数限制 - 7.3 访问全站IP黑名单限制 - 7.4 下载防盗链配置8.Tengine 重要模块 - 8.1 upstream负载均衡模块 - 8.2 rewrite重写模块9.location 在匹配中的优先级10.Tengine root 和 alias 的区别11.Tengine TCP转发配置12.Tengine 常用维护脚本或命令13.参考文档 1.Tengine简介Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。从2011年12月开始，Tengine成为一个开源项目，Tengine团队在积极地开发和维护着它。Tengine团队的核心成员来自于淘宝、搜狗等互联网企业。Tengine是社区合作的成果，我们欢迎大家参与其中，贡献自己的力量。 2.Tengine特性 继承Nginx-1.8.1的所有特性，兼容Nginx的配置 动态模块加载（DSO）支持。加入一个模块不再需要重新编译整个Tengine 支持HTTP/2协议，HTTP/2模块替代SPDY模块 流式上传到HTTP后端服务器或FastCGI服务器，大量减少机器的I/O压力 更加强大的负载均衡能力，包括一致性hash模块、会话保持模块，还可以对后端的服务器进行主动健康检查，根据服务器状态自动上线下线，以及动态解析upstream中出现的域名 输入过滤器机制支持，通过使用这种机制Web应用防火墙的编写更为方便 支持设置proxy、memcached、fastcgi、scgi、uwsgi在后端失败时的重试次数 动态脚本语言Lua支持。扩展功能非常高效简单 支持按指定关键字(域名，url等)收集Tengine运行状态 组合多个CSS、JavaScript文件的访问请求变成一个请求 自动去除空白字符和注释从而减小页面的体积 自动根据CPU数目设置进程个数和绑定CPU亲缘性 监控系统的负载和资源占用从而对系统进行保护 显示对运维人员更友好的出错信息，便于定位出错机器 更强大的防攻击（访问速度限制）模块 更方便的命令行参数，如列出编译的模块列表、支持的指令等 可以根据访问文件类型设置过期时间 3.Tengine工作原理和用途3.1 Tengine处理HTTP请求流程http请求是典型的请求-响应类型的的网络协议。http是文件协议，所以我们在分析请求行与请求头，以及输出响应行与响应头，往往是一行一行的进行处理。通常在一个连接建立好后，读取一行数据，分析出请求行中包含的method、uri、http_version信息。然后再一行一行处理请求头，并根据请求method与请求头的信息来决定是否有请求体以及请求体的长度，然后再去读取请求体。得到请求后，我们处理请求产生需要输出的数据，然后再生成响应行，响应头以及响应体。在将响应发送给客户端之后，一个完整的请求就处理完了。处理流程图： 3.2 Tengine的模块化Nginx由内核和模块组成。Nginx的模块从结构上分为核心模块、基础模块和第三方模块： 核心模块：HTTP模块、EVENT模块和MAIL模块 基础模块：HTTP Access模块、HTTP FastCGI模块、HTTP Proxy模块和HTTP Rewrite模块， 第三方模块：HTTP Upstream Request Hash模块、Notice模块和HTTP Access Key模块。用户根据自己的需要开发的模块都属于第三方模块。正是有了这么多模块的支撑，Nginx的功能才会如此强大，Nginx的模块从功能上分为如下三类。 Handlers（处理器模块）：此类模块直接处理请求，并进行输出内容和修改headers信息等操作。Handlers处理器模块一般只能有一个。 Filters （过滤器模块）：此类模块主要对其他处理器模块输出的内容进行修改操作，最后由Nginx输出。 Proxies （代理类模块）：此类模块是Nginx的HTTP Upstream之类的模块，这些模块主要与后端一些服务比如FastCGI等进行交互，实现服务代理和负载均衡等功能。Nginx模块常规的HTTP请求和响应的过程：Nginx本身做的工作实际很少，当它接到一个HTTP请求时，它仅仅是通过查找配置文件将此次请求映射到一个location block，而此location中所配置的各个指令则会启动不同的模块去完成工作，因此模块可以看做Nginx真正的劳动工作者。通常一个location中的指令会涉及一个handler模块和多个filter模块（当然，多个location可以复用同一个模块）。handler模块负责处理请求，完成响应内容的生成，而filter模块对响应内容进行处理。 3.3 Tengine进程模型Nginx默认采用多进程工作方式，Nginx启动后，会运行一个master进程和多个worker进程。其中master充当整个进程组与用户的交互接口，同时对进程进行监护，管理worker进程来实现重启服务、平滑升级、更换日志文件、配置文件实时生效等功能。worker用来处理基本的网络事件，worker之间是平等的，他们共同竞争来处理来自客户端的请求。nginx的进程模型如图所示： 请求到来时，如何分配均分worker进程来处理他们？ 在创建master进程时，先建立需要监听的socket（listenfd），然后从master进程中fork()出多个worker进程，如此一来每个worker进程多可以监听用户请求的socket。一般来说，当一个连接进来后，所有在Worker都会收到通知，但是只有一个进程可以接受这个连接请求，其它的都失败，这是所谓的惊群现象。nginx提供了一个accept_mutex（互斥锁），有了这把锁之后，同一时刻，就只会有一个进程在accpet连接，这样就不会有惊群问题了。 先打开accept_mutex选项，只有获得了accept_mutex的进程才会去添加accept事件。nginx使用一个叫ngx_accept_disabled的变量来控制是否去竞争accept_mutex锁。ngx_accept_disabled = nginx单进程的所有连接总数 / 8 -空闲连接数量，当ngx_accept_disabled大于0时，不会去尝试获取accept_mutex锁，ngx_accept_disable越大，于是让出的机会就越多，这样其它进程获取锁的机会也就越大。不去accept，每个worker进程的连接数就控制下来了，其它进程的连接池就会得到利用，这样，nginx就控制了多进程间连接的平衡。 每个worker进程都有一个独立的连接池，连接池的大小是worker_connections。这里的连接池里面保存的其实不是真实的连接，它只是一个worker_connections大小的一个ngx_connection_t结构的数组。并且，nginx会通过一个链表free_connections来保存所有的空闲ngx_connection_t，每次获取一个连接时，就从空闲连接链表中获取一个，用完后，再放回空闲连接链表里面。一个nginx能建立的最大连接数，应该是worker_connections 乘以 worker_processes。当然，这里说的是最大连接数，对于HTTP请求本地资源来说，能够支持的最大并发数量是worker_connections 乘以 worker_processes，而如果是HTTP作为反向代理来说，最大并发数量应该是worker_connections 乘以 worker_processes/2。因为作为反向代理服务器，每个并发会建立与客户端的连接和与后端服务的连接，会占用两个连接。 3.4 Tengine事件模型对于一个基本的web服务器来说，事件通常有三种类型，网络事件、信号的处理。Nginx每个worker里面只有一个主线程，多少个worker就能处理多少个并发，何来高并发呢？请求流程：首先，请求到来，要建立连接，然后再接收数据，接收数据后，再发送数据。具体到系统底层，就是读写事件，而当读写事件没有准备好时，不可操作。apache的常用工作方式：每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。这对操作系统来说，是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的cpu开销很大，自然性能就上不去了，而这些开销完全是没有意义的。 Nginx采用异步非阻塞的方式来支持用户请求。Nginx支持select/poll/epoll/kqueue等事件模型。拿epoll为例，当事件没准备好时，放到epoll里面，事件准备好了，我们就去读写，当读写返回EAGAIN时，我们将它再次加入到epoll里面。这样，只要有事件准备好了，我们就去处理它，当事件都没有完全准备好时，就在epoll里面等着。这样，我们就可以并发处理大量的并发了，当然，这里的并发请求，是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（上下文切换），更多的并发数，只是会占用更多的内存而已。 现在的网络服务器基本都采用这种方式，这也是nginx性能高效的主要原因。 推荐设置worker的个数为cpu的核数，因为更多的worker数，只会导致进程来竞争cpu资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了cpu亲缘性的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来cache的失效。 3.5 Tengine信号对nginx来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重新进行一次。如果nginx正在等待事件（epoll_wait时），如果程序收到信号，在信号处理函数处理完后，epoll_wait会返回错误，然后程序可再次进入epoll_wait调用。 3.6 Tengine定时器由于epoll_wait等函数在调用的时候是可以设置一个超时时间的，所以nginx借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入epoll_wait前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出epoll_wait的超时时间后进入epoll_wait。所以，当没有事件产生，也没有中断信号时，epoll_wait会超时，也就是说，定时器事件到了。这时，nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。由此可以看出，当我们写nginx代码时，在处理网络事件的回调函数时，通常做的第一个事情就是判断超时，然后再去处理网络事件 4.Tengine安装部署4.1 安装依赖1yum -y install zlib zlib-devel openssl openssl-devel pcre pcre-devel gcc gcc-c++ autoconf automake jemalloc jemalloc-devel 4.2 编译123456789101112tar -zxvf tengine-2.2.0.tar.gzcd tengine-2.2.0./configure --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-http_concat_module --with-jemalloc --with-http_v2_module --with-http_secure_link_modulemake &amp;&amp; make install#编译选项解析./configure --with-http_stub_status_module #添加监控状态配置 --with-http_ssl_module #支持https --with-http_gzip_static_module #支持静态文件预压缩 --with-http_concat_module #用于合并多个文件在一个响应报文中 --with-jemalloc #支持jemalloc内存管理 --with-http_v2_module #支持http_v2 --with-http_secure_link_module #防下载盗链支持 5.Tengine 文件目录结构 12345678910111213|---certs ：存放域名证书位置，自建的规范目录|---client_body_temp：如果客户端POST一个比较大的文件，长度超过了nginx缓冲区的大小，需要把这个文件的部分或者全部内容暂存到client_body_temp目录下的临时文件|---conf：主配置文件存放目录|---conf.d：存放虚拟主机的配置文件，自建规范目录|---fastcgi_temp：对于来自 FastCGI Server 的 Response，Nginx 将其缓冲到内存中，然后依次发送到客户端浏览器。缓冲区的大小由 fastcgi_buffers 和 fastcgi_buffer_size 两个值控制，fastcgi_buffers 控制 nginx 最多创建 8 个大小为 4K 的缓冲区，而 fastcgi_buffer_size 则是处理 Response 时第一个缓冲区的大小，不包含在前者中，超出部分存在这个目录|---html：静态文件默认存放位置|---include：存放编译代码头文件目录|---logs：默认存放日志目录|---modules:存放模块目录|---proxy_temp:后端返回数据的临时存放目录|---sbin:nginx二进制文件存放目录|---scgi_temp:客户端可能会向服务器端请求大量的数据,服务器端收到的请求报文中的body中可能会有很多的数据,而这些数据都会存放内存中,倘若有很多的用户并发发出请求,服务器端内存无法存放，因此就会把数据临时存放在磁盘上的这些临时文件内|---uwsgi_temp:代理服务器时缓存文件的存放路径 6.Tengine 配置文件详解6.1 tengine主配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108#/usr/local/nginx/conf/nginx.confuser nobody nobody; # 指定运行的用户和用户组 worker_processes auto; # 指定要开启的进程数，一般为CPU的核心数或两倍 worker_cpu_affinity auto; # cpu亲和性 worker_rlimit_nofile 102400; # 这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数(ulimit -n)与nginx进程数相除，但是nginx 分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致 pid logs/nginx.pid; #进程号存放位置events { use epoll; # 指定nginx的工作模式 worker_connections 65535; # 定义每个进程的最大连接数 }http { req_status_zone server \"$server_addr:$server_port\" 10M; # 根据变量值分别统计Tengine的运行状况 include mime.types; # 服务器识别的文件类型 default_type application/octet-stream; # 文件类型未定义时默认识别为二进制流 check_shm_size 20M; #后端检测内存区大小 sendfile on; # 高速传输文件 tcp_nopush on; # 防止网络阻塞 tcp_nodelay on; # 防止网络阻塞 server_tokens off; # 不返回nginx的版本信息 server_info off; #返回错误页面是不返回服务器信息 keepalive_timeout 20s; # 一个keepalive 连接被闲置以后还能保持多久打开状态 keepalive_requests 1000; # 这是一个客户端可以通过一个keepalive连接的请求次数。缺省值是100，但是也可以调得很高，而且这对于测试负载生成工具从哪里使用一个客户端发送这么多请求非常有用 gzip on; # 开启gzip压缩输出 gzip_min_length 1024; # 最小压缩文件大小 gzip_buffers 16 8k; # 压缩缓冲区 gzip_comp_level 6; # 压缩等级 gzip_proxied any; # Nginx作为反向代理的时候启用,根据某些请求和应答来决定是否在对代理请求的应答启用gzip压缩，是否压缩取决于请求头中的“Via”字段启用压缩,这里是如果header头中包含 \"Expires\" 头信息 gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript application/javascript; #压缩类型，默认就已经包含text/html，所以下面就不用再写了 # 写上去也不会有问题，但是会有一个warn gzip_vary on; # 让前端的缓存服务器缓存经过gzip压缩的页面，和http头有关系，加个vary头，给代理服务器用的，有的浏览器支持压缩，有的不支持，所以避免浪费不支持的也压缩，所以根据客户端的HTTP头来判断，是否需要压缩 fastcgi_intercept_errors on; # 这个指令指定是否传递4xx和5xx错误信息到客户端，或者允许nginx使用error_page处理错误信息 fastcgi_connect_timeout 75s; # 指定连接到后端FastCGI的超时时间 fastcgi_send_timeout 300s; # 指定向FastCGI传送请求的超时时间，这个值是已经完成两次握手后向FastCGI传送请求的超时时间。 fastcgi_read_timeout 300s; # 指定接收FastCGI应答的超时时间，这个值是已经完成两次握手后接收FastCGI应答的超时时间。 fastcgi_buffer_size 16k; # 用于指定读取FastCGI应答第一部分需要用多大的缓冲区，这个值表示将使用1个64KB的缓冲区读取应答的第一部分（应答头），可以设置为fastcgi_buffers选项指定的缓冲区大小 fastcgi_buffers 4 16k; # 指定本地需要用多少和多大的缓冲区来缓冲FastCGI的应答请求。如果一个PHP脚本所产生的页面大小为256KB,那么会为其分配4个64KB的缓冲区来缓存,如果页面大小大于256KB，那么大于256K # B的部分会缓存到 fastcgi_busy_buffers_size 32k; # 繁忙模式下的缓冲区大小，默认值是fastcgi_buffers的两倍 #open_file_cache max=10240 inactive=20s; # 最多缓存多少个文件，缓存多少时间 #open_file_cache_valid 30s; # 多少时间检查一次，如果发现20s内没有用过一次的删除 #open_file_cache_min_uses 1; # 在20S中没有使用到这个配置的次数的话就删除 client_body_timeout 90s; # 设置客户端请求头读取超时的时间 client_max_body_size 20m; # 用来设置允许客户端请求的最大的单个文件字节数 client_body_buffer_size 1m; # 设置缓存区的最大值 client_header_buffer_size 128k; # 用于指定来自客户端请求头的header buffer大小 large_client_header_buffers 256 16k; # 用来指定客户端请求中较大的消息头和缓存最大数量和大小 proxy_buffer_size 16k; # 设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 8 32k; # proxy_buffers缓冲区，网页平均在32k以下的设置 proxy_busy_buffers_size 64k; # 高负荷下缓冲大小（proxy_buffers*2） proxy_connect_timeout 300s; # nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 300s; # 后端服务器数据回传时间(代理发送超时) proxy_read_timeout 300s; # 连接成功后，后端服务器响应时间(代理接收超时) #proxy_temp_path /usr/local/nginx/proxy_temp; # 缓存临时目录。后端的响应并不直接返回客户端，而是先写到一个临时文件中，然后被rename一下当做缓存放在 proxy_cache_path #proxy_cache_path ... ： 设置缓存目录，目录里的文件名是 cache_key 的MD5值。 #levels=1:2 keys_zone=cache_one:50m表示采用2级目录结构，Web缓存区名称为cache_one，内存缓存空间大小为100MB，这个缓冲zone可以被多次使用。文件系统上看到的缓存文件名类似于 /usr/local/nginx-1.6/proxy_cache/c/29/b7f54b2df7773722d382f4809d65029c inactive=2d max_size=2g表示2天没有被访问的内容自动清除，硬盘最大缓存空间为2GB，超过这个大学将清除最近最少使用的数据。 #proxy_cache_path /usr/local/nginx/proxy_cache levels=1:2 keys_zone=cache_one:100m inactive=2d max_size=2g;这两句需要搭配location里面的指令使用，默认不开启 map $upstream_addr $short_address { ~^\\d+\\.\\d+\\.(.*) ''; } add_header X-from $short_address$1; proxy_set_header Host $host; proxy_set_header X-User-IP $clientRealIp; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass_header User-Agent; proxy_set_header X-Forwarded-Proto $scheme; map $http_x_forwarded_for $clientRealIp { \"\" $remote_addr; ~^(?P&lt;firstAddr&gt;[0-9\\.]+),?.*$ $firstAddr; } log_format main '{ \"@timestamp\": \"$time_iso8601\", ' '\"remote_addr\": \"$remote_addr\", ' '\"upstream_addr\": \"$upstream_addr\", ' '\"server_addr\": \"$server_addr\", ' '\"http_host\": \"$http_host\",' '\"request_time\": $request_time, ' '\"upstream_response_time\": $upstream_response_time, ' '\"request_uri\": \"$request_uri\", ' '\"status\": \"$status\", ' '\"request\": \"$request\", ' '\"request_method\": \"$request_method\", ' '\"http_referer\": \"$http_referer\", ' '\"body_bytes_sent\": $body_bytes_sent, ' '\"http_x_forwarded_for\": \"$http_x_forwarded_for\", ' '\"request_length\": $request_length, ' '\"http_user_agent\": \"$http_user_agent\", ' '\"scheme\": \"$scheme\",' '\"uri\": \"$uri\",' '\"clientRealIp\": \"$clientRealIp\"}'; include /usr/local/nginx/conf.d/*.conf;} 6.2 状态检测配置文件12345678910111213141516171819202122#/usr/local/nginx/conf.d/admin.confserver { listen 8000; location ~ ^/(phpfpm_status|ping)$ { fastcgi_pass 127.0.0.1:9000; include fastcgi.conf; } location = /nginx_status { stub_status on; access_log off; } location = /nginx_status_detail { req_status_show; } location = /nstatus { check_status; access_log off; #allow IP; #deny all; }} 6.3 tengine常用配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#/usr/local/nginx/conf.d/default.conf(一些常用的配置功能，供参考)server { listen 80 ; listen 443 ssl; server_name www.test1.com www.test2.com; server_tag off; req_status server; # 开启https的配置,证书存放位置需要规范 # ssl on; ssl_certificate /usr/local/nginx/certs/test.crt; ssl_certificate_key /usr/local/nginx/certs/test.key; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; #配置本监听端口下的日志 access_log logs/host.access.log main; location /example { root html; index index.html index.htm; # 最多 5 个排队， 由于每秒处理 10 个请求 + 5个排队，你一秒最多发送 15 个请求过来，再多就直接返回 503 错误给你了 # limit_req zone=ConnLimitZone burst=5 nodelay; # 用于获取用户真实IP，传给后端的服务器获取 proxy_set_header Host $host; proxy_set_header X-real-ip $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 开启与后端tomcat长连接需下面两个指令,tomcat也需要配置长连接参数(tomcat在server.xml配置标签 &lt;Connector keepAliveTimeout=\"20000\" maxKeepAliveRequests=\"3000\" /&gt;) proxy_http_version 1.1; proxy_set_header Connection \"\"; # 反向代理指令，记得proxy_pass 带uri和不带uri的区别 #proxy_pass http://bakend/ ; #与负载均衡结合使用 proxy_pass http://192.168.31.11:7111/ ; # 用户访问局部限制指令,匹配上的立马匹配，不继续下面的匹配动作 allow 192.168.31.69; deny all; deny 192.168.31.70; allow all; # 传输数据限速到20k,为了测试很明显就把此值调的很低 limit_rate 20k; # 数据合并发送 # 合并js css,需要开发合并文件，暂时使用不了 concat on; concat_max_files 20; concat_unique off; concat_delimiter \"\\r\\n\"; concat_ignore_file_error on; #配置web缓存,与主配置文件配置的缓存目录结合使用。缓存也就是将js、css、image等静态文件从tomcat缓存到nginx指定的缓存目录下，既可以减轻tomcat负担，也可以加快访问速度， #但这样缓存及时清理成为了一个问题，所以需要 ngx_cache_purge 这个模块来在过期时间未到之前，手动清理缓存。（有篇文章http://quenlang.blog.51cto.com/4813803/1570671， #对比使用缓存、不使用缓存、使用动静分离三种情况下，高并发性能比较。使用代理缓存功能性能会高出很多倍） #反向代理到哪台机器 proxy_pass http://192.168.31.11; #不显示重定向信息 proxy_redirect off; #设置请求头,获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #引用前面定义的缓存区 cache_one proxy_cache cache_one; #增加头部记录缓存命中率 add_header Nginx-Cache $upstream_cache_status; #proxy_cache_valid ： 为不同的响应状态码设置不同的缓存时间，比如200、302等正常结果可以缓存的时间长点，而404、500等缓存时间设置短一些，这个时间到了文件就会过期，而不论是否刚被访问过。 proxy_cache_valid 200 304 301 302 8h; proxy_cache_valid 404 1m; proxy_cache_valid any 2d; #定义cache_key proxy_cache_key $host$uri$is_args$args; #返回给客户端的浏览器缓存失效时间,指的是静态资源的缓存 expires 30d; } #清除缓存。下面配置的proxy_cache_purge指令用于方便的清除缓存，但必须按装第三方的 ngx_cache_purge 模块才能使用，项目地址：https://github.com/FRiCKLE/ngx_cache_purge/ 。 location ~ /purge(/.*) { #设置只允许指定的IP或IP段才可以清除URL缓存。 allow 127.0.0.1; allow 192.168.31.0/24; deny all; proxy_cache_purge cache_one $host$1$is_args$args; error_page 405 =200 /purge$1; } # 图片防盗链配置 location ~* \\.(gif|jpg|png|bmp)$ { root html; valid_referers none blocked *.feidee.com server_names ~\\.google\\. ~\\.baidu\\.; if ($invalid_referer) { return 403; #rewrite ^/ http://xxxx/403.jpg; } } location /realip { #如果是作为代理，下面三句才会把客户端的IP带到真正的服务器上面 proxy_set_header Host $host; proxy_set_header X-real-ip $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://192.168.31.11/realip/; } error_page 404 /404.html; # redirect server error pages to the static page /50x.html error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } 7.Tengine 安全防护7.1 禁止web服务IP直接访问123456789101112131415161718#/usr/local/nginx/conf.d/defend.conf(禁止IP直接访问配置，默认开启)#Nginx配置禁止IP直接访问，如果是直接使用IP访问，直接返回客户端403错误#注意:对于https，必须指定证书，否则连域名访问也会一起禁止server { listen 80 default; #server_name _; return 403; }server { listen 443 default; #server_name _; ssl_certificate /usr/local/nginx/certs/test.crt; ssl_certificate_key /usr/local/nginx/certs/test.key; return 403; } 7.2 连接和请求数限制123456789101112131415161718192021222324252627282930313233343536#/usr/local/nginx/conf.d/limit_rate.conf(连接和请求数限制脚本，默认关闭)# 这里取得原始用户的IP地址map $http_x_forwarded_for $clientRealIp { \"\" $remote_addr; ~^(?P&lt;firstAddr&gt;[0-9\\.]+),?.*$ $firstAddr;}# 设置白名单geo $clientRealIp $whiteiplist { default 1; 192.168.31.241 1; 192.168.31.251 0; 192.168.31.236 0; 192.168.31.0/24 0; }map $whiteiplist $limit { 1 $clientRealIp; 0 \"\"; }# server 段配置# if ( $whiteiplist = 0 ){# return 403;# }# if ( $http_user_agent ~ Dalvik.* ){# return 403;# }# 针对原始用户 IP 地址做限制limit_conn_zone $limit zone=TotalConnLimitZone:20m ;limit_conn TotalConnLimitZone 50; # 每个IP最大连接数limit_conn_log_level notice;# 针对原始用户 IP 地址做限制limit_req_zone $limit zone=ConnLimitZone:20m rate=20r/s; #每个地址每秒只能请求同一个URL20次limit_req zone=ConnLimitZone burst=10 nodelay; # 如果开启此条规则，burst=10的限制将会在nginx全局生效 一共有10块令牌,并且每秒钟只新增1块令牌,10块令牌发完后 多出来的那些请求就会返回503.limit_req_log_level notice; 7.3 访问全站IP黑名单限制1234567#/usr/local/nginx/conf.d/globle_blacklistip.conf(访问全站限制IP)#配置全局的对网站访问的限制，对特定的url进行访问限制可以在location里面进行配置#############globle setting ##########deny 192.168.31.69;deny 192.168.31.70;#deny 192.168.31.0/24 ;allow all; 7.4 下载防盗链配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#/usr/local/nginx/conf.d/secure_link.conf(下载防盗链配置)nginx 配置如下:server { listen 80; server_name www.testchao.com #本机域名 access_log ../logs/ www.testchao.com.access.log main; #日志 location / { root html; #md5_hash[,expiration_time] MD5哈希值和过期时间 secure_link $arg_st,$arg_e; #md5值对比结果,使用上面提供的uri、密钥、过期时间生成md5哈希值.如果它生成的md5哈希值与用户提交过来的哈希值一致，那么这个变量的值为1，否则为0 secure_link_md5 ttlsa.com$uri$arg_e; if ($secure_link = \"\") { return 403; } if ($secure_link = \"0\") { return 403; } } #php解析 location ~ \\.php$ { root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root/$fastcgi_script_name; include fastcgi_params; }}#下载页面php如下:#download.php&lt;html&gt;&lt;br&gt;nginx下载页面&lt;br&gt;&lt;a href=\"/test.php\" class=\"btn btn-danger go-link\" role=\"button\" target=\"_blank\" rel=\"nofollow\" _hover-ignore=\"1\"&gt;下载地址&lt;/a&gt;&lt;/html&gt;#test.php 生成连接页面&lt;?php # 作用：生成nginx secure link链接 # 站点：www.qingye.info # 作者：青叶 # 时间：2020-01-12$secret = 'qingye'; # 密钥 $path = '/web/nginx-1.4.2.tar.gz.jpg'; # 下载文件 # 下载到期时间,time是当前时间,300表示300秒,也就是说从现在到300秒之内文件不过期 $expire = time()+10;# 用文件路径、密钥、过期时间生成加密串 $md5 = base64_encode(md5($secret . $path . $expire .$_SERVER['REMOTE_ADDR'], true)); $md5 = strtr($md5, '+/', '-_'); $md5 = str_replace('=', '', $md5);# 加密后的下载地址$url = http://www.qingye.info/web/nginx-1.4.2.tar.gz.jpg?st='.$md5.'&amp;e='.$expire;#echo '&lt;a href=http://www.qingye.info/web/nginx-1.4.2.tar.gz.jpg?st='.$md5.'&amp;e='.$expire.'&gt;nginx-1.4.2&lt;/a&gt;';#echo '&lt;br&gt;http://www.qingye.info/web/nginx-1.4.2.tar.gz.jpg?st='.$md5.'&amp;e='.$expire;header(\"Location: $url\"); ?&gt; 8.Tengine 重要模块8.1 upstream负载均衡模块1234567891011121314151617181920#upstream模块详细例子upstream backend {#负载均衡算法，这个ip_hash根据IP来辨别，有时候因为访问都是从cdn或其他同一个IP过来，导致负载均衡发挥不好ip_hash;server 192.168.31.225:8080 weight 2;server 192.168.31.226:8080 weight=1 max_fails=2 fail_timeout=30s ;server 192.168.31.227:8080 backup; }server {location / {proxy_pass http://backend;proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; }#weight ： 轮询权值也是可以用在ip_hash的，默认值为1#max_fails ： 允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。#fail_timeout ： 有两层含义，一是在 30s 时间内最多容许 2 次失败；二是在经历了 2 次失败以后，30s时间内不分配请求到这台服务器。#backup ： 预留的备份机器。当其他所有的非backup机器出现故障的时候，才会请求backup机器，因此这台机器的压力最轻。（为什么我的1.6.3版本里配置backup启动nginx时说invalid parameter \"backup\"？）#max_conns： 限制同时连接到某台后端服务器的连接数，默认为0即无限制。因为queue指令是commercial，所以还是保持默认吧。#proxy_next_upstream ： 这个指令属于 http_proxy 模块的，指定后端返回什么样的异常响应时，使用另一个realserver 关于nginx使用负载均衡会话跟踪问题(ngx_http_upstream_session_sticky_module 模块，tenginx默认已经安装) 这个模块的作用是通过cookie黏贴的方式将来自同一个客户端（浏览器）的请求发送到同一个后端服务器上处理，这样一定程度上可以解决多个backend servers的session同步的问题 —— 因为不再需要同步， 而RR轮询模式(nginx的默认后端调度模式)必须要运维人员自己考虑session同步的实现。 语法：session_sticky [cookie=name] [domain=your_domain] [path=your_path] [maxage=time] [mode=insert|rewrite|prefix] [option=indirect] [maxidle=time] [maxlife=time] [fallback=on|off] [hash=plain|md5]默认值：session_sticky cookie=route mode=insert fallback=on上下文：upstream 说明: 本指令可以打开会话保持的功能，下面是具体的参数： cookie设置用来记录会话的cookie名称 domain设置cookie作用的域名，默认不设置 path设置cookie作用的URL路径，默认不设置 maxage设置cookie的生存期，默认不设置，即为session cookie，浏览器关闭即失效 mode设置cookie的模式: insert: 在回复中本模块通过Set-Cookie头直接插入相应名称的cookie。 prefix: 不会生成新的cookie，但会在响应的cookie值前面加上特定的前缀，当浏览器带着这个有特定标识的cookie再次请求时，模块在传给后端服务前先删除加入的前缀，后端服务拿到的还是原来的cookie值，这些动作对后端透明。如：”Cookie: NAME=SRV~VALUE”。 rewrite: 使用服务端标识覆盖后端设置的用于session sticky的cookie。如果后端服务在响应头中没有设置该cookie，则认为该请求不需要进行session sticky，使用这种模式，后端服务可以控制哪些请求需要sesstion sticky，哪些请求不需要。 option 设置用于session sticky的cookie的选项，可设置成indirect或direct。indirect不会将session sticky的cookie传送给后端服务，该cookie对后端应用完全透明。direct则与indirect相反。 maxidle设置session cookie的最长空闲的超时时间 maxlife设置session cookie的最长生存期 fallback设置是否重试其他机器，当sticky的后端机器挂了以后，是否需要尝试其他机器 hash 设置cookie中server标识是用明文还是使用md5值，默认使用md5maxage是cookie的生存期。不设置时，浏览器或App关闭后就失效。下次启动时，又会随机分配后端服务器。所以如果希望该客户端的请求长期落在同一台后端服务器上，可以设置maxage。 hash不论是明文还是hash值，都有固定的数目。因为hash是server的标识，所以有多少个server，就有等同数量的hash值。 一些例外： 同一客户端的请求，有可能落在不同的后端服务器上## 如果客户端启动时同时发起多个请求。由于这些请求都没带cookie，所以服务器会随机选择后端服务器，返回不同的cookie。当这些请求中的最后一个请求返回时，客户端的cookie才会稳定下来，值以最后返回的cookie为准。 cookie不一定生效## 由于cookie最初由服务器端下发，如果客户端禁用cookie，则cookie不会生效。 客户端可能不带cookie## Android客户端发送请求时，一般不会带上所有的cookie，需要明确指定哪些cookie会带上。如果希望用sticky做负载均衡，请对Android开发说加上cookie。 注意事项： cookie名称不要和业务使用的cookie重名。Sticky默认的cookie名称是route，可以改成任何值。但切记，不可以与业务中使用的cookie重名。客户端发的第一个请求是不带cookie的。服务器下发的cookie，在客户端下一次请求时才能生效另外内置的 ip_hash 也可以实现根据客户端IP来分发请求，但它很容易造成负载不均衡的情况，而如果nginx前面有CDN网络或者来自同一局域网的访问，它接收的客户端IP是一样的，容易造成负载不均衡现象。 这个模块并不合适不支持 Cookie 或手动禁用了cookie的浏览器，此时默认session_sticky就会切换成RR。它不能与ip_hash同时使用。 123456789101112upstream backend { check interval=3000 rise=2 fall=5 timeout=1000 type=http; check_http_send \"HEAD / HTTP/1.0\\r\\n\\r\\n\"; check_http_expect_alive http_2xx http_3xx; server 192.168.31.226:8080 weight=1; server 192.168.31.227:8080 weight=1; session_sticky;#在insert + indirect模式或者prefix模式下需要配置session_sticky_hide_cookie#这种模式不会将保持会话使用的cookie传给后端服务，让保持会话的cookie对后端透明#session_sticky cookie=uid fallback=on mode=insert option=indirect hash=plain;#配置起来超级简单，一般来说一个session_sticky指令就够了。} 负载均衡其它调度方案 轮询（默认） ： 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某台服务器宕机，故障系统被自动剔除，使用户访问不受影响。Weight 指定轮询权值，Weight值越大，分配到的访问机率越高，主要用于后端每个服务器性能不均的情况下。 ip_hash ： 每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题。当然如果这个节点不可用了，会发到下个节点，而此时没有session同步的话就注销掉了。 least_conn ： 请求被发送到当前活跃连接最少的realserver上。会考虑weight的值。 url_hash ： 此方法按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身是不支持url_hash的，如果需要使用这种调度算法，必须安装Nginx 的hash软件包 nginx_upstream_hash 。 fair ： 这是比上面两个更加智能的负载均衡算法。此种算法可以依据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身是不支持fair的，如果需要使用这种调度算法，必须下载Nginx的 upstream_fair 模块 8.2 rewrite重写模块rewrite模块rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在server{},location{},if{}中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如 http://seanlook.com/a/we/index.php?id=1&amp;u=str 只对/a/we/index.php重写。语法rewrite regex replacement [flag]; flag标志位 1234last : 相当于Apache的[L]标记，表示完成rewritebreak : 停止执行当前虚拟主机的后续rewrite指令集redirect : 返回302临时重定向，地址栏会显示跳转后的地址permanent : 返回301永久重定向，地址栏会显示跳转后的地址 if指令与全局变量,if判断指令: 12345678语法为if(condition){...}，对给定的条件condition进行判断。如果为真，大括号内的rewrite指令将被执行，if条件(conditon)可以是如下任何内容：当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false直接比较变量和内容时，使用=或!=~正则表达式匹配，~*不区分大小写的匹配，!~区分大小写的不匹配-f和!-f用来判断是否存在文件-d和!-d用来判断是否存在目录-e和!-e用来判断是否存在文件或目录-x和!-x用来判断文件是否可执行 12345678910111213141516171819202122232425if ($http_user_agent ~ MSIE) { rewrite ^(.*)$ /msie/$1 break;} //如果UA包含\"MSIE\"，rewrite请求到/msid/目录下if ($http_cookie ~* \"id=([^;]+)(?:;|$)\") { set $id $1; } //如果cookie匹配正则，设置变量$id等于正则引用部分if ($request_method = POST) { return 405;} //如果提交方法为POST，则返回状态405（Method not allowed）。return不能返回301,302if ($slow) { limit_rate 10k;} //限速，$slow可以通过 set 指令设置if (!-f $request_filename){ break; proxy_pass http://127.0.0.1;} //如果请求的文件名不存在，则反向代理到localhost 。这里的break也是停止rewrite检查if ($args ~ post=140){ rewrite ^ http://example.com/ permanent;} //如果query string中包含\"post=140\"，永久重定向到example.comlocation ~* \\.(gif|jpg|png|swf|flv)$ { valid_referers none blocked www.jefflei.com www.leizhenfang.com; if ($invalid_referer) { return 404; } //防盗链} 下面是可以用作if判断的全局变量: 123456789101112131415161718192021$args ： #这个变量等于请求行中的参数，同$query_string$content_length ： 请求头中的Content-length字段。$content_type ： 请求头中的Content-Type字段。$document_root ： 当前请求在root指令中指定的值。$host ： 请求主机头字段，否则为服务器名称。$http_user_agent ： 客户端agent信息$http_cookie ： 客户端cookie信息$limit_rate ： 这个变量可以限制连接速率。$request_method ： 客户端请求的动作，通常为GET或POST。$remote_addr ： 客户端的IP地址。$remote_port ： 客户端的端口。$remote_user ： 已经经过Auth Basic Module验证的用户名。$request_filename ： 当前请求的文件路径，由root或alias指令与URI请求生成。$scheme ： HTTP方法（如http，https）。$server_protocol ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。$server_addr ： 服务器地址，在完成一次系统调用后可以确定这个值。$server_name ： 服务器名称。$server_port ： 请求到达服务器的端口号。$request_uri ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。$uri ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。$document_uri ： 与$uri相同。 常用正则: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253. ： 匹配除换行符以外的任意字符? ： 重复0次或1次+ ： 重复1次或更多次* ： 重复0次或更多次\\d ：匹配数字^ ： 匹配字符串的开始$ ： 匹配字符串的介绍{n} ： 重复n次{n,} ： 重复n次或更多次[c] ： 匹配单个字符c[a-z] ： 匹配a-z小写字母的任意一个#小括号()之间匹配的内容，可以在后面通过$1来引用，$2表示的是前面第二个()里的内容。正则里面容易让人困惑的是\\转义特殊字符。break语法：break默认值：none使用字段：server, location, if完成当前设置的重写规则，停止执行其他的重写规则。return语法：return code默认值：none使用字段：server, location, if停止处理并为客户端返回状态码。非标准的444状态码将关闭连接，不发送任何响应头。可以使用的状态码有：204，400，402-406，408，410, 411, 413, 416与500-504。如果状态码附带文字段落，该文本将被放置在响应主体。相反，如果状态码后面是一个URL，该URL将成为location头补值。没有状态码的URL将被视为一个302状态码。rewrite语法：rewrite regex replacement flag默认值：none使用字段：server, location, if按照相关的正则表达式与字符串修改URI，指令按照在配置文件中出现的顺序执行。可以在重写指令后面添加标记。注意：如果替换的字符串以http://开头，请求将被重定向，并且不再执行多余的rewrite指令。尾部的标记(flag)可以是以下的值：last - 停止处理重写模块指令，之后搜索location与更改后的URI匹配。break - 完成重写指令。redirect - 返回302临时重定向，如果替换字段用http://开头则被使用。permanent - 返回301永久重定向。rewrite_log语法：rewrite_log on | off默认值：rewrite_log off使用字段：server, location, if变量：无启用时将在error log中记录notice级别的重写日志。set语法：set variable value默认值：none使用字段：server, location, if为给定的变量设置一个特定值。uninitialized_variable_warn语法：uninitialized_variable_warn on|off默认值：uninitialized_variable_warn on使用字段：http, server, location, if控制是否记录未初始化变量的警告信息。 例子: 1234567891011121314151617#last和break实现URL重写，浏览器地址栏URL地址不变location ~ ^/best/ {rewrite ^/best/(.*) /test/$1 break;proxy_pass http://www.taob.com}#更换域名server{server_name www.taob.com;rewrite ^/(.*)$ http://www.tb.com/$1 permanent;}#或者server {server_name www.tb.com www.taob.com;if ($host != 'www.tb.com')rewrite ^/(.*)$ http://www.tb.com/$1 permanent;} 9.location 在匹配中的优先级配置文件示例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051location = / {# 仅仅匹配请求 /[ configuration A ]}location / {# 匹配所有以 / 开头的请求。# 但是如果有更长的同类型的表达式，则选择更长的表达式。# 如果有正则表达式可以匹配，则优先匹配正则表达式。[ configuration B ]}location /documents/ {# 匹配所有以 /documents/ 开头的请求。# 但是如果有更长的同类型的表达式，则选择更长的表达式。# 如果有正则表达式可以匹配，则优先匹配正则表达式。[ configuration C ]}location ^~ /images/ {# 匹配所有以 /images/ 开头的表达式，如果匹配成功，则停止匹配查找。# 所以，即便有符合的正则表达式location，也不会被使用[ configuration D ]}location ~* \\.(gif|jpg|jpeg)$ {# 匹配所有以 gif jpg jpeg结尾的请求。# 但是 以 /images/开头的请求，将使用 Configuration D[ configuration E ]} 匹配结果： URL 匹配结果 原因 / configuration A =优先级最高，匹配到结束 /index.html configuration B 路径匹配 /documents/document.html configuration C 第二个匹配到，往后继续匹配，发现第三个匹配最确 /images/1.gif configuration D 同时匹配第二个，第四个，和第五个。但是由于优先级问题 /documents/1.jpg configuration E 同时匹配第二个，第三个，和第五个。第五个是正则表达式 优先级如下: (location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ,* 正则顺序) &gt; (location 部分起始路径) &gt; (/) 注： ~ 和 ~都是正则匹配 其中 ~ 不区分大小写 ~ 区分大小写 10.Tengine root 和 alias 的区别nginx配置下有两个指定目录的执行，root和alias 例如有以下配置文件： 123location /img/ { alias /var/www/image/;} 若按照上述配置的话，则访问/img/目录里面的文件时，ningx会自动去/var/www/image/目录找文件 123location /img/ { root /var/www/image;} 若按照这种配置的话，则访问/img/目录下的文件时，nginx会去/var/www/image/img/目录下找文件。alias是一个目录别名的定义，root则是最上层目录的定义还有一个重要的区别是alias后面必须要用“/”结束，否则会找不到文件的。。。而root则可有可无 11.Tengine TCP转发配置123456789101112131415#nginx tcp 配置，这一段必须放在http上下文tcp {timeout 1d;proxy_read_timeout 10d;proxy_send_timeout 10d;proxy_connect_timeout 30;upstream api_server {server 192.168.31.212:389 weight=5 max_fails=1 fail_timeout=10s;}server {listen 1389;proxy_connect_timeout 1s;proxy_pass api_server; }} 12.Tengine 常用维护脚本或命令123456789101112131415161718192021222324252627282930313233343536373839404142#1、查看安装的模块/usr/local/nginx/sbin/nginx -m#2、检测配置文件语法/usr/local/nginx/sbin/nginx -t#3、启动tenginx/usr/local/nginx/sbin/nginx#4、配置文件重新加载/usr/local/nginx/sbin/nginx -s reload#5、关闭nginx/usr/local/nginx/sbin/nginx -s stop#6、日志备份脚本# /bin/bashlogs_path=\"/usr/local/nginx/logs/\"pid_path=\"/usr/local/nginx/logs/nginx.pid\"cut_path=\"/usr/local/nginx/logs/bak/\"[ -e $cut_path ] || mkdir -p $cut_path cd $logs_pathfor log_name in `ls *.log`;do mv ${logs_path}${log_name} ${cut_path}${log_name}_$(date +\"%Y-%m-%d\").logdoneif [[ -s $pid_path ]]; then kill -USR1 `cat ${pid_path}`fifind ${cut_path} -type f -name \"*.log\" -mtime +7 | xargs rm -f 13.参考文档 http://tengine.taobao.org/documentation_cn.html","link":"/2020/01/12/tenginx/"}],"tags":[{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"glusterfs","slug":"glusterfs","link":"/tags/glusterfs/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"ntp","slug":"ntp","link":"/tags/ntp/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"tengine","slug":"tengine","link":"/tags/tengine/"},{"name":"http2","slug":"http2","link":"/tags/http2/"},{"name":"openssl","slug":"openssl","link":"/tags/openssl/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":"yum源","slug":"yum源","link":"/tags/yum%E6%BA%90/"}],"categories":[{"name":"glusterfs","slug":"glusterfs","link":"/categories/glusterfs/"},{"name":"ntp","slug":"ntp","link":"/categories/ntp/"},{"name":"rkhunter","slug":"rkhunter","link":"/categories/rkhunter/"},{"name":"Tengine","slug":"Tengine","link":"/categories/Tengine/"},{"name":"yum","slug":"yum","link":"/categories/yum/"}]}