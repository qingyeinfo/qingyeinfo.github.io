<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>青叶の博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://qingye.info/"/>
  <updated>2020-04-17T17:23:22.831Z</updated>
  <id>http://qingye.info/</id>
  
  <author>
    <name>青叶</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>利用LXCFS提升容器资源可见性</title>
    <link href="http://qingye.info/2020/03/30/k8s/K8S-LXCFS/"/>
    <id>http://qingye.info/2020/03/30/k8s/K8S-LXCFS/</id>
    <published>2020-03-30T15:40:07.000Z</published>
    <updated>2020-04-17T17:23:22.831Z</updated>
    
    <content type="html"><![CDATA[<h2 id="你好呀，本博客已经迁移至-青叶の博客-持续更新，如需访问，请点击-青叶の博客"><a href="#你好呀，本博客已经迁移至-青叶の博客-持续更新，如需访问，请点击-青叶の博客" class="headerlink" title="!!!你好呀，本博客已经迁移至[青叶の博客]持续更新，如需访问，请点击:青叶の博客"></a><font color='red'>!!!你好呀，本博客已经迁移至[青叶の博客]持续更新，如需访问，请点击:<a href="http://qingye.info">青叶の博客</a></font></h2><p>由于默认情况下容器挂载的是宿主机的硬件配置信息，导致有些应用根据这些信息来决定启动内存等的大小，导致应用内存溢出等问题。</p><p>LXCFS简介</p><p>社区中常见的做法是利用 lxcfs来提供容器中的资源可见性。lxcfs 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器，它也可以支持Docker容器。</p><p>LXCFS通过用户态文件系统，在容器中提供下列 procfs 的文件。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">/proc/cpuinfo</span></span><br><span class="line"><span class="string">/proc/diskstats</span></span><br><span class="line"><span class="string">/proc/meminfo</span></span><br><span class="line"><span class="string">/proc/stat</span></span><br><span class="line"><span class="string">/proc/swaps</span></span><br><span class="line"><span class="string">/proc/uptime</span></span><br></pre></td></tr></table></figure><p>LXCFS的示意图如下：</p><p><img src="https://cloud.qingye.info/images/20200330/image2018-11-12_9-20-40.png" alt="LXCFS的示意图"></p><p>比如，把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup中读取正确的内存限制。从而使得应用获得正确的资源约束设定。</p><p>安装lxcfs ，先安装需要使用的依赖包：</p><p>yum install <a href="http://mirror.centos.org/centos/7/os/x86_64/Packages/fuse-libs-2.9.2-10.el7.x86_64.rpm">http://mirror.centos.org/centos/7/os/x86_64/Packages/fuse-libs-2.9.2-10.el7.x86_64.rpm</a></p><p>用deamonset方式在每个节点启动一个lxcfs,lxcfs-daemonset.yaml配置如下:</p><a id="more"></a><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">lxcfs</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">lxcfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">lxcfs</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">lxcfs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lxcfs</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">reg.test.sui.internal/library/lxcfs:2.0.8-1</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cgroup</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/sys/fs/cgroup</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lxcfs</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/lxcfs</span></span><br><span class="line">          <span class="attr">mountPropagation:</span> <span class="string">Bidirectional</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">usr-local</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/local</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cgroup</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/sys/fs/cgroup</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">usr-local</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/usr/local</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lxcfs</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/lxcfs</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br></pre></td></tr></table></figure><p>然后在发版平台deploy的模板配置资源限制的信息，主要信息如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/proc/cpuinfo</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">lxcfs-proc-cpuinfo</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/proc/meminfo</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">lxcfs-proc-meminfo</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/proc/diskstats</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">lxcfs-proc-diskstats</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/proc/stat</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">lxcfs-proc-stat</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/proc/swaps</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">lxcfs-proc-swaps</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/proc/uptime</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">lxcfs-proc-uptime</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pull-registry-secret</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/lxcfs/proc/cpuinfo</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">""</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">lxcfs-proc-cpuinfo</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/lxcfs/proc/diskstats</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">""</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">lxcfs-proc-diskstats</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/lxcfs/proc/meminfo</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">""</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">lxcfs-proc-meminfo</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/lxcfs/proc/stat</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">""</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">lxcfs-proc-stat</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/lxcfs/proc/swaps</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">""</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">lxcfs-proc-swaps</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/lxcfs/proc/uptime</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">""</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">lxcfs-proc-uptime</span></span><br></pre></td></tr></table></figure><p>启动应用之后即可看到内存大小就是cgroup分配的内存大小,注意不要使用alpine镜像，这个镜像挂载仍然有问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;你好呀，本博客已经迁移至-青叶の博客-持续更新，如需访问，请点击-青叶の博客&quot;&gt;&lt;a href=&quot;#你好呀，本博客已经迁移至-青叶の博客-持续更新，如需访问，请点击-青叶の博客&quot; class=&quot;headerlink&quot; title=&quot;!!!你好呀，本博客已经迁移至[青叶の博客]持续更新，如需访问，请点击:青叶の博客&quot;&gt;&lt;/a&gt;&lt;font color=&#39;red&#39;&gt;!!!你好呀，本博客已经迁移至[青叶の博客]持续更新，如需访问，请点击:&lt;a href=&quot;http://qingye.info&quot;&gt;青叶の博客&lt;/a&gt;&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;由于默认情况下容器挂载的是宿主机的硬件配置信息，导致有些应用根据这些信息来决定启动内存等的大小，导致应用内存溢出等问题。&lt;/p&gt;
&lt;p&gt;LXCFS简介&lt;/p&gt;
&lt;p&gt;社区中常见的做法是利用 lxcfs来提供容器中的资源可见性。lxcfs 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器，它也可以支持Docker容器。&lt;/p&gt;
&lt;p&gt;LXCFS通过用户态文件系统，在容器中提供下列 procfs 的文件。&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;/proc/cpuinfo&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;/proc/diskstats&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;/proc/meminfo&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;/proc/stat&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;/proc/swaps&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;/proc/uptime&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;LXCFS的示意图如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cloud.qingye.info/images/20200330/image2018-11-12_9-20-40.png&quot; alt=&quot;LXCFS的示意图&quot;&gt;&lt;/p&gt;
&lt;p&gt;比如，把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup中读取正确的内存限制。从而使得应用获得正确的资源约束设定。&lt;/p&gt;
&lt;p&gt;安装lxcfs ，先安装需要使用的依赖包：&lt;/p&gt;
&lt;p&gt;yum install &lt;a href=&quot;http://mirror.centos.org/centos/7/os/x86_64/Packages/fuse-libs-2.9.2-10.el7.x86_64.rpm&quot;&gt;http://mirror.centos.org/centos/7/os/x86_64/Packages/fuse-libs-2.9.2-10.el7.x86_64.rpm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;用deamonset方式在每个节点启动一个lxcfs,lxcfs-daemonset.yaml配置如下:&lt;/p&gt;
    
    </summary>
    
    
      <category term="k8s" scheme="http://qingye.info/categories/k8s/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>nginx-ingress-controller部署</title>
    <link href="http://qingye.info/2020/03/30/k8s/nginx-ingress-controller/"/>
    <id>http://qingye.info/2020/03/30/k8s/nginx-ingress-controller/</id>
    <published>2020-03-30T15:20:07.000Z</published>
    <updated>2020-04-16T13:13:18.858Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-打标签"><a href="#1-打标签" class="headerlink" title="1.打标签"></a><em>1.打标签</em></h3><p>由于nginx要部署到指定节点上，所以需要对node打个label，通过nodeSelector调度到节点上，同时要确保部署节点主机的80、443、18080、10254端口没有被占用。<br>kubectl label node ss-1-centos221 nginx-ingress=nginx<br>kubectl label node ss-1-centos221-2 nginx-ingress=nginx</p><h3 id="2-准备nginx-ingress-controller部署文件"><a href="#2-准备nginx-ingress-controller部署文件" class="headerlink" title="2.准备nginx-ingress-controller部署文件"></a><em>2.准备nginx-ingress-controller部署文件</em></h3><h4 id="2-1-nginx-ingress-controller-rbac-yaml-：设置rabc权限"><a href="#2-1-nginx-ingress-controller-rbac-yaml-：设置rabc权限" class="headerlink" title="2.1 nginx-ingress-controller-rbac.yaml ：设置rabc权限"></a>2.1 nginx-ingress-controller-rbac.yaml ：设置rabc权限</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-serviceaccount</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-clusterrole</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">configmaps</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">endpoints</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">nodes</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">secrets</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">nodes</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">services</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"extensions"</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ingresses</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">events</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">create</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">patch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"extensions"</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ingresses/status</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">update</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-role</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">configmaps</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">secrets</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">namespaces</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">configmaps</span></span><br><span class="line">    <span class="attr">resourceNames:</span></span><br><span class="line">      <span class="comment"># Defaults to "&lt;election-id&gt;-&lt;ingress-class&gt;"</span></span><br><span class="line">      <span class="comment"># Here: "&lt;ingress-controller-leader&gt;-&lt;nginx&gt;"</span></span><br><span class="line">      <span class="comment"># This has to be adapted if you change either parameter</span></span><br><span class="line">      <span class="comment"># when launching the nginx-ingress-controller.</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"ingress-controller-leader-nginx"</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">update</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">configmaps</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">create</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">endpoints</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-role-nisa-binding</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-role</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-ingress-serviceaccount</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-clusterrole-nisa-binding</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-clusterrole</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-ingress-serviceaccount</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="2-2-nginx-ingress-controller-cm-yaml-：设置配置文件"><a href="#2-2-nginx-ingress-controller-cm-yaml-：设置配置文件" class="headerlink" title="2.2 nginx-ingress-controller-cm.yaml   ：设置配置文件"></a>2.2 nginx-ingress-controller-cm.yaml   ：设置配置文件</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-configuration</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">enable-vts-status:</span> <span class="string">"true"</span></span><br><span class="line">  <span class="attr">vts-default-filter-key:</span> <span class="string">"$server_name"</span></span><br><span class="line">  <span class="attr">proxy-body-size:</span> <span class="string">20m</span></span><br><span class="line">  <span class="attr">upstream-keepalive-connections:</span> <span class="string">"300"</span></span><br><span class="line">  <span class="attr">access-log-path:</span> <span class="string">"/var/log/nginx/access.log"</span></span><br><span class="line">  <span class="attr">error-log-path:</span> <span class="string">"/var/log/nginx/error.log"</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tcp-services</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">1068:</span> <span class="string">"default/example-service-nodeport:80"</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">udp-services</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><h4 id="2-3-nginx-ingress-controller-ds-yaml-：设置nginx-ingress-controller启动配置"><a href="#2-3-nginx-ingress-controller-ds-yaml-：设置nginx-ingress-controller启动配置" class="headerlink" title="2.3 nginx-ingress-controller-ds.yaml    ：设置nginx-ingress-controller启动配置"></a>2.3 nginx-ingress-controller-ds.yaml    ：设置nginx-ingress-controller启动配置</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-controller</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span> </span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">ingress-nginx</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">prometheus.io/port:</span> <span class="string">'10254'</span></span><br><span class="line">        <span class="attr">prometheus.io/scrape:</span> <span class="string">'true'</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">nginx-ingress-serviceaccount</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirstWithHostNet</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-log</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/log/nginx</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-time</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/etc/localtime</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-ingress-controller</span></span><br><span class="line">          <span class="string">image:k8s.qingye.info/test/nginx-ingress-controller:0.15.0</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">/nginx-ingress-controller</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--default-backend-service=$(POD_NAMESPACE)/default-http-backend</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--configmap=$(POD_NAMESPACE)/nginx-configuration</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--udp-services-configmap=$(POD_NAMESPACE)/udp-services</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--annotations-prefix=nginx.ingress.kubernetes.io</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAME</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAMESPACE</span></span><br><span class="line">              <span class="attr">valueFrom:</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                  <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">            <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">            <span class="attr">hostPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">            <span class="attr">containerPort:</span> <span class="number">443</span></span><br><span class="line">            <span class="attr">hostPort:</span> <span class="number">443</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tcp</span></span><br><span class="line">            <span class="attr">containerPort:</span> <span class="number">18080</span></span><br><span class="line">            <span class="attr">hostPort:</span> <span class="number">18080</span></span><br><span class="line">          <span class="attr">livenessProbe:</span></span><br><span class="line">            <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">            <span class="attr">httpGet:</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">/healthz</span></span><br><span class="line">              <span class="attr">port:</span> <span class="number">10254</span></span><br><span class="line">              <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">            <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">            <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">            <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">            <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">readinessProbe:</span></span><br><span class="line">            <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">            <span class="attr">httpGet:</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">/healthz</span></span><br><span class="line">              <span class="attr">port:</span> <span class="number">10254</span></span><br><span class="line">              <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">            <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">            <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">            <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-log</span></span><br><span class="line">            <span class="attr">mountPath:</span> <span class="string">/var/log/nginx/</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">host-time</span></span><br><span class="line">            <span class="attr">mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">"node-role.kubernetes.io/node"</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">"Equal"</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">""</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">"NoSchedule"</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">nginx-ingress:</span> <span class="string">"nginx"</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-controller-service</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">https</span></span><br></pre></td></tr></table></figure><h4 id="2-4-default-http-backend-yaml-：设置默认后端"><a href="#2-4-default-http-backend-yaml-：设置默认后端" class="headerlink" title="2.4  default-http-backend.yaml ：设置默认后端"></a>2.4  default-http-backend.yaml ：设置默认后端</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">60</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">default-http-backend</span></span><br><span class="line">        <span class="comment"># Any image is permissible as long as:</span></span><br><span class="line">        <span class="comment"># 1. It serves a 404 page at /</span></span><br><span class="line">        <span class="comment"># 2. It serves 200 on a /healthz endpoint</span></span><br><span class="line">        <span class="string">image:k8s.qingye.info/test/defaultbackend:1.4</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/healthz</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">20Mi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">20Mi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8080</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br></pre></td></tr></table></figure><h3 id="3-开始安装"><a href="#3-开始安装" class="headerlink" title="3.开始安装"></a><em>3.开始安装</em></h3><p>kubectl create -f  nginx-ingress-controller-rbac.yaml</p><p>kubectl create -f  nginx-ingress-controller-cm.yaml </p><p>kubectl create -f  nginx-ingress-controller-ds.yaml</p><p>kubectl create -f  default-http-backend.yaml</p><h3 id="4-安装完毕后，看下有没成功启动"><a href="#4-安装完毕后，看下有没成功启动" class="headerlink" title="4.安装完毕后，看下有没成功启动"></a><em>4.安装完毕后，看下有没成功启动</em></h3><p>kubectl get pod -n kube-system |egrep “backend|nginx”<br><img src="https://cloud.qingye.info/images/20200330/image2019-4-21_16-42-55.png" alt="nginx-ingress启动成功截图"></p><p>启动成功。可以访问管理页面查看流量状态 <a href="http://ip:18080/nginx_status">http://ip:18080/nginx_status</a> 或者<a href="http://ip:18080/nginx_status">http://ip:18080/nginx_status</a></p><p><img src="https://cloud.qingye.info/images/20200330/image2019-4-21_16-44-29.png" alt="管理页面截图"></p><h3 id="5-配置ingres，以下是一个例子"><a href="#5-配置ingres，以下是一个例子" class="headerlink" title="5.配置ingres，以下是一个例子"></a><em>5.配置ingres，以下是一个例子</em></h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-scope</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">"nginx"</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/vts-filter-key:</span> <span class="string">$uri</span> <span class="string">$server_name</span></span><br><span class="line">    <span class="attr">prometheus.io/probe:</span> <span class="string">'true'</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">k8s.scope.qingye.cn</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">         <span class="attr">serviceName:</span> <span class="string">weave-scope-app</span></span><br><span class="line">         <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-打标签&quot;&gt;&lt;a href=&quot;#1-打标签&quot; class=&quot;headerlink&quot; title=&quot;1.打标签&quot;&gt;&lt;/a&gt;&lt;em&gt;1.打标签&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;由于nginx要部署到指定节点上，所以需要对node打个label，通过nodeSelector调度到节点上，同时要确保部署节点主机的80、443、18080、10254端口没有被占用。&lt;br&gt;kubectl label node ss-1-centos221 nginx-ingress=nginx&lt;br&gt;kubectl label node ss-1-centos221-2 nginx-ingress=nginx&lt;/p&gt;
&lt;h3 id=&quot;2-准备nginx-ingress-controller部署文件&quot;&gt;&lt;a href=&quot;#2-准备nginx-ingress-controller部署文件&quot; class=&quot;headerlink&quot; title=&quot;2.准备nginx-ingress-controller部署文件&quot;&gt;&lt;/a&gt;&lt;em&gt;2.准备nginx-ingress-controller部署文件&lt;/em&gt;&lt;/h3&gt;&lt;h4 id=&quot;2-1-nginx-ingress-controller-rbac-yaml-：设置rabc权限&quot;&gt;&lt;a href=&quot;#2-1-nginx-ingress-controller-rbac-yaml-：设置rabc权限&quot; class=&quot;headerlink&quot; title=&quot;2.1 nginx-ingress-controller-rbac.yaml ：设置rabc权限&quot;&gt;&lt;/a&gt;2.1 nginx-ingress-controller-rbac.yaml ：设置rabc权限&lt;/h4&gt;&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;83&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;84&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;85&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;86&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;87&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;88&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;89&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;90&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;91&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;92&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;93&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;94&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;95&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;96&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;97&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;98&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;99&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;100&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;101&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;102&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;103&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;104&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;105&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;106&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;107&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;108&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;109&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;110&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;111&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;112&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;113&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;114&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;115&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;116&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;117&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;118&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;119&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;120&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;121&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;122&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;123&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;124&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;125&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;126&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;---&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;apiVersion:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;v1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ServiceAccount&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-serviceaccount&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;namespace:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;kube-system&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;---&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;apiVersion:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ClusterRole&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-clusterrole&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;rules:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;configmaps&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;endpoints&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nodes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;pods&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;secrets&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;list&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;watch&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nodes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;get&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;services&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;get&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;list&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;watch&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;extensions&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ingresses&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;get&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;list&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;watch&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;events&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;create&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;patch&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;extensions&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ingresses/status&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;update&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;---&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;apiVersion:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;Role&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-role&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;namespace:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;kube-system&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;rules:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;configmaps&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;pods&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;secrets&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;namespaces&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;get&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;configmaps&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resourceNames:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;comment&quot;&gt;# Defaults to &quot;&amp;lt;election-id&amp;gt;-&amp;lt;ingress-class&amp;gt;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;comment&quot;&gt;# Here: &quot;&amp;lt;ingress-controller-leader&amp;gt;-&amp;lt;nginx&amp;gt;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;comment&quot;&gt;# This has to be adapted if you change either parameter&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;comment&quot;&gt;# when launching the nginx-ingress-controller.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;ingress-controller-leader-nginx&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;get&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;update&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;configmaps&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;create&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;endpoints&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;get&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;---&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;apiVersion:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;RoleBinding&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-role-nisa-binding&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;namespace:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;kube-system&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;roleRef:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;apiGroup:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;Role&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-role&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;subjects:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ServiceAccount&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-serviceaccount&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;namespace:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;kube-system&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;---&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;apiVersion:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ClusterRoleBinding&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-clusterrole-nisa-binding&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;roleRef:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;apiGroup:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ClusterRole&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-clusterrole&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;subjects:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ServiceAccount&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;nginx-ingress-serviceaccount&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;attr&quot;&gt;namespace:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;kube-system&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="k8s" scheme="http://qingye.info/categories/k8s/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>跨域请求以及实现跨域的方案</title>
    <link href="http://qingye.info/2020/03/29/linux/Cross-domain-requests/"/>
    <id>http://qingye.info/2020/03/29/linux/Cross-domain-requests/</id>
    <published>2020-03-29T12:09:07.000Z</published>
    <updated>2020-04-16T13:14:08.552Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-什么是跨域请求"><a href="#1-什么是跨域请求" class="headerlink" title="1.什么是跨域请求"></a><em>1.什么是跨域请求</em></h3><p>A cross-domain solution (CDS) is a means of information assurance that provides the ability to manually or automatically access or transfer between two or more differing security domains.</p><p>解决两个安全域之间的信息传递，这个就叫做CDS——跨域解决方案.</p><p>在 HTML 中，&lt;a&gt;, &lt;form&gt;, &lt;img&gt;, &lt;script&gt;, &lt;iframe&gt;, &lt;link&gt;等标签以及 Ajax(异步 JavaScript 和 XML) 都可以指向一个资源地址，而所谓的跨域请求就是指：当前发起请求的域与该请求指向的资源所在的域不一样。<br>这里的域指的是这样的一个概念：我们认为若协议 + 域名 + 端口号均相同，那么就是同域.</p><h3 id="2-跨域请求的安全问题"><a href="#2-跨域请求的安全问题" class="headerlink" title="2.跨域请求的安全问题"></a><em>2.跨域请求的安全问题</em></h3><p>通常，浏览器会对上面提到的跨域请求作出限制。浏览器之所以要对跨域请求作出限制，是出于安全方面的考虑，因为跨域请求有可能被不法分子利用来发动 CSRF攻击。</p><p>CSRF攻击：<br>CSRF（Cross-site request forgery），中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。CSRF攻击者在用户已经登录目标网站之后，诱使用户访问一个攻击页面，利用目标网站对用户的信任，以用户身份在攻击页面对目标网站发起伪造用户操作的请求，达到攻击目的。</p><p>CSRF 攻击的原理大致描述如下：有两个网站，其中A网站是真实受信任的网站，而B网站是危险网站。在用户登陆了受信任的A网站是，本地会存储A网站相关的Cookie，并且浏览器也维护这一个Session会话。这时，如果用户在没有登出A网站的情况下访问危险网站B，那么危险网站B就可以模拟发出一个对A网站的请求（跨域请求）对A网站进行操作，而在A网站的角度来看是并不知道请求是由B网站发出来的（Session和Cookie均为A网站的），这时便成功发动一次CSRF 攻击。</p><p>因而 CSRF 攻击可以简单理解为：攻击者盗用了你的身份，以你的名义发送而已请求。CSRF能够做的事情包括：以你名义发送邮件，发消息，盗取你的账号，甚至于购买商品，虚拟货币转账……造成的问题包括：个人隐私泄露以及财产安全。</p><h3 id="3-同源策略"><a href="#3-同源策略" class="headerlink" title="3.同源策略"></a><em>3.同源策略</em></h3><p>在客户端编程语言中，如javascript和ActionScript，同源策略是一个很重要的安全理念，它在保证数据的安全性方面有着重要的意义。同源策略规定跨域之间的脚本是隔离的，一个域的脚本不能访问和操作另外一个域的绝大部分属性和方法</p><p>概述：<br>同源策略是 Netscape 提出的一个著名的安全策略<br>同源策略是浏览器最核心最基础的安全策略<br>现在所有的可支持 Javascript 的浏览器都会使用这个策略<br>web构建在同源策略基础之上，浏览器对非同源脚本的限制措施是对同源策略的具体实现</p><p>同源策略的含义：<br>DOM 层面的同源策略：限制了来自不同源的”Document”对象或 JS 脚本，对当前“document”对象的读取或设置某些属性<br>Cookie和XMLHttprequest层面的同源策略：禁止 Ajax 直接发起跨域HTTP请求（其实可以发送请求，结果被浏览器拦截，不展示），同时 Ajax 请求不能携带与本网站不同源的 Cookie。<br>同源策略的非绝对性：&lt;script&gt;&lt;img&gt;&lt;iframe&gt;&lt;link&gt;&lt;video&gt;&lt;audio&gt;等带有src属性的标签可以从不同的域加载和执行资源。<br>其他插件的同源策略：flash、java applet、silverlight、googlegears等浏览器加载的第三方插件也有各自的同源策略，只是这些同源策略不属于浏览器原生的同源策略，如果有漏洞则可能被黑客利用，从而留下XSS攻击的后患</p><a id="more"></a><p>跨域例子：<br><img src="https://cloud.qingye.info/images/20200329/cross.png" alt="跨域例子"></p><p>特例:</p><p>Web页面上调用js文件时则不受是否跨域的影响,不仅如此，凡是拥有”src”这个属性的标签都拥有跨域的能力，比如&lt;script&gt;、&lt;img&gt;、&lt;iframe&gt;.</p><h3 id="4-跨域解决方法"><a href="#4-跨域解决方法" class="headerlink" title="4.跨域解决方法"></a><em>4.跨域解决方法</em></h3><p>虽然在安全层面上同源限制是必要的，但有时同源策略会对我们的合理用途造成影响，为了避免开发的应用受到限制，有多种方式可以绕开同源策略，我们经常使用的 JSONP, CORS 方法</p><h4 id="4-1-JSONP"><a href="#4-1-JSONP" class="headerlink" title="4.1 JSONP"></a><em>4.1 JSONP</em></h4><p>原理：</p><ul><li><p>JSONP 是一种非官方的跨域数据交互协议</p></li><li><p>JSONP 本质上是利用 &lt;script&gt;&lt;img&gt;&lt;iframe&gt; 等标签不受同源策略限制，可以从不同域加载并执行资源的特性，来实现数据跨域传输。</p></li><li><p>JSONP由两部分组成：回调函数和数据。回调函数是当响应到来时应该在页面中调用的函数，而数据就是传入回调函数中的JSON数据。</p></li><li><p>JSONP 的理念就是，与服务端约定好一个回调函数名，服务端接收到请求后，将返回一段 Javascript，在这段 Javascript 代码中调用了约定好的回调函数，并且将数据作为参数进行传递。当网页接收到这段Javascript 代码后，就会执行这个回调函数，这时数据已经成功传输到客户端了。<br>示例：<br>定义两个域名,分别为a.test.com和b.test.com,在a.test.com/jsonp3.html页面定义一个函数，然后在远程b.test.com/remote3.js中传入数据进行调用</p><p>a.test.com/jsonp3.html</p></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">xmlns</span>=<span class="string">"http://www.w3.org/1999/xhtml"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span></span><br><span class="line"><span class="actionscript">    <span class="comment">// 得到航班信息查询结果后的回调函数</span></span></span><br><span class="line"><span class="actionscript">    <span class="keyword">var</span> flightHandler = <span class="function"><span class="keyword">function</span><span class="params">(data)</span></span>&#123;</span></span><br><span class="line"><span class="actionscript">        alert(<span class="string">'你查询的航班结果是：票价 '</span> + data.price + <span class="string">' 元，'</span> + <span class="string">'余票 '</span> + data.tickets + <span class="string">' 张。'</span>);</span></span><br><span class="line">    &#125;;</span><br><span class="line"><span class="actionscript">    <span class="comment">// 提供jsonp服务的url地址（不管是什么类型的地址，最终生成的返回值都是一段javascript代码）</span></span></span><br><span class="line"><span class="actionscript">    <span class="keyword">var</span> url = <span class="string">"http://b.test.com/remote3.js?callback=flightHandler"</span>;</span></span><br><span class="line"><span class="actionscript">    <span class="comment">// 创建script标签，设置其属性</span></span></span><br><span class="line"><span class="javascript">    <span class="keyword">var</span> script = <span class="built_in">document</span>.createElement(<span class="string">'script'</span>);</span></span><br><span class="line"><span class="actionscript">    script.setAttribute(<span class="string">'src'</span>, url);</span></span><br><span class="line"><span class="actionscript">    <span class="comment">// 把script标签加入head，此时调用开始</span></span></span><br><span class="line"><span class="javascript">    <span class="built_in">document</span>.getElementsByTagName(<span class="string">'head'</span>)[<span class="number">0</span>].appendChild(script);</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>b.test.com/remote3.js里面的内容</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flightHandler(&#123;</span><br><span class="line">    <span class="string">"price"</span>: <span class="number">1780</span>,</span><br><span class="line">    <span class="string">"tickets"</span>: <span class="number">5</span></span><br><span class="line">&#125;,</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>访问<a href="http://a.test.com/jsonp3.html">http://a.test.com/jsonp3.html</a></p><p><img src="https://cloud.qingye.info/images/20200329/jsonp.png" alt="jsonp例子"></p><p>JSONP优缺点：<br>JSONP 的优点是：它不像XMLHttpRequest对象实现的Ajax请求那样受到同源策略的限制；它的兼容性更好，在更加古老的浏览器中都可以运行。</p><p>JSONP 的缺点是：它只支持 GET 请求，而不支持 POST 请求等其他类型的 HTTP 请求</p><h4 id="4-2-CORS"><a href="#4-2-CORS" class="headerlink" title="4.2 CORS"></a><em>4.2 CORS</em></h4><p>介绍<br>跨源资源共享 Cross-Origin Resource Sharing(CORS) 是一个新的 W3C 标准，它新增的一组HTTP首部字段，允许服务端其声明哪些源站有权限访问哪些资源。换言之，它允许浏览器向声明了 CORS 的跨域服务器，发出 XMLHttpReuest 请求，从而克服 Ajax 只能同源使用的限制。</p><p>另外，规范也要求对于非简单请求，浏览器必须首先使用 OPTION 方法发起一个预检请求(preflight request)，从而获知服务端是否允许该跨域请求，在服务器确定允许后，才发起实际的HTTP请求。对于简单请求、非简单请求以及预检请求的详细资料可以阅读HTTP访问控制（CORS） 。</p><p>HTTP 协议 Header 简析<br>下面对 CORS 中新增的 HTTP 首部字段进行简析：</p><p>Access-Control-Allow-Origin</p><p>响应首部中可以携带这个头部表示服务器允许哪些域可以访问该资源，其语法如下：</p><p>Access-Control-Allow-Origin: &lt;origin&gt; | *<br>其中，origin 参数的值指定了允许访问该资源的外域 URI。对于不需要携带身份凭证的请求，服务器可以指定该字段的值为通配符，表示允许来自所有域的请求。</p><p>Access-Control-Allow-Methods</p><p>该首部字段用于预检请求的响应，指明实际请求所允许使用的HTTP方法。其语法如下：</p><p>Access-Control-Allow-Methods: &lt;method&gt;[, &lt;method&gt;]*<br>Access-Control-Allow-Headers</p><p>该首部字段用于预检请求的响应。指明了实际请求中允许携带的首部字段。其语法如下：</p><p>Access-Control-Allow-Headers: &lt;field-name&gt;[, &lt;field-name&gt;]*<br>Access-Control-Max-Age</p><p>该首部字段用于预检请求的响应，指定了预检请求能够被缓存多久，其语法如下：</p><p>Access-Control-Max-Age: &lt;delta-seconds&gt;</p><p>Access-Control-Allow-Credentials</p><p>该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。其语法如下：</p><p>Access-Control-Allow-Credentials: true<br>另外，如果要把 Cookie 发送到服务器，除了服务端要带上Access-Control-Allow-Credentials首部字段外，另一方面请求中也要带上withCredentials属性。</p><p>但是需要注意的是：如果需要在 Ajax 中设置和获取 Cookie，那么Access-Control-Allow-Origin首部字段不能设置为* ，必须设置为具体的 origin 源站。详细可阅读文章CORS 跨域 Cookie 的设置与获取</p><p>示例<br>在a.test.com/test.html页面配置获取b.test.com/test2.html页面的内容,下图 &lt;img src=”<a href="http://b.test.com/test.png&quot;">http://b.test.com/test.png&quot;</a> /&gt; 是为了证明html带有src标签都不受同源策略的限制</p><p><a href="http://a.test.com/test.html">http://a.test.com/test.html</a></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span> </span><br><span class="line"><span class="actionscript"><span class="keyword">var</span> http = <span class="keyword">new</span> XMLHttpRequest(); </span></span><br><span class="line"><span class="actionscript">http.open(<span class="string">"get"</span>, <span class="string">"http://b.test.com/test2.html"</span>, <span class="literal">true</span>);</span></span><br><span class="line">if (http)</span><br><span class="line">&#123; </span><br><span class="line"><span class="actionscript">    http.onload = <span class="function"><span class="keyword">function</span><span class="params">()</span> </span></span></span><br><span class="line">    &#123;  </span><br><span class="line">        alert(http.responseText); </span><br><span class="line">    &#125; </span><br><span class="line">    http.send(); </span><br><span class="line">&#125; </span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"http://b.test.com/test.png"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p><a href="http://b.test.com/test2.html">http://b.test.com/test2.html</a></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">head</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;/<span class="name">head</span>&gt;</span> </span><br><span class="line">     <span class="tag">&lt;<span class="name">body</span>&gt;</span>  </span><br><span class="line">     hello</span><br><span class="line">     <span class="tag">&lt;/<span class="name">body</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在b.test.com的服务端未配置跨域配置访问<a href="http://a.test.com/test.html结果">http://a.test.com/test.html结果</a><br><img src="https://cloud.qingye.info/images/20200329/corss-2.png" alt="跨域访问结果"></p><p>在b.test.com的nginx服务端配置跨域配置访问<a href="http://a.test.com/test.html">http://a.test.com/test.html</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">server</span> <span class="string">&#123;</span></span><br><span class="line"><span class="string">listen</span> <span class="number">80</span> <span class="string">;</span></span><br><span class="line"><span class="string">server_name</span> <span class="string">b.test.com;</span></span><br><span class="line"><span class="string">index</span> <span class="string">index.jsp;</span></span><br><span class="line"><span class="string">req_status</span> <span class="string">server;</span></span><br><span class="line"><span class="string">proxy_set_header</span> <span class="string">Host</span> <span class="string">$host;</span></span><br><span class="line"><span class="string">proxy_set_header</span> <span class="string">X-Real-IP</span> <span class="string">$remote_addr;</span></span><br><span class="line"><span class="string">proxy_set_header</span> <span class="string">X-Forwarded-For</span> <span class="string">$proxy_add_x_forwarded_for;</span></span><br><span class="line"><span class="string">proxy_pass_header</span> <span class="string">User-Agent;</span></span><br><span class="line"><span class="string">proxy_set_header</span> <span class="string">X-Forwarded-Proto</span> <span class="string">$scheme;</span></span><br><span class="line"><span class="string">access_log</span> <span class="string">logs/access.log</span> <span class="string">main_1;</span></span><br><span class="line"> </span><br><span class="line"><span class="string">add_header</span> <span class="string">Access-Control-Allow-Origin</span> <span class="string">http://a.test.com;</span></span><br><span class="line"><span class="string">add_header</span> <span class="string">Access-Control-Allow-Methods</span> <span class="string">GET,POST,OPTIONS;</span></span><br><span class="line"><span class="string">add_header</span> <span class="string">Access-Control-Allow-Credentials</span> <span class="literal">true</span><span class="string">;</span></span><br><span class="line"><span class="string">add_header</span> <span class="string">Access-Control-Allow-Headers</span> <span class="string">DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type;</span></span><br><span class="line"><span class="string">add_header</span> <span class="string">Access-Control-Max-Age</span> <span class="number">1728000</span><span class="string">;</span></span><br><span class="line"><span class="string">location</span> <span class="string">/</span> <span class="string">&#123;</span></span><br><span class="line"><span class="string">root</span> <span class="string">html;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>访问效果,可以正常访问</p><p><img src="https://cloud.qingye.info/images/20200329/corss-3.png" alt="跨域访问结果2"></p><p>与 JSONP 的比较</p><ul><li>JSONP 只能实现 GET 请求，而 CORS 支持所有类型的 HTTP 请求</li><li>使用 CORS ，开发者可以是使用普通的 XMLHttpRequest 发起请求和获取数据，比起 JSONP 有更好的错误处理虽然绝大多数现代的浏览器都已经支持 CORS，但是 CORS 的兼容性比不上 JSONP，一些比较老的浏览器只支持 JSONP</li></ul><h3 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5.参考资料"></a><em>5.参考资料</em></h3><p>1.<a href="https://www.jianshu.com/p/f880878c1398">https://www.jianshu.com/p/f880878c1398</a></p><p>2.<a href="http://http//www.cnblogs.com/dowinning/archive/2012/04/19/json-jsonp-jquery.html">http://http//www.cnblogs.com/dowinning/archive/2012/04/19/json-jsonp-jquery.html</a></p><p>3.<a href="http://www.cnblogs.com/hustskyking/articles/ten-methods-cross-domain.html">http://www.cnblogs.com/hustskyking/articles/ten-methods-cross-domain.html</a></p><p>4.<a href="http://www.cnblogs.com/chopper/archive/2012/03/24/2403945.html">http://www.cnblogs.com/chopper/archive/2012/03/24/2403945.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-什么是跨域请求&quot;&gt;&lt;a href=&quot;#1-什么是跨域请求&quot; class=&quot;headerlink&quot; title=&quot;1.什么是跨域请求&quot;&gt;&lt;/a&gt;&lt;em&gt;1.什么是跨域请求&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;A cross-domain solution (CDS) is a means of information assurance that provides the ability to manually or automatically access or transfer between two or more differing security domains.&lt;/p&gt;
&lt;p&gt;解决两个安全域之间的信息传递，这个就叫做CDS——跨域解决方案.&lt;/p&gt;
&lt;p&gt;在 HTML 中，&amp;lt;a&amp;gt;, &amp;lt;form&amp;gt;, &amp;lt;img&amp;gt;, &amp;lt;script&amp;gt;, &amp;lt;iframe&amp;gt;, &amp;lt;link&amp;gt;等标签以及 Ajax(异步 JavaScript 和 XML) 都可以指向一个资源地址，而所谓的跨域请求就是指：当前发起请求的域与该请求指向的资源所在的域不一样。&lt;br&gt;这里的域指的是这样的一个概念：我们认为若协议 + 域名 + 端口号均相同，那么就是同域.&lt;/p&gt;
&lt;h3 id=&quot;2-跨域请求的安全问题&quot;&gt;&lt;a href=&quot;#2-跨域请求的安全问题&quot; class=&quot;headerlink&quot; title=&quot;2.跨域请求的安全问题&quot;&gt;&lt;/a&gt;&lt;em&gt;2.跨域请求的安全问题&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;通常，浏览器会对上面提到的跨域请求作出限制。浏览器之所以要对跨域请求作出限制，是出于安全方面的考虑，因为跨域请求有可能被不法分子利用来发动 CSRF攻击。&lt;/p&gt;
&lt;p&gt;CSRF攻击：&lt;br&gt;CSRF（Cross-site request forgery），中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。CSRF攻击者在用户已经登录目标网站之后，诱使用户访问一个攻击页面，利用目标网站对用户的信任，以用户身份在攻击页面对目标网站发起伪造用户操作的请求，达到攻击目的。&lt;/p&gt;
&lt;p&gt;CSRF 攻击的原理大致描述如下：有两个网站，其中A网站是真实受信任的网站，而B网站是危险网站。在用户登陆了受信任的A网站是，本地会存储A网站相关的Cookie，并且浏览器也维护这一个Session会话。这时，如果用户在没有登出A网站的情况下访问危险网站B，那么危险网站B就可以模拟发出一个对A网站的请求（跨域请求）对A网站进行操作，而在A网站的角度来看是并不知道请求是由B网站发出来的（Session和Cookie均为A网站的），这时便成功发动一次CSRF 攻击。&lt;/p&gt;
&lt;p&gt;因而 CSRF 攻击可以简单理解为：攻击者盗用了你的身份，以你的名义发送而已请求。CSRF能够做的事情包括：以你名义发送邮件，发消息，盗取你的账号，甚至于购买商品，虚拟货币转账……造成的问题包括：个人隐私泄露以及财产安全。&lt;/p&gt;
&lt;h3 id=&quot;3-同源策略&quot;&gt;&lt;a href=&quot;#3-同源策略&quot; class=&quot;headerlink&quot; title=&quot;3.同源策略&quot;&gt;&lt;/a&gt;&lt;em&gt;3.同源策略&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;在客户端编程语言中，如javascript和ActionScript，同源策略是一个很重要的安全理念，它在保证数据的安全性方面有着重要的意义。同源策略规定跨域之间的脚本是隔离的，一个域的脚本不能访问和操作另外一个域的绝大部分属性和方法&lt;/p&gt;
&lt;p&gt;概述：&lt;br&gt;同源策略是 Netscape 提出的一个著名的安全策略&lt;br&gt;同源策略是浏览器最核心最基础的安全策略&lt;br&gt;现在所有的可支持 Javascript 的浏览器都会使用这个策略&lt;br&gt;web构建在同源策略基础之上，浏览器对非同源脚本的限制措施是对同源策略的具体实现&lt;/p&gt;
&lt;p&gt;同源策略的含义：&lt;br&gt;DOM 层面的同源策略：限制了来自不同源的”Document”对象或 JS 脚本，对当前“document”对象的读取或设置某些属性&lt;br&gt;Cookie和XMLHttprequest层面的同源策略：禁止 Ajax 直接发起跨域HTTP请求（其实可以发送请求，结果被浏览器拦截，不展示），同时 Ajax 请求不能携带与本网站不同源的 Cookie。&lt;br&gt;同源策略的非绝对性：&amp;lt;script&amp;gt;&amp;lt;img&amp;gt;&amp;lt;iframe&amp;gt;&amp;lt;link&amp;gt;&amp;lt;video&amp;gt;&amp;lt;audio&amp;gt;等带有src属性的标签可以从不同的域加载和执行资源。&lt;br&gt;其他插件的同源策略：flash、java applet、silverlight、googlegears等浏览器加载的第三方插件也有各自的同源策略，只是这些同源策略不属于浏览器原生的同源策略，如果有漏洞则可能被黑客利用，从而留下XSS攻击的后患&lt;/p&gt;
    
    </summary>
    
    
      <category term="http" scheme="http://qingye.info/categories/http/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="http" scheme="http://qingye.info/tags/http/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="cross-domain" scheme="http://qingye.info/tags/cross-domain/"/>
    
      <category term="跨域" scheme="http://qingye.info/tags/%E8%B7%A8%E5%9F%9F/"/>
    
  </entry>
  
  <entry>
    <title>RESTful API 了解</title>
    <link href="http://qingye.info/2020/03/29/linux/restful-api/"/>
    <id>http://qingye.info/2020/03/29/linux/restful-api/</id>
    <published>2020-03-29T11:40:07.000Z</published>
    <updated>2020-03-29T11:52:27.138Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-REST概念"><a href="#1-REST概念" class="headerlink" title="1.REST概念"></a><em>1.REST概念</em></h3><p>REST全称是Representational State Transfer。要理解RESTful架构，需要理解Representational State Transfer这个词组到底是什么意思，它的每一个词都有些什么涵义。下面我们结合REST原则，围绕资源展开讨论，从资源的定义、获取、表述、关联、状态变迁等角度，列举一些关键概念并加以解释。</p><ul><li>资源与URI</li><li>统一资源接口</li><li>资源的表述</li><li>资源的链接</li><li>状态的转移</li></ul><h3 id="2-RESTful-API概念"><a href="#2-RESTful-API概念" class="headerlink" title="2.RESTful API概念"></a><em>2.RESTful API概念</em></h3><p>在开发的过程中，我们经常会听到前后端分离这个技术名词，顾名思义，就是前台的开发和后台的开发分离开。这个技术方案的实现就是要借助API，API简单说就是开发人员提供编程接口被其他人调用，他们调用之后会返回数据供其使用。API的类型有多种，但是现在比较主流且实用的就是RESTful API。</p><p>RESTful API 的总结：<br>1.每一个URL代表一种资源<br>2.客户端和服务器之间，传递这种资源的某种表现层<br>3.客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。具体为：<br>GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。</p><h4 id="2-1-RESTful-API方法-GET"><a href="#2-1-RESTful-API方法-GET" class="headerlink" title="2.1 RESTful API方法-GET"></a><em>2.1 RESTful API方法-GET</em></h4><ul><li><p>安全且幂等</p></li><li><p>获取表示</p></li><li><p>变更时获取表示（缓存）</p></li><li><p>200（OK） - 表示已在响应中发出</p></li><li><p>204（无内容） - 资源有空表示</p></li><li><p>301（Moved Permanently） - 资源的URI已被更新</p></li><li><p>303（See Other） - 其他（如，负载均衡）</p></li><li><p>304（not modified）- 资源未更改（缓存）</p></li><li><p>400 （bad request）- 指代坏请求（如，参数错误）</p></li><li><p>404 （not found）- 资源不存在</p></li><li><p>406 （not acceptable）- 服务端不支持所需表示</p></li><li><p>500 （internal server error）- 通用错误响应</p></li><li><p>503 （Service Unavailable）- 服务端当前无法处理请求</p><a id="more"></a><h4 id="2-2-RESTful-API方法-POST"><a href="#2-2-RESTful-API方法-POST" class="headerlink" title="2.2 RESTful API方法-POST"></a><em>2.2 RESTful API方法-POST</em></h4></li><li><p>不安全且不幂等</p></li><li><p>使用服务端管理的（自动产生）的实例号创建资源</p></li><li><p>创建子资源</p></li><li><p>部分更新资源</p></li><li><p>如果没有被修改，则不过更新资源（乐观锁）</p></li><li><p>200（OK）- 如果现有资源已被更改</p></li><li><p>201（created）- 如果新资源被创建</p></li><li><p>202（accepted）- 已接受处理请求但尚未完成（异步处理）</p></li><li><p>301（Moved Permanently）- 资源的URI被更新</p></li><li><p>303（See Other）- 其他（如，负载均衡）</p></li><li><p>400（bad request）- 指代坏请求</p></li><li><p>404 （not found）- 资源不存在</p></li><li><p>406 （not acceptable）- 服务端不支持所需表示</p></li><li><p>409 （conflict）- 通用冲突</p></li><li><p>412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突）</p></li><li><p>415 （unsupported media type）- 接受到的表示不受支持</p></li><li><p>500 （internal server error）- 通用错误响应</p></li><li><p>503 （Service Unavailable）- 服务当前无法处理请求</p></li></ul><h4 id="2-3-RESTful-API方法-PUT"><a href="#2-3-RESTful-API方法-PUT" class="headerlink" title="2.3 RESTful API方法-PUT"></a><em>2.3 RESTful API方法-PUT</em></h4><ul><li>不安全但幂等</li><li>用客户端管理的实例号创建一个资源</li><li>通过替换的方式更新资源</li><li>如果未被修改，则更新资源（乐观锁）</li><li>200 （OK）- 如果已存在资源被更改</li><li>201 （created）- 如果新资源被创建</li><li>301（Moved Permanently）- 资源的URI已更改</li><li>303 （See Other）- 其他（如，负载均衡）</li><li>400 （bad request）- 指代坏请求</li><li>404 （not found）- 资源不存在</li><li>406 （not acceptable）- 服务端不支持所需表示</li><li>409 （conflict）- 通用冲突</li><li>412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突）</li><li>415 （unsupported media type）- 接受到的表示不受支持</li><li>500 （internal server error）- 通用错误响应</li><li>503 （Service Unavailable）- 服务当前无法处理请求</li></ul><h4 id="2-4-RESTful-API方法-DELETE"><a href="#2-4-RESTful-API方法-DELETE" class="headerlink" title="2.4 RESTful API方法-DELETE"></a><em>2.4 RESTful API方法-DELETE</em></h4><ul><li>不安全但幂等</li><li>删除资源</li><li>200 （OK）- 资源已被删除</li><li>301 （Moved Permanently）- 资源的URI已更改</li><li>303 （See Other）- 其他，如负载均衡</li><li>400 （bad request）- 指代坏请求</li><li>404 （not found）- 资源不存在</li><li>409 （conflict）- 通用冲突</li><li>500 （internal server error）- 通用错误响应</li><li>503 （Service Unavailable）- 服务端当前无法处理请求</li></ul><h3 id="3-常见问题"><a href="#3-常见问题" class="headerlink" title="3.常见问题"></a><em>3.常见问题</em></h3><ul><li>POST和PUT用于创建资源时有什么区别?<br>POST和PUT在创建资源的区别在于，所创建的资源的名称(URI)是否由客户端决定。 例如为我的博文增加一个java的分类，生成的路径就是分类名/categories/java，那么就可以采用PUT方法。不过很多人直接把POST、GET、PUT、DELETE直接对应上CRUD，例如在一个典型的rails实现的RESTful应用中就是这么做的。</li></ul><p>我认为，这是因为rails默认使用服务端生成的ID作为URI的缘故，而不少人就是通过rails实践REST的，所以很容易造成这种误解。</p><ul><li>客户端不一定都支持这些HTTP方法吧?<br>的确有这种情况，特别是一些比较古老的基于浏览器的客户端，只能支持GET和POST两种方法。</li></ul><p>在实践上，客户端和服务端都可能需要做一些妥协。例如rails框架就支持通过隐藏参数_method=DELETE来传递真实的请求方法， 而像Backbone这样的客户端MVC框架则允许传递_method传输和设置X-HTTP-Method-Override头来规避这个问题。</p><ul><li>统一接口是否意味着不能扩展带特殊语义的方法?<br>统一接口并不阻止你扩展方法，只要方法对资源的操作有着具体的、可识别的语义即可，并能够保持整个接口的统一性。</li></ul><p>像WebDAV就对HTTP方法进行了扩展，增加了LOCK、UPLOCK等方法。而github的API则支持使用PATCH方法来进行issue的更新，例如:</p><p>PATCH /repos/:owner/:repo/issues/:number<br>不过，需要注意的是，像PATCH这种不是HTTP标准方法的，服务端需要考虑客户端是否能够支持的问题。</p><ul><li>统一资源接口对URI有什么指导意义?<br>统一资源接口要求使用标准的HTTP方法对资源进行操作，所以URI只应该来表示资源的名称，而不应该包括资源的操作。</li></ul><p>通俗来说，URI不应该使用动作来描述。例如，下面是一些不符合统一接口要求的URI:</p><p>GET /getUser/1<br>POST /createUser<br>PUT /updateUser/1<br>DELETE /deleteUser/1</p><h3 id="4-RESTful-API-应用"><a href="#4-RESTful-API-应用" class="headerlink" title="4.RESTful API 应用"></a><em>4.RESTful API 应用</em></h3><ul><li>Django Rest framework</li></ul><p>1.建立 Models<br>2.依靠 Serialiers 将数据库取出的数据 Parse 为 API 的数据（可用于返回给客户端，也可用于浏览器显示）<br>3.ViewSet 是一个 views 的集合，根据客户端的请求（GET、POST等），返回 Serialiers 处理的数据，权限 Premissions 也在这一步做处理<br>4.ViewSet 可在 Routers 进行注册，注册后会显示在 Api Root 页上<br>5.在 urls 里注册 ViewSet 生成的 view，指定监听的 url</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-REST概念&quot;&gt;&lt;a href=&quot;#1-REST概念&quot; class=&quot;headerlink&quot; title=&quot;1.REST概念&quot;&gt;&lt;/a&gt;&lt;em&gt;1.REST概念&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;REST全称是Representational State Transfer。要理解RESTful架构，需要理解Representational State Transfer这个词组到底是什么意思，它的每一个词都有些什么涵义。下面我们结合REST原则，围绕资源展开讨论，从资源的定义、获取、表述、关联、状态变迁等角度，列举一些关键概念并加以解释。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;资源与URI&lt;/li&gt;
&lt;li&gt;统一资源接口&lt;/li&gt;
&lt;li&gt;资源的表述&lt;/li&gt;
&lt;li&gt;资源的链接&lt;/li&gt;
&lt;li&gt;状态的转移&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2-RESTful-API概念&quot;&gt;&lt;a href=&quot;#2-RESTful-API概念&quot; class=&quot;headerlink&quot; title=&quot;2.RESTful API概念&quot;&gt;&lt;/a&gt;&lt;em&gt;2.RESTful API概念&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;在开发的过程中，我们经常会听到前后端分离这个技术名词，顾名思义，就是前台的开发和后台的开发分离开。这个技术方案的实现就是要借助API，API简单说就是开发人员提供编程接口被其他人调用，他们调用之后会返回数据供其使用。API的类型有多种，但是现在比较主流且实用的就是RESTful API。&lt;/p&gt;
&lt;p&gt;RESTful API 的总结：&lt;br&gt;1.每一个URL代表一种资源&lt;br&gt;2.客户端和服务器之间，传递这种资源的某种表现层&lt;br&gt;3.客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。具体为：&lt;br&gt;GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。&lt;/p&gt;
&lt;h4 id=&quot;2-1-RESTful-API方法-GET&quot;&gt;&lt;a href=&quot;#2-1-RESTful-API方法-GET&quot; class=&quot;headerlink&quot; title=&quot;2.1 RESTful API方法-GET&quot;&gt;&lt;/a&gt;&lt;em&gt;2.1 RESTful API方法-GET&lt;/em&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安全且幂等&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;获取表示&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;变更时获取表示（缓存）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;200（OK） - 表示已在响应中发出&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;204（无内容） - 资源有空表示&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;301（Moved Permanently） - 资源的URI已被更新&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;303（See Other） - 其他（如，负载均衡）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;304（not modified）- 资源未更改（缓存）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;400 （bad request）- 指代坏请求（如，参数错误）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;404 （not found）- 资源不存在&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;406 （not acceptable）- 服务端不支持所需表示&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;500 （internal server error）- 通用错误响应&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;503 （Service Unavailable）- 服务端当前无法处理请求&lt;/p&gt;
    
    </summary>
    
    
      <category term="http" scheme="http://qingye.info/categories/http/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="http" scheme="http://qingye.info/tags/http/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="RESTful API" scheme="http://qingye.info/tags/RESTful-API/"/>
    
  </entry>
  
  <entry>
    <title>k8s+dubbo架构集群内外网络通讯解决方案</title>
    <link href="http://qingye.info/2020/03/29/k8s/k8s-dubbo/"/>
    <id>http://qingye.info/2020/03/29/k8s/k8s-dubbo/</id>
    <published>2020-03-29T11:12:07.000Z</published>
    <updated>2020-04-16T13:07:11.210Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-问题"><a href="#1-问题" class="headerlink" title="1.问题"></a><em>1.问题</em></h3><p>k8s有自己的一套网络管理机制，集群内的容器和容器之间是可以相互通信的。</p><p>但是在容器化升级改造的过程中，不可能一步到位的将所有的服务全部迁移到k8s的容器当中来，毕竟新的技术在没有经过实践趟坑时，肯定不能轻易的全面铺开升级。</p><p>那么就涉及到集群外的服务访问集群内的服务，集群内容器中的ip都是k8s管理的IP，dubbo服务注册的也是获取的容器内分配的IP。</p><p>比如宿主机ip是10.201.7.xx,容器内的ip就是172.66.4.x。群外的和宿主主机同网段的服务通过拿到dubbo的注册的172.66.4.x也根本没法访问容器内的dubbo服务。</p><h3 id="2-分析"><a href="#2-分析" class="headerlink" title="2.分析"></a><em>2.分析</em></h3><p>k8s是通过Service来暴露集群内的服务，假如dubbo服务注册的是Service暴露的端口和宿主的IP那么集群外的服务就可以直接访问集群内容器中的服务了。</p><p>该方案主要有两个难点：</p><p>1.如何获取Service暴露的端口和宿主机的IP注入到POD环境变量；</p><p>2.Dubbo服务如何从环境变量获取IP和端口注册到ZK；</p><p>关于难点1：</p><p>通过downward-api的方式向Pod内注入NodeIP的env；</p><p>通过给Pod注入env的方式将NodePort注入到Pod内；（要求先创建Service）</p><p>关于难点2：</p><p>Dubbo在启动阶段提供两对系统属性，用于设置外部通信的IP和端口地址。</p><p>DUBBO_IP_TO_REGISTRY — 注册到注册中心的IP地址<br>DUBBO_PORT_TO_REGISTRY — 注册到注册中心的端口<br>DUBBO_IP_TO_BIND — 监听IP地址<br>DUBBO_PORT_TO_BIND — 监听端口</p><a id="more"></a><p>详见链接：<a href="https://www.jianshu.com/p/b045dbdb8e12">https://www.jianshu.com/p/b045dbdb8e12</a></p><p>因此，将NodeIP和NodePort的变量名分别设置为DUBBO_IP_TO_REGISTRY和DUBBO_PORT_TO_REGISTRY</p><p>Dubbo服务启动自动获取这两个变量，并注册到ZK，不需要修改任何代码；</p><p>（备注：验证过程发现dubbo2.5.3版本不生效，dubbo2.6.0版本验证成功；2.5.3到2.6.0版本之间未验证）</p><h3 id="3-实现过程"><a href="#3-实现过程" class="headerlink" title="3.实现过程"></a><em>3.实现过程</em></h3><p>Helm Chart文件templates/deployment.yaml<br><img src="https://cloud.qingye.info/images/20200329/image2019-3-28_17-20-32.png" alt="deployment截图"></p><p>配置说明</p><p>DUBBO_PORT_TO_BIND：Dubbo服务默认启动端口为20880，该变量指定Dubbo服务监听端口；端口要与Service中NodePort相同，否则dubbo服务调用报错；</p><p>DUBBO_PORT_TO_REGISTRY ：注册到ZK的端口</p><p>DUBBO_IP_TO_REGISTRY：注册到ZK的IP地址</p><p>Helm Chart文件values.yaml<br><img src="https://cloud.qingye.info/images/20200329/image2019-3-28_17-22-2.png" alt="Helm Chart文件valuesyaml截图"></p><p>配置说明</p><p>1.指定Service的类型为NodePort；</p><p>2.指定dubbo服务默认启动端口为8080；</p><p>发版过程Pipeline文件<br><img src="https://cloud.qingye.info/images/20200329/image2019-3-28_17-23-26.png" alt="发版过程Pipeline截图"></p><p>步骤1：连接k8s集群，更新helm仓库；</p><p>步骤2：查询服务是否部署，如果未部署则部署（保证能获取到Service的NodePort）；</p><p>步骤3：查询对应服务的NodePort，并赋值给变量；</p><p>步骤4：更新服务，将获取到的NodePort注入POD系统变量。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-问题&quot;&gt;&lt;a href=&quot;#1-问题&quot; class=&quot;headerlink&quot; title=&quot;1.问题&quot;&gt;&lt;/a&gt;&lt;em&gt;1.问题&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;k8s有自己的一套网络管理机制，集群内的容器和容器之间是可以相互通信的。&lt;/p&gt;
&lt;p&gt;但是在容器化升级改造的过程中，不可能一步到位的将所有的服务全部迁移到k8s的容器当中来，毕竟新的技术在没有经过实践趟坑时，肯定不能轻易的全面铺开升级。&lt;/p&gt;
&lt;p&gt;那么就涉及到集群外的服务访问集群内的服务，集群内容器中的ip都是k8s管理的IP，dubbo服务注册的也是获取的容器内分配的IP。&lt;/p&gt;
&lt;p&gt;比如宿主机ip是10.201.7.xx,容器内的ip就是172.66.4.x。群外的和宿主主机同网段的服务通过拿到dubbo的注册的172.66.4.x也根本没法访问容器内的dubbo服务。&lt;/p&gt;
&lt;h3 id=&quot;2-分析&quot;&gt;&lt;a href=&quot;#2-分析&quot; class=&quot;headerlink&quot; title=&quot;2.分析&quot;&gt;&lt;/a&gt;&lt;em&gt;2.分析&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;k8s是通过Service来暴露集群内的服务，假如dubbo服务注册的是Service暴露的端口和宿主的IP那么集群外的服务就可以直接访问集群内容器中的服务了。&lt;/p&gt;
&lt;p&gt;该方案主要有两个难点：&lt;/p&gt;
&lt;p&gt;1.如何获取Service暴露的端口和宿主机的IP注入到POD环境变量；&lt;/p&gt;
&lt;p&gt;2.Dubbo服务如何从环境变量获取IP和端口注册到ZK；&lt;/p&gt;
&lt;p&gt;关于难点1：&lt;/p&gt;
&lt;p&gt;通过downward-api的方式向Pod内注入NodeIP的env；&lt;/p&gt;
&lt;p&gt;通过给Pod注入env的方式将NodePort注入到Pod内；（要求先创建Service）&lt;/p&gt;
&lt;p&gt;关于难点2：&lt;/p&gt;
&lt;p&gt;Dubbo在启动阶段提供两对系统属性，用于设置外部通信的IP和端口地址。&lt;/p&gt;
&lt;p&gt;DUBBO_IP_TO_REGISTRY — 注册到注册中心的IP地址&lt;br&gt;DUBBO_PORT_TO_REGISTRY — 注册到注册中心的端口&lt;br&gt;DUBBO_IP_TO_BIND — 监听IP地址&lt;br&gt;DUBBO_PORT_TO_BIND — 监听端口&lt;/p&gt;
    
    </summary>
    
    
      <category term="k8s" scheme="http://qingye.info/categories/k8s/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>深刻理解Docker镜像大小</title>
    <link href="http://qingye.info/2020/03/28/k8s/Docker-Image/"/>
    <id>http://qingye.info/2020/03/28/k8s/Docker-Image/</id>
    <published>2020-03-28T02:07:07.000Z</published>
    <updated>2020-04-16T13:04:25.944Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-docker镜像分析"><a href="#1-docker镜像分析" class="headerlink" title="1.docker镜像分析"></a><em>1.docker镜像分析</em></h3><p>是否还记得第一个接触Docker的时候，你从Docker Hub下拉的那个镜像呢？在那个处女镜像的基础上，你运行了容器生涯的处女容器。镜像的基石作用已经很明显，在Docker的世界里，可以说是：No Image，No Container。</p><p>再进一步思考Docker镜像，大家可能很快就会联想到以下几类镜像：</p><p>1.系统级镜像:如Ubuntu镜像，CentOS镜像以及Debian容器等；</p><p>2.工具栈镜像:如Golang镜像，Flask镜像，Tomcat镜像等；</p><p>3.服务级镜像:如MySQL镜像，MongoDB镜像，RabbitMQ镜像等；</p><p>4.应用级镜像:如WordPress镜像，DockerRegistry镜像等。</p><p>镜像林林总总，想要运行Docker容器，必须要有Docker镜像；想要有Docker镜像，必须要先下载Docker镜像；既然涉及到下载Docker镜像，自然会存在Docker镜像存储。谈到Docker镜像存储，那我们首先来聊聊Docker镜像大小方面的知识。</p><p>以下将从三个角度来分析Docker镜像的大小问题：Dockerfile与镜像、联合文件系统以及镜像共享关系。</p><p>Dockerfile与镜像<br>Dockerfile由多条指令构成，随着深入研究Dockerfile与镜像的关系，很快大家就会发现，Dockerfile中的每一条指令都会对应于Docker镜像中的一层。</p><p>继续以如下Dockerfile为例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FROM</span> <span class="string">ubuntu:14.04</span></span><br><span class="line"><span class="string">ADD</span> <span class="string">run.sh</span> <span class="string">/</span></span><br><span class="line"><span class="string">VOLUME</span> <span class="string">/data</span></span><br><span class="line"><span class="string">CMD</span> <span class="string">["./run.sh"]</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>通过docker build以上Dockerfile的时候，会在Ubuntu:14.04镜像基础上，添加三层独立的镜像，依次对应于三条不同的命令。镜像示意图如下：</p><p>有了Dockerfile与镜像关系的初步认识之后，我们再进一步联系到每一层镜像的大小。</p><p>不得不说，在层级化管理的Docker镜像中，有不少层大小都为0。那些镜像层大小不为0的情况，归根结底的原因是：构建Docker镜像时，对当前的文件系统造成了修改更新。而修改更新的情况主要有两种：</p><p>1.ADD或COPY命令:ADD或者COPY的作用是在docker build构建镜像时向容器中添加内容，只要内容添加成功，当前构建的那层镜像就是添加内容的大小，如以上命令ADD run.sh /，新构建的那层镜像大小为文件run.sh的大小。</p><p>2.RUN命令:RUN命令的作用是在当前空的镜像层内运行一条命令，倘若运行的命令需要更新磁盘文件，那么所有的更新内容都在存储在当前镜像层中。举例说明：RUN echo DaoCloud命令不涉及文件系统内容的修改，故命令运行完之后当前镜像层的大小为0；RUN wget <a href="http://abc.com/def.tar命令会将压缩包下载至当前目录下，因此当前这一层镜像的大小为:对文件系统内容的增量修改部分，即def.tar文件的大小。">http://abc.com/def.tar命令会将压缩包下载至当前目录下，因此当前这一层镜像的大小为:对文件系统内容的增量修改部分，即def.tar文件的大小。</a></p><h3 id="2-联合文件系统"><a href="#2-联合文件系统" class="headerlink" title="2.联合文件系统"></a><em>2.联合文件系统</em></h3><p>Dockerfile中命令与镜像层一一对应，那么是否意味着docker build完毕之后，镜像的总大小＝每一层镜像的大小总和呢？答案是肯定的。依然以上图为例：如果ubuntu:14.04镜像的大小为200MB，而run.sh的大小为5MB，那么以上三层镜像从上到下，每层大小依次为0、0以及5MB，那么最终构建出的镜像大小的确为0+0+5+200=205MB。</p><p>虽然最终镜像的大小是每层镜像的累加，但是需要额外注意的是：Docker镜像的大小并不等于容器中文件系统内容的大小（不包括挂载文件，/proc、/sys等虚拟文件）。个中缘由，就和联合文件系统有很大的关系了。</p><p>首先来看一下这个简单的Dockerfile例子(假如在Dockerfile当前目录下有一个100MB的压缩文件compressed.tar):</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FROM</span> <span class="string">ubuntu:14.04</span></span><br><span class="line"><span class="string">ADD</span> <span class="string">compressed.tar</span> <span class="string">/</span></span><br><span class="line"><span class="string">RUN</span> <span class="string">rm</span> <span class="string">/compressed.tar</span></span><br><span class="line"><span class="string">ADD</span> <span class="string">compressed.tar</span> <span class="string">/</span></span><br></pre></td></tr></table></figure><p>1.FROM ubuntu:镜像ubuntu:14.04的大小为200MB；</p><p>2.ADD compressed.tar /: compressed.tar文件为100MB，因此当前镜像层的大小为100MB，镜像总大小为300MB；</p><p>3.RUN rm /compressed.tar:删除文件compressed.tar,此时的删除并不会删除下一层的compressed.tar文件，只会在当前层产生一个compressed.tar的删除标记，确保通过该层将看不到compressed.tar,因此当前镜像层的大小也为0，镜像总大小为300MB；</p><p>4.ADD compressed.tar /:compressed.tar文件为100MB，因此当前镜像层的大小为300MB+100MB，镜像总大小为400MB；</p><p>分析完毕之后，我们发现镜像的总大小为400MB，但是如果运行该镜像的话，我们很快可以发现在容器根目录下执行du -sh之后，显示的数值并非400MB，而是300MB左右。主要的原因还是：联合文件系统的性质保证了两个拥有compressed.tar文件的镜像层，仅仅会容器看到一个。同时这也说明了一个现状，当用户基于一个非常大，甚至好几个GB的镜像运行容器时，在容器内部查看根目录大小，发现竟然只有500MB不到，设置更小。</p><p>分析至此，有一点大家需要非常注意：镜像大小和容器大小有着本质的区别。</p><h3 id="3-镜像共享关系"><a href="#3-镜像共享关系" class="headerlink" title="3.镜像共享关系"></a><em>3.镜像共享关系</em></h3><p>Docker镜像说大不大，说小不小，但是一旦镜像的总数上来之后，岂不是对本地磁盘造成很大的存储压力？平均每个镜像500MB，岂不是100个镜像就需要准备50GB的存储空间？</p><p>结果往往不是我们想象的那样，Docker在镜像复用方面设计得非常出色，大大节省镜像占用的磁盘空间。Docker镜像的复用主要体现在：多个不同的Docker镜像可以共享相同的镜像层。</p><p>假设本地镜像存储中只有一个ubuntu:14.04的镜像，我们以两个Dockerfile来说明镜像复用：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FROM</span> <span class="string">ubuntu:14.04</span></span><br><span class="line"><span class="string">RUN</span> <span class="string">apt-get</span> <span class="string">update</span></span><br><span class="line"><span class="string">FROM</span> <span class="string">ubuntu:14.04</span></span><br><span class="line"><span class="string">ADD</span> <span class="string">compressed.tar</span> <span class="string">/</span></span><br></pre></td></tr></table></figure><p>假设最终docker build构建出来的镜像名分别为image1和image2，由于两个Dockerfile均基于ubuntu:14.04，因此，image1和image2这两个镜像均复用了镜像ubuntu:14.04。 假设RUN apt-get update修改的文件系统内容为20MB，最终本地三个镜像的大小关系应该如下：</p><p>ubuntu:14.04: 200MB</p><p>image1:200MB(ubuntu:14.04)+20MB=220MB</p><p>image2:200MB(ubuntu:14.04)+100MB=300MB</p><p>如果仅仅是单纯的累加三个镜像的大小，那结果应该是：200+220+300=720MB，但是由于镜像复用的存在，实际占用的磁盘空间大小是：200＋20+100=320MB，足足节省了400MB的磁盘空间。在此，足以证明镜像复用的巨大好处。</p><h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a><em>4.总结</em></h3><p>学习Docker的同时，往往有三部分内容是分不开的，那就是Dockerfile，Docker镜像与Docker容器，分析Docker镜像大小也是如此。Docker镜像的大小，貌似平淡无奇，却是优化镜像，容器磁盘限额必须要涉及的内容。</p><p>本系列将通过以下多篇文章来分析Docker镜像：</p><p>1.深刻理解 Docker 镜像大小</p><p>2.其实 docker commit 很简单</p><p>3.不得不说的 docker save 与 docker export 区别</p><p>4.为什么有些容器文件动不得</p><p>5.打破 MNT Namespace 的容器 VOLUME</p><h3 id="5-参考文章"><a href="#5-参考文章" class="headerlink" title="5.参考文章"></a><em>5.参考文章</em></h3><p>作者：孙宏亮 来源：CSDN 原文：<a href="https://blog.csdn.net/shlazww/article/details/47375009?utm_source=copy">https://blog.csdn.net/shlazww/article/details/47375009?utm_source=copy</a> 版权声明：本文为博主原创文章，转载请附上博文链接！</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-docker镜像分析&quot;&gt;&lt;a href=&quot;#1-docker镜像分析&quot; class=&quot;headerlink&quot; title=&quot;1.docker镜像分析&quot;&gt;&lt;/a&gt;&lt;em&gt;1.docker镜像分析&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;是否还记得第一个接触Docker的时候，你从Docker Hub下拉的那个镜像呢？在那个处女镜像的基础上，你运行了容器生涯的处女容器。镜像的基石作用已经很明显，在Docker的世界里，可以说是：No Image，No Container。&lt;/p&gt;
&lt;p&gt;再进一步思考Docker镜像，大家可能很快就会联想到以下几类镜像：&lt;/p&gt;
&lt;p&gt;1.系统级镜像:如Ubuntu镜像，CentOS镜像以及Debian容器等；&lt;/p&gt;
&lt;p&gt;2.工具栈镜像:如Golang镜像，Flask镜像，Tomcat镜像等；&lt;/p&gt;
&lt;p&gt;3.服务级镜像:如MySQL镜像，MongoDB镜像，RabbitMQ镜像等；&lt;/p&gt;
&lt;p&gt;4.应用级镜像:如WordPress镜像，DockerRegistry镜像等。&lt;/p&gt;
&lt;p&gt;镜像林林总总，想要运行Docker容器，必须要有Docker镜像；想要有Docker镜像，必须要先下载Docker镜像；既然涉及到下载Docker镜像，自然会存在Docker镜像存储。谈到Docker镜像存储，那我们首先来聊聊Docker镜像大小方面的知识。&lt;/p&gt;
&lt;p&gt;以下将从三个角度来分析Docker镜像的大小问题：Dockerfile与镜像、联合文件系统以及镜像共享关系。&lt;/p&gt;
&lt;p&gt;Dockerfile与镜像&lt;br&gt;Dockerfile由多条指令构成，随着深入研究Dockerfile与镜像的关系，很快大家就会发现，Dockerfile中的每一条指令都会对应于Docker镜像中的一层。&lt;/p&gt;
&lt;p&gt;继续以如下Dockerfile为例：&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;ubuntu:14.04&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;ADD&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;run.sh&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;VOLUME&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/data&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;CMD&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;[&quot;./run.sh&quot;]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="k8s" scheme="http://qingye.info/categories/k8s/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
      <category term="docker镜像" scheme="http://qingye.info/tags/docker%E9%95%9C%E5%83%8F/"/>
    
  </entry>
  
  <entry>
    <title>k8s常见问题解决</title>
    <link href="http://qingye.info/2020/03/27/k8s/K8S-Common-problem/"/>
    <id>http://qingye.info/2020/03/27/k8s/K8S-Common-problem/</id>
    <published>2020-03-27T14:06:07.000Z</published>
    <updated>2020-04-16T13:05:48.664Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-基础镜像制作规范和使用"><a href="#1-基础镜像制作规范和使用" class="headerlink" title="1.基础镜像制作规范和使用"></a><em>1.基础镜像制作规范和使用</em></h3><p>1.1 时区修改<br>官网下载centos镜像默认时区是国外的时区，需要把它修改成上海时区。运维提供提一个基础的镜像给业务部门下载使用。</p><p>1.2 规范docker运行程序的用户<br>基于安全和规范的考虑，docker统一使用一个uid为1001的，用户名为pub的普通账号运行程序，需要在镜像和容器的宿主机上面同时新建。这个也是运维在初始化基础镜像和容器宿主机上面统一创建。</p><p>1.3 规范每个容器的cpu和内存(每个容器分配1核2G内存,视情况而定)</p><h3 id="2-镜像下载策略"><a href="#2-镜像下载策略" class="headerlink" title="2.镜像下载策略"></a><em>2.镜像下载策略</em></h3><p>2.1 默认的镜像拉取策略是“IfNotPresent”，在镜像已经存在的情况下，kubelet将不在去拉取镜像。 如果总是想要拉取镜像，必须设置拉取策略为“Always”或者设置镜像标签为“:latest”。</p><p>如果没有指定镜像的标签，它会被假定为“:latest”,同时拉取策略为“Always”。</p><h3 id="3-增加k8s各个命名空间的服务账号-default-的自定义权限-RABC"><a href="#3-增加k8s各个命名空间的服务账号-default-的自定义权限-RABC" class="headerlink" title="3.增加k8s各个命名空间的服务账号 default 的自定义权限(RABC)"></a><em>3.增加k8s各个命名空间的服务账号 default 的自定义权限(RABC)</em></h3><p>首先，了解下什么是RABC：</p><p>基于角色的访问控制（Role-Based Access Control, 即”RBAC”）使用”rbac.authorization.k8s.io” API Group实现授权决策，允许管理员通过Kubernetes API动态配置策略。</p><p>在RBAC API中，一个角色包含了一套表示一组权限的规则。 权限以纯粹的累加形式累积（没有”否定”的规则）。 角色可以由命名空间（namespace）内的Role对象定义，而整个Kubernetes集群范围内有效的角色则通过ClusterRole对象实现</p><p>比如授予default命名空间的default ServiceAccount账号能够”get”, “watch”, “list”, “patch”   default 命名空间里面的pod资源</p><p>3.1 定义role，一个Role对象只能用于授予对某一单一命名空间中资源的访问权限。 以下示例描述了”default”命名空间中的一个Role对象的定义，用于授予对pod的读访问权限：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> <span class="string">[""]</span> <span class="comment"># 空字符串""表明使用core API group</span></span><br><span class="line">  <span class="attr">resources:</span> <span class="string">["pods"]</span></span><br><span class="line">  <span class="attr">verbs:</span> <span class="string">["get",</span> <span class="string">"watch"</span><span class="string">,</span> <span class="string">"list"</span><span class="string">,</span> <span class="string">"patch"</span><span class="string">]</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>3.2 RoleBinding，角色绑定将一个角色中定义的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即subject, 包括用户——User、用户组——Group、或者服务账户——Service Account）以及对被授予角色的引用。 在命名空间中可以通过RoleBinding对象授予权限，而集群范围的权限授予则通过ClusterRoleBinding对象完成</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">read-pods</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">""</span></span><br></pre></td></tr></table></figure><h3 id="4-docker使用http代理上传或者下载镜像"><a href="#4-docker使用http代理上传或者下载镜像" class="headerlink" title="4.docker使用http代理上传或者下载镜像"></a><em>4.docker使用http代理上传或者下载镜像</em></h3><p>1.在里面系统shell里面设置的代理docker服务无法获取信息，需要在docker服务启动里面写代理信息。</p><p>默认情况下这个配置文件夹并不存在，我们要创建它。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line"><span class="comment">#创建一个文件 /etc/systemd/system/docker.service.d/http-proxy.conf</span></span><br><span class="line"><span class="comment">#包含 HTTP_PROXY 环境变量:</span></span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"><span class="string">"HTTP_PROXY=http://username:pwd@192.168.34.216:3128/"</span> <span class="string">"HTTPS_PROXY=http://username:pwd@192.168.34.216:3128/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#有个注意的地方，如果直接写密码，docker无法识别，需要把密码进行url编码，注意，只需要把密码进行编码</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#编码地址:http://tool.chinaz.com/tools/urlencode.aspx</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果有局域网或者国内的registry，我们还需要使用 NO_PROXY 变量声明一下，比如测试环境的:</span></span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">"HTTP_PROXY=http://pub:pwd@192.168.34.216:3128/"</span> <span class="string">"HTTPS_PROXY=http://username:pwd@192.168.34.216:3128/"</span> <span class="string">"NO_PROXY=localhost,127.0.0.1,registry.test.internal"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#刷新systemd配置:</span></span><br><span class="line"></span><br><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line"><span class="comment">#用系统命令验证环境变量加上去没:</span></span><br><span class="line"></span><br><span class="line">$ systemctl show --property=Environment docker</span><br><span class="line">Environment=HTTP_PROXY=http://username:<span class="built_in">pwd</span>@192.168.34.216:3128/ HTTPS_PROXY=http://username:<span class="built_in">pwd</span>@192.168.34.216:3128/ NO_PROXY=localhost,127.0.0.1,registry.test.internal</span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="5-docker忽略https通信"><a href="#5-docker忽略https通信" class="headerlink" title="5.docker忽略https通信"></a><em>5.docker忽略https通信</em></h3><p>有时我们内网搭建的容器私有仓库没有使用https，但是docker通信默认是使用https的，这是怎么办呢？经过查询，docker可以自定义对某个私有仓库不使用https，只需要在docker服务启动的脚本上面修改：<br>修改/usr/lib/systemd/system/docker.service(centos7配置路径)文件的启动命令ExecStart=/usr/bin/dockerd 改为 ExecStart=/usr/bin/dockerd <em>–insecure-registry registry.test.internal*，如果有多个域名需要使用http通信，可使用</em>–insecure-registry registry.xxx.xxx* 多个标记，然后reload配置和重启docker服务，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#刷新systemd配置:</span></span><br><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line"><span class="comment">#重启docker服务：</span></span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="6-master节点故障迁移"><a href="#6-master节点故障迁移" class="headerlink" title="6.master节点故障迁移"></a><em>6.master节点故障迁移</em></h3><p>master节点的故障迁移简单就四点</p><p>第一步，把故障的节点从集群里面删掉</p><p>第二步，另外找一台安装好master组件，使用kubeadm join xxx  –experimental-control-plane 即可</p><p>第三步，nginx-lb的nginx配置需要修改IP，然后重启nginx-lb 容器</p><p>第四步，把keepalive的配置迁移到新节点，启动keepalive即可(/etc/keepalived/keepalived.conf里面对应的IP需要修改)</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-基础镜像制作规范和使用&quot;&gt;&lt;a href=&quot;#1-基础镜像制作规范和使用&quot; class=&quot;headerlink&quot; title=&quot;1.基础镜像制作规范和使用&quot;&gt;&lt;/a&gt;&lt;em&gt;1.基础镜像制作规范和使用&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;1.1 时区修改&lt;br&gt;官网下载centos镜像默认时区是国外的时区，需要把它修改成上海时区。运维提供提一个基础的镜像给业务部门下载使用。&lt;/p&gt;
&lt;p&gt;1.2 规范docker运行程序的用户&lt;br&gt;基于安全和规范的考虑，docker统一使用一个uid为1001的，用户名为pub的普通账号运行程序，需要在镜像和容器的宿主机上面同时新建。这个也是运维在初始化基础镜像和容器宿主机上面统一创建。&lt;/p&gt;
&lt;p&gt;1.3 规范每个容器的cpu和内存(每个容器分配1核2G内存,视情况而定)&lt;/p&gt;
&lt;h3 id=&quot;2-镜像下载策略&quot;&gt;&lt;a href=&quot;#2-镜像下载策略&quot; class=&quot;headerlink&quot; title=&quot;2.镜像下载策略&quot;&gt;&lt;/a&gt;&lt;em&gt;2.镜像下载策略&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;2.1 默认的镜像拉取策略是“IfNotPresent”，在镜像已经存在的情况下，kubelet将不在去拉取镜像。 如果总是想要拉取镜像，必须设置拉取策略为“Always”或者设置镜像标签为“:latest”。&lt;/p&gt;
&lt;p&gt;如果没有指定镜像的标签，它会被假定为“:latest”,同时拉取策略为“Always”。&lt;/p&gt;
&lt;h3 id=&quot;3-增加k8s各个命名空间的服务账号-default-的自定义权限-RABC&quot;&gt;&lt;a href=&quot;#3-增加k8s各个命名空间的服务账号-default-的自定义权限-RABC&quot; class=&quot;headerlink&quot; title=&quot;3.增加k8s各个命名空间的服务账号 default 的自定义权限(RABC)&quot;&gt;&lt;/a&gt;&lt;em&gt;3.增加k8s各个命名空间的服务账号 default 的自定义权限(RABC)&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;首先，了解下什么是RABC：&lt;/p&gt;
&lt;p&gt;基于角色的访问控制（Role-Based Access Control, 即”RBAC”）使用”rbac.authorization.k8s.io” API Group实现授权决策，允许管理员通过Kubernetes API动态配置策略。&lt;/p&gt;
&lt;p&gt;在RBAC API中，一个角色包含了一套表示一组权限的规则。 权限以纯粹的累加形式累积（没有”否定”的规则）。 角色可以由命名空间（namespace）内的Role对象定义，而整个Kubernetes集群范围内有效的角色则通过ClusterRole对象实现&lt;/p&gt;
&lt;p&gt;比如授予default命名空间的default ServiceAccount账号能够”get”, “watch”, “list”, “patch”   default 命名空间里面的pod资源&lt;/p&gt;
&lt;p&gt;3.1 定义role，一个Role对象只能用于授予对某一单一命名空间中资源的访问权限。 以下示例描述了”default”命名空间中的一个Role对象的定义，用于授予对pod的读访问权限：&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;Role&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;apiVersion:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;namespace:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;default&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;pod-reader&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;rules:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;bullet&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;apiGroups:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;[&quot;&quot;]&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# 空字符串&quot;&quot;表明使用core API group&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;resources:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;[&quot;pods&quot;]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;attr&quot;&gt;verbs:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;[&quot;get&quot;,&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;watch&quot;&lt;/span&gt;&lt;span class=&quot;string&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;list&quot;&lt;/span&gt;&lt;span class=&quot;string&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;patch&quot;&lt;/span&gt;&lt;span class=&quot;string&quot;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="k8s" scheme="http://qingye.info/categories/k8s/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[10.使用Spinnaker持续发布应用]</title>
    <link href="http://qingye.info/2020/03/26/k8s/K8S-10-Use-Spinnaker-CICD/"/>
    <id>http://qingye.info/2020/03/26/k8s/K8S-10-Use-Spinnaker-CICD/</id>
    <published>2020-03-26T15:30:07.000Z</published>
    <updated>2020-04-16T13:13:13.324Z</updated>
    
    <content type="html"><![CDATA[<h3 id="本节介绍如何使用spinnaker的持续发布k8s的应用"><a href="#本节介绍如何使用spinnaker的持续发布k8s的应用" class="headerlink" title="本节介绍如何使用spinnaker的持续发布k8s的应用"></a>本节介绍如何使用spinnaker的持续发布k8s的应用</h3><p>1.首先使用spinnaker创建一个nginxdemo 的应用，再在nginxdemo应用里面创建 Pipeline。进入nginxdemo 详情页面，点击 “PIPELINES”，目前是没有任何信息的，点击 “+ Create”，弹框中选择类型为 Pipeline，输入流程名称，这里我命名为 nginxdemo-pipe。因为第一次创建，下边 “Copy From” 选择没出来，后续在创建时，我们也可以通过 “Copy From” 方式选择已存在的 Pipeline，非常方便就复制了一个一样配置的流程。创建完毕后，就会出现详细配置 Pipeline State 的页面了</p><p><img src="https://cloud.qingye.info/images/20200326/spin1.png" alt="图1"></p><p>2.配置 Configuration 项</p><p>刚开始这里只有一个 Configuration 选项，可以配置 Automated Triggers、Parameters、Notifications 等，这里说下 Automated Triggers 和 Parameters 这两个非常有用，我们可以将此视为 Pipeline 启动前的一些初始化配置，比如启动需要的参数配置、自动触发配置等，为后续各阶段提供必要的信息。<br>Automated Triggers 自动触发，它提供 7 种类型的触发方式：</p><ul><li>CRON：调度触发，可以执行一个 cron 调度任务来定时任务触发该流程。</li><li>Git：当执行 Git push 操作时，触发该流程</li><li>Jenkins：监听 Jenkins 的某一个 Job</li><li>Travis：监听 Travis 的某一个 Job</li><li>Pipeline：监听另一个 Pipeline 执行</li><li>Pub/Sub：当接受到 pubsub 消息时触发</li><li>Docker Registry：当 image 更新时触发。</li></ul><p>基本能满足我们日常持续集成或交付的需求，当然每一个类型都需要配置相应的参数，比如 Cron 类型，需要配置执行频率、启动时间等。下图我们选择Docker Registry作为触发类型</p><a id="more"></a><p><img src="https://cloud.qingye.info/images/20200326/spin2.png" alt="图2"></p><p>3.下拉框可以选择私有仓库，选择ops/nginx的镜像</p><p><img src="https://cloud.qingye.info/images/20200326/spin3.png" alt="图3"></p><p>4.增加阶段，我们直接增加一个发布的阶段。<br><img src="https://cloud.qingye.info/images/20200326/spin4.png" alt="图4"></p><p>5.选择阶段的类型，我们选择deploy<br><img src="https://cloud.qingye.info/images/20200326/spin5.png" alt="图5"></p><p>6.增加需要发布的服务器组，这里我们先增加之前定义的nginxdemo-test 服务器组<br><img src="https://cloud.qingye.info/images/20200326/spin6.png" alt="图6"><br><img src="https://cloud.qingye.info/images/20200326/spin7.png" alt="图7"></p><p>7.选择容器：选择ops/nginx的镜像，阶段类型：选择红/黑部署，新版本的服务器组起来后service会把请求转到新版本的后端，旧版本的disable，但是旧版本的服务器组还存在集群中，方便后续快速回滚的需求<br><img src="https://cloud.qingye.info/images/20200326/spin8.png" alt="图8"><br><img src="https://cloud.qingye.info/images/20200326/spin9.png" alt="图9"><br><img src="https://cloud.qingye.info/images/20200326/spin10.png" alt="图10"></p><p>8.填写探针开发环境，测试环境，生产环境按照下面格式填写，更改容器端口和检测页面即可。<br><img src="https://cloud.qingye.info/images/20200326/spin11.png" alt="图11"></p><p>9.手动触发一个发布流程<br><img src="https://cloud.qingye.info/images/20200326/spin12.png" alt="图12"></p><p>10.选择发布容器镜像的版本<br><img src="https://cloud.qingye.info/images/20200326/spin13.png" alt="图13"></p><p>11.点击run，开始发布新的版本<br><img src="https://cloud.qingye.info/images/20200326/spin14.png" alt="图14"></p><p>12.部署完成<br><img src="https://cloud.qingye.info/images/20200326/spin15.png" alt="图15"></p><p>13.查看服务器组，看到选择的镜像版本的服务已经启动<br><img src="https://cloud.qingye.info/images/20200326/spin16.png" alt="图16"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;本节介绍如何使用spinnaker的持续发布k8s的应用&quot;&gt;&lt;a href=&quot;#本节介绍如何使用spinnaker的持续发布k8s的应用&quot; class=&quot;headerlink&quot; title=&quot;本节介绍如何使用spinnaker的持续发布k8s的应用&quot;&gt;&lt;/a&gt;本节介绍如何使用spinnaker的持续发布k8s的应用&lt;/h3&gt;&lt;p&gt;1.首先使用spinnaker创建一个nginxdemo 的应用，再在nginxdemo应用里面创建 Pipeline。进入nginxdemo 详情页面，点击 “PIPELINES”，目前是没有任何信息的，点击 “+ Create”，弹框中选择类型为 Pipeline，输入流程名称，这里我命名为 nginxdemo-pipe。因为第一次创建，下边 “Copy From” 选择没出来，后续在创建时，我们也可以通过 “Copy From” 方式选择已存在的 Pipeline，非常方便就复制了一个一样配置的流程。创建完毕后，就会出现详细配置 Pipeline State 的页面了&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cloud.qingye.info/images/20200326/spin1.png&quot; alt=&quot;图1&quot;&gt;&lt;/p&gt;
&lt;p&gt;2.配置 Configuration 项&lt;/p&gt;
&lt;p&gt;刚开始这里只有一个 Configuration 选项，可以配置 Automated Triggers、Parameters、Notifications 等，这里说下 Automated Triggers 和 Parameters 这两个非常有用，我们可以将此视为 Pipeline 启动前的一些初始化配置，比如启动需要的参数配置、自动触发配置等，为后续各阶段提供必要的信息。&lt;br&gt;Automated Triggers 自动触发，它提供 7 种类型的触发方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CRON：调度触发，可以执行一个 cron 调度任务来定时任务触发该流程。&lt;/li&gt;
&lt;li&gt;Git：当执行 Git push 操作时，触发该流程&lt;/li&gt;
&lt;li&gt;Jenkins：监听 Jenkins 的某一个 Job&lt;/li&gt;
&lt;li&gt;Travis：监听 Travis 的某一个 Job&lt;/li&gt;
&lt;li&gt;Pipeline：监听另一个 Pipeline 执行&lt;/li&gt;
&lt;li&gt;Pub/Sub：当接受到 pubsub 消息时触发&lt;/li&gt;
&lt;li&gt;Docker Registry：当 image 更新时触发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基本能满足我们日常持续集成或交付的需求，当然每一个类型都需要配置相应的参数，比如 Cron 类型，需要配置执行频率、启动时间等。下图我们选择Docker Registry作为触发类型&lt;/p&gt;
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
      <category term="spinnaker" scheme="http://qingye.info/tags/spinnaker/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[9.基于Jenkins和Spinnaker的CI/CD流程]</title>
    <link href="http://qingye.info/2020/03/26/k8s/K8S-9-Jenkins-Spinnaker-CICD/"/>
    <id>http://qingye.info/2020/03/26/k8s/K8S-9-Jenkins-Spinnaker-CICD/</id>
    <published>2020-03-26T15:07:07.000Z</published>
    <updated>2020-04-16T13:12:27.875Z</updated>
    
    <content type="html"><![CDATA[<h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><p><img src="https://cloud.qingye.info/images/20200326/kubernetes-jenkins-spinnaker-ci-cd.png" alt="基于Jenkins和Spinnaker的CI/CD流程"></p><p>流程说明:<br>1.用户向Gitlab提交代码，代码中包含Dockerfile<br>2.将代码提交到远程仓库<br>3.Gitlab提交触发Jenkins自动构建<br>4.Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库<br>5.更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息(不是新业务不用更改此配置)<br>6.Jenkins构建完成可触发Spinnaker的自动发布流程，或者手动触发spinnaker发布。<br>7.Spinnaker会根据发布流程更新kubernetes YAML配置文件<br>8.Spinnaker调用kubernetes的API，部署应用</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;流程图&quot;&gt;&lt;a href=&quot;#流程图&quot; class=&quot;headerlink&quot; title=&quot;流程图&quot;&gt;&lt;/a&gt;流程图&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://cloud.qingye.info/images/20200326/kubernetes-jen
      
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
      <category term="spinnaker" scheme="http://qingye.info/tags/spinnaker/"/>
    
      <category term="Jenkins" scheme="http://qingye.info/tags/Jenkins/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[8.spinnaker基本介绍]</title>
    <link href="http://qingye.info/2020/03/26/k8s/K8S-8-spinnaker/"/>
    <id>http://qingye.info/2020/03/26/k8s/K8S-8-spinnaker/</id>
    <published>2020-03-26T14:07:07.000Z</published>
    <updated>2020-04-16T13:11:58.547Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a><em>1.概念</em></h3><p>Spinnaker 是 Netflix 的开源项目，是一个持续交付平台，它定位于将产品快速且持续的部署到多种云平台上。Spinnaker 通过将发布和各个云平台解耦，来将部署流程流水线化，从而降低平台迁移或多云品台部署应用的复杂度，它本身内部支持 Google、AWS EC2、Microsoft Azure、Kubernetes和OpenStack 等云平台，并且它可以无缝集成其他持续集成（CI）流程，如 git、Jenkins、Travis CI、Docker registry、cron 调度器等。简而言之，Spinnaker是致力于提供在多种平台上实现开箱即用的集群管理和部署功能的平台。</p><h3 id="2-功能"><a href="#2-功能" class="headerlink" title="2.功能"></a><em>2.功能</em></h3><h4 id="2-1：集群管理主要用于管理云上的资源，它分为以下几个块"><a href="#2-1：集群管理主要用于管理云上的资源，它分为以下几个块" class="headerlink" title="2.1：集群管理主要用于管理云上的资源，它分为以下几个块"></a>2.1：集群管理主要用于管理云上的资源，它分为以下几个块</h4><ul><li>Server Group：服务组，是资源管理单位，识别可部署组件和基础配置设置，它并且关联了一个负载均衡器和安全组，当部署完毕后，服务组就相当于一组运行中的软件实例集合，如（VM 实例，Kubernetes pods）。</li><li>Cluster：集群，由用户定义的，对服务组的逻辑分组。</li><li>Applications：应用，是对集群的逻辑分组。</li><li>Load Balancer：负载均衡，用于将外部网络流量重定向到服务组中的机器实例，还可以指定一系列规则，用来对服务组中的机器实例做健康监测。</li><li>Security Group：安全组，定义了网络访问权限，由IP、端口和通信协议组成的防火墙</li></ul><h4 id="2-2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分"><a href="#2-2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分" class="headerlink" title="2.2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分"></a>2.2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分</h4><ul><li>管道 部署管理的核心是管道，在Spinnaker的定义中，管道由一系列的阶段（stages）组成。管道可以人工触发，也可以配置为自动触发，比如由 Jenkins Job 完成时、Docker Images 上传到仓库时，CRON 定时器、其他管道中的某一阶段。同时，管道可以配置参数和通知，可以在管道一些阶段上执行时发送邮件消息。Spinnaker 已经内置了一些阶段，如执行自定义脚本、触发 Jenkins 任务等。</li><li>阶段 阶段在 Spinnaker 中，可以作为管道的一个自动构建模块的功能组成。我们可以随意在管道中定义各个阶段执行顺序。Spinnaker 提供了很多阶段供我们选择使用，比如执行发布（Deploy）、执行自定义脚本 (script)、触发 Jenkins 任务 (jenkins)等，功能很强大。</li><li>部署策略 Spinnaker 支持精细的部署策略，比如 红/黑（蓝/绿）部署，多阶段环境部署，滚动红/黑策略，canary 发布等。用户可以为每个环境使用不同部署策略，比如，测试环境可以使用红/黑策略，生产环境使用滚动红/黑策略，它封装好了必须的步骤，用户不需要复杂操作，就可以实现企业级上线。</li></ul><h3 id="3-Spinnaker-架构所依赖的各个组件"><a href="#3-Spinnaker-架构所依赖的各个组件" class="headerlink" title="3.Spinnaker 架构所依赖的各个组件"></a><em>3.Spinnaker 架构所依赖的各个组件</em></h3><a id="more"></a><p><img src="https://cloud.qingye.info/images/20200326/20171205101757573.png" alt="Spinnaker 架构图"></p><ul><li>Deck：面向用户 UI 界面组件，提供直观简介的操作界面，可视化操作发布部署流程。</li><li>API： 面向调用 API 组件，我们可以不使用提供的 UI，直接调用 API 操作，由它后台帮我们执行发布等任务。</li><li>Gate：是 API 的网关组件，可以理解为代理，所有请求由其代理转发。</li><li>Rosco：是构建 beta 镜像的组件，需要配置 Packer 组件使用。</li><li>Orca：是核心流程引擎组件，用来管理流程。</li><li>Igor：是用来集成其他 CI 系统组件，如 Jenkins 等一个组件。</li><li>Echo：是通知系统组件，发送邮件等信息。</li><li>Front50：是存储管理组件，需要配置 Redis、Cassandra 等组件使用。</li><li>Cloud driver 是它用来适配不同的云平台的组件，比如 Kubernetes，Google、AWS EC2、Microsoft Azure 等。</li><li>Fiat 是鉴权的组件，配置权限管理，支持 OAuth、SAML、LDAP、GitHub teams、Azure groups、 Google Groups 等。</li></ul><p>各组件监听端口:</p><table><thead><tr><th>组件</th><th>端口</th><th>依赖组件</th><th>端口</th></tr></thead><tbody><tr><td>Clouddriver</td><td>7002</td><td>Redis</td><td>6379</td></tr><tr><td>Fiat</td><td>7003</td><td></td><td></td></tr><tr><td>Front50</td><td>8080</td><td>minio</td><td>9000</td></tr><tr><td>Orca</td><td>8083</td><td></td><td></td></tr><tr><td>Gate</td><td>8084</td><td></td><td></td></tr><tr><td>Rosco</td><td>8087</td><td></td><td></td></tr><tr><td>Igor</td><td>8088</td><td></td><td></td></tr><tr><td>Echo</td><td>8089</td><td></td><td></td></tr><tr><td>Deck</td><td>80</td><td></td><td></td></tr></tbody></table><p>以上组件除了核心组件外，一些组价可选择配置是否启动，比如不做权限管理的话，Fiat 就可以不启动，不集成其他 CI 的话，那就可以不启动 Igor 组件等。这些都可以在配置文件中配置，各个组件独立服务运行，有各自的服务端口，且各个组件都有自己的独立的项目 GitHub 地址</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;headerlink&quot; title=&quot;1.概念&quot;&gt;&lt;/a&gt;&lt;em&gt;1.概念&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;Spinnaker 是 Netflix 的开源项目，是一个持续交付平台，它定位于将产品快速且持续的部署到多种云平台上。Spinnaker 通过将发布和各个云平台解耦，来将部署流程流水线化，从而降低平台迁移或多云品台部署应用的复杂度，它本身内部支持 Google、AWS EC2、Microsoft Azure、Kubernetes和OpenStack 等云平台，并且它可以无缝集成其他持续集成（CI）流程，如 git、Jenkins、Travis CI、Docker registry、cron 调度器等。简而言之，Spinnaker是致力于提供在多种平台上实现开箱即用的集群管理和部署功能的平台。&lt;/p&gt;
&lt;h3 id=&quot;2-功能&quot;&gt;&lt;a href=&quot;#2-功能&quot; class=&quot;headerlink&quot; title=&quot;2.功能&quot;&gt;&lt;/a&gt;&lt;em&gt;2.功能&lt;/em&gt;&lt;/h3&gt;&lt;h4 id=&quot;2-1：集群管理主要用于管理云上的资源，它分为以下几个块&quot;&gt;&lt;a href=&quot;#2-1：集群管理主要用于管理云上的资源，它分为以下几个块&quot; class=&quot;headerlink&quot; title=&quot;2.1：集群管理主要用于管理云上的资源，它分为以下几个块&quot;&gt;&lt;/a&gt;2.1：集群管理主要用于管理云上的资源，它分为以下几个块&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Server Group：服务组，是资源管理单位，识别可部署组件和基础配置设置，它并且关联了一个负载均衡器和安全组，当部署完毕后，服务组就相当于一组运行中的软件实例集合，如（VM 实例，Kubernetes pods）。&lt;/li&gt;
&lt;li&gt;Cluster：集群，由用户定义的，对服务组的逻辑分组。&lt;/li&gt;
&lt;li&gt;Applications：应用，是对集群的逻辑分组。&lt;/li&gt;
&lt;li&gt;Load Balancer：负载均衡，用于将外部网络流量重定向到服务组中的机器实例，还可以指定一系列规则，用来对服务组中的机器实例做健康监测。&lt;/li&gt;
&lt;li&gt;Security Group：安全组，定义了网络访问权限，由IP、端口和通信协议组成的防火墙&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;2-2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分&quot;&gt;&lt;a href=&quot;#2-2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分&quot; class=&quot;headerlink&quot; title=&quot;2.2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分&quot;&gt;&lt;/a&gt;2.2：部署管理功能用于创建一个持续交付流程，它可分为管道和阶段两大部分&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;管道 部署管理的核心是管道，在Spinnaker的定义中，管道由一系列的阶段（stages）组成。管道可以人工触发，也可以配置为自动触发，比如由 Jenkins Job 完成时、Docker Images 上传到仓库时，CRON 定时器、其他管道中的某一阶段。同时，管道可以配置参数和通知，可以在管道一些阶段上执行时发送邮件消息。Spinnaker 已经内置了一些阶段，如执行自定义脚本、触发 Jenkins 任务等。&lt;/li&gt;
&lt;li&gt;阶段 阶段在 Spinnaker 中，可以作为管道的一个自动构建模块的功能组成。我们可以随意在管道中定义各个阶段执行顺序。Spinnaker 提供了很多阶段供我们选择使用，比如执行发布（Deploy）、执行自定义脚本 (script)、触发 Jenkins 任务 (jenkins)等，功能很强大。&lt;/li&gt;
&lt;li&gt;部署策略 Spinnaker 支持精细的部署策略，比如 红/黑（蓝/绿）部署，多阶段环境部署，滚动红/黑策略，canary 发布等。用户可以为每个环境使用不同部署策略，比如，测试环境可以使用红/黑策略，生产环境使用滚动红/黑策略，它封装好了必须的步骤，用户不需要复杂操作，就可以实现企业级上线。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;3-Spinnaker-架构所依赖的各个组件&quot;&gt;&lt;a href=&quot;#3-Spinnaker-架构所依赖的各个组件&quot; class=&quot;headerlink&quot; title=&quot;3.Spinnaker 架构所依赖的各个组件&quot;&gt;&lt;/a&gt;&lt;em&gt;3.Spinnaker 架构所依赖的各个组件&lt;/em&gt;&lt;/h3&gt;
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
      <category term="spinnaker" scheme="http://qingye.info/tags/spinnaker/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[7.harbor从1.7.5升级到1.9.0]</title>
    <link href="http://qingye.info/2020/03/25/k8s/K8S-7-harbor-update-to-1.9.0/"/>
    <id>http://qingye.info/2020/03/25/k8s/K8S-7-harbor-update-to-1.9.0/</id>
    <published>2020-03-25T15:35:07.000Z</published>
    <updated>2020-04-16T13:11:30.321Z</updated>
    
    <content type="html"><![CDATA[<h3 id="升级步骤"><a href="#升级步骤" class="headerlink" title="升级步骤"></a><em>升级步骤</em></h3><p>前文：harbor1.9新功能众多，包括tag 保留和配额、可与 CI/CD 工具集成的 Webhook 通知、数据复制、Syslog 集成以及 CVE 例外策略等安全功能。harbor在1.8版本改变较大，因此需要分两步进行升级，升级到v1.8.0，再升级到v1.9.0。</p><p>准备工作：<br>1.下载harbor1.8.0和1.9.0版本的离线安装包</p><p>wget <a href="https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz">https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz</a></p><p>wget <a href="https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.0.tgz">https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.0.tgz</a></p><p>2.创建文件备份的目录</p><p>mkdir /data_backup1</p><p>mkdir /data_backup2</p><p>3.从docker hub拉取镜像</p><p>docker pull goharbor/harbor-migrator:v1.8.0</p><p>docker pull goharbor/harbor-migrator:v1.9.0</p><p>一、harbor1.7.5升级到1.8.0<br>1.关闭harbor</p><p>cd /usr/local/harbor</p><p>docker-compose down</p><p>2.数据备份</p><a id="more"></a><p>mv /usr/local/harbor /data_backup1/</p><p>3.解压离线安装包</p><p>tar xf harbor-offline-installer-v1.8.0.tgz -C /usr/local/</p><p>4.升级harbor配置文件</p><p>docker run -it –rm -v /data_backup1/harbor/harbor.cfg:/harbor-migration/harbor-cfg/harbor.cfg -v /usr/local/harbor/harbor.yml:/harbor-migration/harbor-cfg-out/harbor.yml goharbor/harbor-migrator:v1.8.0 –cfg up</p><p>由于1.8后的版本docker-compse已交由harbor.yml控制，因此需要转化为将cfg文件转化为yml文件</p><p>5.安装并启动</p><p>cd /usr/local/harbor</p><p>sh install.sh –with-clair</p><p>看成执行成功就可以登录访问harbor了</p><p>6.测试</p><p>登录后看成版本是否为1.8.0，对应的镜像在不在</p><p>并尝试上传和拉取镜像，都成功则升级成功</p><p>二、harbor1.8.0升级到1.9.0<br>1.关闭harbor</p><p>cd /usr/local/harbor</p><p>docker-compose down</p><p>2.数据备份</p><p>mv /usr/local/harbor /data_backup2/</p><p>3.解压离线安装包</p><p>tar xf harbor-offline-installer-v1.9.0.tgz -C /usr/local/</p><p>4.升级harbor配置文件</p><p>！！！由于从1.8版本开始后不再需要cfg文件，因此需要升级的配置文件是yml，而且在1.9版本中新加入了一个参数chart，所以需要在1.8的yml文件中添加该参数</p><p>网上升级到1.9以上分享的人不多，没有找到完全一致的解决方案，因此这个坑要特别注意</p><p>对应问题可见 <a href="https://github.com/goharbor/harbor/issues/9146">https://github.com/goharbor/harbor/issues/9146</a></p><p>vim /data_backup2/harbor/harbor.yml ，在其中加入下面一段</p><p>chart:<br>  absolute_url: disabled</p><p>docker run -it –rm -v /data_backup2/harbor/harbor.yml:/harbor-migration/harbor-cfg/harbor.yml -v /usr/local/harbor/harbor.yml:/harbor-migration/harbor-cfg-out/harbor.yml goharbor/harbor-migrator:v1.9.0 –cfg up</p><p>5.安装并启动</p><p>cd /usr/local/harbor</p><p>sh install.sh –with-clair</p><p>看成执行成功就可以登录访问harbor了</p><p>6.测试</p><p>登录后看成版本是否为1.9.0，对应的镜像在不在</p><p>成功的版本界面如下图<br><img src="https://cloud.qingye.info/images/20200325/harbor.png" alt="harbor升级截图"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><em>总结</em></h3><ul><li>注意尝试上传和拉取镜像，都成功则升级成功</li><li>稳定运行一段时间，将旧版本的备份数据清理</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;升级步骤&quot;&gt;&lt;a href=&quot;#升级步骤&quot; class=&quot;headerlink&quot; title=&quot;升级步骤&quot;&gt;&lt;/a&gt;&lt;em&gt;升级步骤&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;前文：harbor1.9新功能众多，包括tag 保留和配额、可与 CI/CD 工具集成的 Webhook 通知、数据复制、Syslog 集成以及 CVE 例外策略等安全功能。harbor在1.8版本改变较大，因此需要分两步进行升级，升级到v1.8.0，再升级到v1.9.0。&lt;/p&gt;
&lt;p&gt;准备工作：&lt;br&gt;1.下载harbor1.8.0和1.9.0版本的离线安装包&lt;/p&gt;
&lt;p&gt;wget &lt;a href=&quot;https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz&quot;&gt;https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.0.tgz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;wget &lt;a href=&quot;https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.0.tgz&quot;&gt;https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.0.tgz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2.创建文件备份的目录&lt;/p&gt;
&lt;p&gt;mkdir /data_backup1&lt;/p&gt;
&lt;p&gt;mkdir /data_backup2&lt;/p&gt;
&lt;p&gt;3.从docker hub拉取镜像&lt;/p&gt;
&lt;p&gt;docker pull goharbor/harbor-migrator:v1.8.0&lt;/p&gt;
&lt;p&gt;docker pull goharbor/harbor-migrator:v1.9.0&lt;/p&gt;
&lt;p&gt;一、harbor1.7.5升级到1.8.0&lt;br&gt;1.关闭harbor&lt;/p&gt;
&lt;p&gt;cd /usr/local/harbor&lt;/p&gt;
&lt;p&gt;docker-compose down&lt;/p&gt;
&lt;p&gt;2.数据备份&lt;/p&gt;
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
      <category term="harbor" scheme="http://qingye.info/tags/harbor/"/>
    
      <category term="镜像仓库" scheme="http://qingye.info/tags/%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[6.harbor私有仓库部署]</title>
    <link href="http://qingye.info/2020/03/25/k8s/K8S-6-harbor/"/>
    <id>http://qingye.info/2020/03/25/k8s/K8S-6-harbor/</id>
    <published>2020-03-25T15:26:07.000Z</published>
    <updated>2020-04-16T13:10:43.533Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-私有仓库版本"><a href="#1-私有仓库版本" class="headerlink" title="1.私有仓库版本"></a><em>1.私有仓库版本</em></h3><p>镜像仓库版本：</p><ul><li>Harbor v1.7.5</li><li>Clair v2.0.7（漏洞库{+}<a href="https://github.com/coreos/clair+）">https://github.com/coreos/clair+）</a></li></ul><h3 id="2-部署步骤"><a href="#2-部署步骤" class="headerlink" title="2.部署步骤"></a><em>2.部署步骤</em></h3><ul><li>内网组件资源要事先准备好</li></ul><p>2.1.安装docker 服务</p><p>yum -y install <a href="http://192.168.3.146/docker-ce-cli-18.09.0-3.el7.x86_64.rpm">http://192.168.3.146/docker-ce-cli-18.09.0-3.el7.x86_64.rpm</a> <a href="http://192.168.3.146/docker-ce-18.09.3-3.el7.x86_64.rpm">http://192.168.3.146/docker-ce-18.09.3-3.el7.x86_64.rpm</a> <a href="http://192.168.3.146/containerd.io-1.2.4-3.1.el7.x86_64.rpm">http://192.168.3.146/containerd.io-1.2.4-3.1.el7.x86_64.rpm</a></p><p>2.2.安装docker-docker-compose</p><p>wget  <a href="http://192.168.3.146/docker-compose">http://192.168.3.146/docker-compose</a> </p><p>mv docker-compose /usr/local/bin/docker-compose &amp;&amp; chmod 755  /usr/local/bin/docker-compose</p><p>2.3.部署harbor</p><p>cd /usr/local/src &amp;&amp; wget  <a href="http://192.168.3.146/harbor-offline-installer-v1.7.5.tgz">http://192.168.3.146/harbor-offline-installer-v1.7.5.tgz</a> &amp;&amp; tar -zxvf  harbor-offline-installer-v1.7.5.tgz &amp;&amp; mv harbor /usr/local/</p><p>修改harbor 配置文件 /usr/local/harbor/harbor.cfg，一般修改hostname为harbor的访问域名，ui_url_protocol为访问协议，设置成http，harbor_admin_password 设置harbor的admin账号默认密码，其他邮箱，认证模式可以部署完系统之后在系统配置页面修改。</p><p>执行安装命令:cd /usr/local/harbor &amp;&amp; sh install.sh –with-clair</p><p>harbor启动命令:docker-compose -f docker-compose.clair.yml -f docker-compose.yml start</p><p>2.4.配置复制管理(记得做主从复制harbor的机器仓库的域名都要指向主harbor那一台，要不做镜像复制同步就有问题)</p><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a><em>3.总结</em></h3><ul><li>记得做主从复制harbor的机器仓库的域名都要指向主harbor那一台，要不做镜像复制同步就有问题</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-私有仓库版本&quot;&gt;&lt;a href=&quot;#1-私有仓库版本&quot; class=&quot;headerlink&quot; title=&quot;1.私有仓库版本&quot;&gt;&lt;/a&gt;&lt;em&gt;1.私有仓库版本&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;镜像仓库版本：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Harbor v1.7.5&lt;/l
      
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
      <category term="harbor" scheme="http://qingye.info/tags/harbor/"/>
    
      <category term="镜像仓库" scheme="http://qingye.info/tags/%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[5.k8s高可用集群部署]</title>
    <link href="http://qingye.info/2020/03/25/k8s/K8S-5-%20High-availability-cluster-deployment/"/>
    <id>http://qingye.info/2020/03/25/k8s/K8S-5-%20High-availability-cluster-deployment/</id>
    <published>2020-03-25T13:59:07.000Z</published>
    <updated>2020-04-16T13:10:18.196Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-高可用集群概述"><a href="#1-高可用集群概述" class="headerlink" title="1.高可用集群概述"></a><em>1.高可用集群概述</em></h3><p>部署k8s的高可用集群，要做到无单电，主要是依靠负载均衡和k8s集群自身的高可用性。对于k8s的api，它本身是无状态的，自身不具备高可用，但是如果要它提供高可用的服务，做到其他节点和组件能无时无刻都可以访问它，需要在k8s api前面加一层tcp负载均衡器。一般k8s集群至少三个master节点，所以tcp转发层需要对这三台k8s api节点做负载均衡。这里有人可能问了，为什么需要tcp负载均衡，不做http负载均衡？原因就是k8s api的通信是https加密的，负载均衡器做https解析会比较麻烦，而且做tcp负载均衡效率也会高一些，也会比较方便，高版本的nginx就直接支持tcp转发。还有就是这三台master节点的服务器怎么放呢？这个要看个人的资源环境，比如你只是单机房的，那只能把master节点部署在一个机房里面了，然后三台服务器之间加个vip，使用keepalived做vip漂移，达到k8s api的高可用，如果你是同城三机房的，可以在每个机房部署一个master节点，然后每个机房部署一个高可用的负载均衡器，负载均衡器把请求转发到三个机房的k8s api上面去。然后对于etcd数据库，他本身自带高可用功能，部署三个以上节点的节点就好了，对于kube-scheduler组件，它们内部会自动选择一个leader节点，默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态，对于kube-controller-manager和kube-scheduler也是类似，默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态。对于k8s核心组件的功能，下面再说明一下：</p><p><strong>kube-apiserver：</strong>集群核心,集群API接口、集群各个组件通信的中枢；集群安全控制。</p><p><strong>etcd：</strong>集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复；因此高可用集群部署首先就是etcd是高可用集群；</p><p><strong>kube-scheduler：</strong>集群Pod的调度中心；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态；</p><p><strong>kube-controller-manager：</strong>集群状态管理器，当集群状态与期望不同时，kcm会努力让集群恢复期望状态，比如：当一个pod死掉，kcm会努力新建一个pod来恢复对应replicas set期望的状态；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态；</p><p><strong>kubelet：</strong>kubernetes node agent，负责与node上的docker engine打交道；</p><p><strong>kube-proxy：</strong>每个node上一个，负责service vip到endpoint pod的流量转发，当前主要通过设置iptables规则实现。</p><h3 id="2-k8s高可用架构图"><a href="#2-k8s高可用架构图" class="headerlink" title="2.k8s高可用架构图"></a><em>2.k8s高可用架构图</em></h3><p>这里单机房和多机房的区别就在于单机房整个机房就一个vip，多机房就是每个机房都有一个vip，每个机房都有一个api访问入口。<br><img src="https://cloud.qingye.info/images/20200325/k8s-ha.png" alt="Kubernetes 高可用架构图"></p><a id="more"></a><h3 id="3-部署过程"><a href="#3-部署过程" class="headerlink" title="3.部署过程"></a><em>3.部署过程</em></h3><ul><li>本次单机房v1.9.2版本，版本比较老了，因为文档比较老了，不过新版本大同小异</li></ul><h4 id="3-1：资源清单"><a href="#3-1：资源清单" class="headerlink" title="3.1：资源清单"></a>3.1：资源清单</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">主机节点清单</span></span><br><span class="line">    <span class="string">主机名</span>                 <span class="string">IP地址</span>               <span class="string">说明</span>            <span class="string">组件</span></span><br><span class="line">    <span class="string">k8s-master1</span>        <span class="number">192.168</span><span class="number">.3</span><span class="number">.148</span>    <span class="string">master</span>        <span class="string">keepalived、nginx、etcd、kubelet、kube-apiserver、kube-scheduler、kube-proxy、kube-dashboard、heapster、calico</span></span><br><span class="line">    <span class="string">k8s-master2</span>        <span class="number">192.168</span><span class="number">.3</span><span class="number">.149</span>    <span class="string">master</span>        <span class="string">keepalived、nginx、etcd、kubelet、kube-apiserver、kube-scheduler、kube-proxy、kube-dashboard、heapster、calico</span></span><br><span class="line">    <span class="string">k8s-master3</span>        <span class="number">192.168</span><span class="number">.3</span><span class="number">.150</span>    <span class="string">master</span>        <span class="string">keepalived、nginx、etcd、kubelet、kube-apiserver、kube-scheduler、kube-proxy、kube-dashboard、heapster、calico</span></span><br><span class="line">    <span class="string">无</span>                        <span class="number">192.168</span><span class="number">.3</span><span class="number">.157</span>    <span class="string">虚拟IP</span>        <span class="string">无</span></span><br><span class="line">    <span class="string">k8s-node1</span>           <span class="number">192.168</span><span class="number">.3</span><span class="number">.154</span>    <span class="string">node</span>          <span class="string">kubelet、kube-proxy</span></span><br><span class="line">    <span class="string">k8s-node2</span>           <span class="number">192.168</span><span class="number">.3</span><span class="number">.155</span>    <span class="string">node</span>          <span class="string">kubelet、kube-proxy</span></span><br><span class="line"></span><br><span class="line"><span class="string">kubernetes对应docker版本</span></span><br><span class="line">    <span class="string">Kubernetes</span> <span class="number">1.9</span>  <span class="string">&lt;--Docker</span> <span class="number">1.11</span><span class="number">.2</span> <span class="string">to</span> <span class="number">1.13</span><span class="number">.1</span> <span class="string">and</span> <span class="number">17.03</span><span class="string">.x</span></span><br><span class="line">    <span class="string">Kubernetes</span> <span class="number">1.8</span>  <span class="string">&lt;--Docker</span> <span class="number">1.11</span><span class="number">.2</span> <span class="string">to</span> <span class="number">1.13</span><span class="number">.1</span> <span class="string">and</span> <span class="number">17.03</span><span class="string">.x</span></span><br><span class="line">    <span class="string">Kubernetes</span> <span class="number">1.7</span>  <span class="string">&lt;--Docker</span> <span class="number">1.10</span><span class="number">.3</span><span class="string">,</span>  <span class="number">1.11</span><span class="number">.2</span><span class="string">,</span>  <span class="number">1.12</span><span class="number">.6</span></span><br><span class="line">    <span class="string">Kubernetes</span> <span class="number">1.6</span>  <span class="string">&lt;--Docker</span> <span class="number">1.10</span><span class="number">.3</span><span class="string">,</span>  <span class="number">1.11</span><span class="number">.2</span><span class="string">,</span>  <span class="number">1.12</span><span class="number">.6</span></span><br><span class="line">    <span class="string">Kubernetes</span> <span class="number">1.5</span>  <span class="string">&lt;--Docker</span> <span class="number">1.10</span><span class="number">.3</span><span class="string">,</span>  <span class="number">1.11</span><span class="number">.2</span><span class="string">,</span>  <span class="number">1.12</span><span class="number">.3</span></span><br><span class="line"></span><br><span class="line"><span class="string">docker镜像准备</span></span><br><span class="line">    <span class="string">master节点：</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.3</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/kube-proxy-amd64:v1.9.2</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/kube-controller-manager-amd64:v1.9.2</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/kube-apiserver-amd64:v1.9.2</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/kube-scheduler-amd64:v1.9.2</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.7</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.7</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.7</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/etcd-amd64:3.1.10</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/heapster-grafana-amd64:v4.4.3</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/heapster-amd64:v1.4.2</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/pause-amd64:3.0</span></span><br><span class="line">    <span class="string">quay.io/calico/node:v3.0.3</span></span><br><span class="line">    <span class="string">quay.io/calico/kube-controllers:v2.0.1</span></span><br><span class="line">    <span class="string">quay.io/calico/cni:v2.0.1</span></span><br><span class="line">    <span class="string">quay.io/coreos/flannel:v0.9.1-amd64</span></span><br><span class="line">    <span class="string">nginx:latest</span></span><br><span class="line"></span><br><span class="line">    <span class="string">node节点:</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/pause-amd64:3.0</span></span><br><span class="line">    <span class="string">gcr.io/google_containers/kube-proxy-amd64:v1.9.2</span></span><br><span class="line">    <span class="string">quay.io/calico/node:v3.0.3</span></span><br><span class="line">    <span class="string">quay.io/calico/kube-controllers:v2.0.1</span></span><br><span class="line">    <span class="string">quay.io/calico/cni:v2.0.1</span></span><br><span class="line">    <span class="string">quay.io/coreos/flannel:v0.9.1-amd64</span></span><br></pre></td></tr></table></figure><h4 id="3-2：软件安装-所有节点都同样处理"><a href="#3-2：软件安装-所有节点都同样处理" class="headerlink" title="3.2：软件安装,所有节点都同样处理"></a>3.2：软件安装,所有节点都同样处理</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">    mkdir -p /data/soft/docker</span><br><span class="line">    yum -y install epel-release</span><br><span class="line">    yum install -y git wget lrzsz vim net-tools yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">    yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">    cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF</span><br><span class="line">    [kubernetes]</span><br><span class="line">    name=Kubernetes</span><br><span class="line">    baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">    enabled=1</span><br><span class="line">    gpgcheck=0</span><br><span class="line">    EOF</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cd</span> /data/soft/docker</span><br><span class="line">    yumdownloader docker-ce-17.03.0.ce</span><br><span class="line">    yumdownloader docker-ce-selinux-17.03.0.ce</span><br><span class="line">    yum remove docker docker-common docker-selinux docker-engine</span><br><span class="line">    yum localinstall *</span><br><span class="line">    yum -y install kubelet-1.9.2 kubectl-1.9.2 kubeadm-1.9.2</span><br><span class="line"></span><br><span class="line">    systemctl stop firewalld &amp;&amp; systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">    systemctl start docker &amp;&amp; systemctl <span class="built_in">enable</span> docker</span><br><span class="line">    systemctl start kubelet &amp;&amp; systemctl <span class="built_in">enable</span> kubelet</span><br><span class="line"></span><br><span class="line">    master节点：</span><br><span class="line">    yum install -y keepalived</span><br><span class="line">    systemctl <span class="built_in">enable</span> keepalived &amp;&amp; systemctl start keepalived</span><br></pre></td></tr></table></figure><h4 id="3-3：系统设置，所有节点都同样处理"><a href="#3-3：系统设置，所有节点都同样处理" class="headerlink" title="3.3：系统设置，所有节点都同样处理"></a>3.3：系统设置，所有节点都同样处理</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#selinux</span></span><br><span class="line">    setenforce 0</span><br><span class="line">    cp /etc/selinux/config /etc/selinux/config.bak</span><br><span class="line">    sed -i <span class="string">'/SELINUX=enforcing/s/enforcing/disabled/'</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置系统路由参数,防止kubeadm报路由警告、禁用swap</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"</span></span><br><span class="line"><span class="string">    vm.swappiness = 0</span></span><br><span class="line"><span class="string">    net.bridge.bridge-nf-call-ip6tables = 1</span></span><br><span class="line"><span class="string">    net.bridge.bridge-nf-call-iptables = 1</span></span><br><span class="line"><span class="string">    net.ipv4.ip_forward=1</span></span><br><span class="line"><span class="string">    "</span> &gt;&gt; /etc/sysctl.conf</span><br><span class="line">    sysctl -p</span><br><span class="line">    swapoff -a</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改kubelet环境变量,修改KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs</span></span><br><span class="line"></span><br><span class="line">    cp /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.bak</span><br><span class="line">    sed -i <span class="string">'/KUBELET_CGROUP_ARGS/s/systemd/cgroupfs/'</span> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line"></span><br><span class="line">    cat &gt; /etc/systemd/system/kubelet.service.d/20-pod-infra-image.conf &lt;&lt;EOF</span><br><span class="line">    [Service]</span><br><span class="line">    Environment=<span class="string">"KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.test.sui.internal/test/pause-amd64:3.0"</span></span><br><span class="line">    EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改docker配置</span></span><br><span class="line">    cp -p /usr/lib/systemd/system/docker.service /usr/lib/systemd/system/docker.service.bak</span><br><span class="line">    sed -i <span class="string">'/ExecStart=/s/$/&amp; --insecure-registry registry.test.sui.internal/'</span> /usr/lib/systemd/system/docker.service</span><br><span class="line"></span><br><span class="line">    systemctl daemon-reload</span><br><span class="line">    systemctl restart kubelet</span><br><span class="line">    systemctl restart docker</span><br><span class="line"></span><br><span class="line"><span class="comment">#防火墙设置</span></span><br><span class="line">    master:在所有master节点上开放相关firewalld端口（因为以上服务基于docker部署，如果docker版本为17.x，可以不进行以下设置，因为docker会自动修改iptables添加相关端口）</span><br><span class="line">    协议    方向    端口        说明</span><br><span class="line">    TCP        Inbound    16443*        Load balancer Kubernetes API server port</span><br><span class="line">    TCP        Inbound    6443*        Kubernetes API server</span><br><span class="line">    TCP        Inbound    4001        etcd listen client port</span><br><span class="line">    TCP        Inbound    2379-2380    etcd server client API</span><br><span class="line">    TCP        Inbound    10250        Kubelet API</span><br><span class="line">    TCP        Inbound    10251        kube-scheduler</span><br><span class="line">    TCP        Inbound    10252        kube-controller-manager</span><br><span class="line">    TCP        Inbound    10255        Read-only Kubelet API</span><br><span class="line">    TCP        Inbound    30000-32767    NodePort Services</span><br><span class="line"></span><br><span class="line">    $ systemctl status firewalld</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=16443/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=6443/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=4001/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=2379-2380/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=10250/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=10251/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=10252/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=10255/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=30000-32767/tcp --permanent</span><br><span class="line">    $ firewall-cmd --reload</span><br><span class="line">    $ firewall-cmd --list-all --zone=public</span><br><span class="line"></span><br><span class="line">    public (active)</span><br><span class="line">      target: default</span><br><span class="line">      icmp-block-inversion: no</span><br><span class="line">      interfaces: ens2f1 ens1f0 nm-bond</span><br><span class="line">      sources:</span><br><span class="line">      services: ssh dhcpv6-client</span><br><span class="line">      ports: 4001/tcp 6443/tcp 2379-2380/tcp 10250/tcp 10251/tcp 10252/tcp 10255/tcp 30000-32767/tcp</span><br><span class="line">      protocols:</span><br><span class="line">      masquerade: no</span><br><span class="line">      forward-ports:</span><br><span class="line">      <span class="built_in">source</span>-ports:</span><br><span class="line">      icmp-blocks:</span><br><span class="line">      rich rules:</span><br><span class="line"></span><br><span class="line"><span class="comment">#node:在所有worker节点上开放相关firewalld端口（因为以上服务基于docker部署，如果docker版本为17.x，可以不进行以下设置，因为docker会自动修改iptables添加相关端口）</span></span><br><span class="line">    协议    方向    端口        说明</span><br><span class="line">    TCP        Inbound    10250        Kubelet API</span><br><span class="line">    TCP        Inbound    10255        Read-only Kubelet API</span><br><span class="line">    TCP        Inbound    30000-32767    NodePort Services</span><br><span class="line"></span><br><span class="line">    $ systemctl status firewalld</span><br><span class="line"></span><br><span class="line">    $ firewall-cmd --zone=public --add-port=10250/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=10255/tcp --permanent</span><br><span class="line">    $ firewall-cmd --zone=public --add-port=30000-32767/tcp --permanent</span><br><span class="line"></span><br><span class="line">    $ firewall-cmd --reload</span><br><span class="line"></span><br><span class="line">    $ firewall-cmd --list-all --zone=public</span><br><span class="line">    public (active)</span><br><span class="line">      target: default</span><br><span class="line">      icmp-block-inversion: no</span><br><span class="line">      interfaces: ens2f1 ens1f0 nm-bond</span><br><span class="line">      sources:</span><br><span class="line">      services: ssh dhcpv6-client</span><br><span class="line">      ports: 10250/tcp 10255/tcp 30000-32767/tcp</span><br><span class="line">      protocols:</span><br><span class="line">      masquerade: no</span><br><span class="line">      forward-ports:</span><br><span class="line">      <span class="built_in">source</span>-ports:</span><br><span class="line">      icmp-blocks:</span><br><span class="line">      rich rules:</span><br><span class="line"></span><br><span class="line">所有节点,在所有kubernetes节点上允许kube-proxy的forward</span><br><span class="line">    $ firewall-cmd --permanent --direct --add-rule ipv4 filter INPUT 1 -i docker0 -j ACCEPT -m comment --comment <span class="string">"kube-proxy redirects"</span></span><br><span class="line">    $ firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD 1 -o docker0 -j ACCEPT -m comment --comment <span class="string">"docker subnet"</span></span><br><span class="line">    $ firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD 1 -i flannel.1 -j ACCEPT -m comment --comment <span class="string">"flannel subnet"</span></span><br><span class="line">    $ firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD 1 -o flannel.1 -j ACCEPT -m comment --comment <span class="string">"flannel subnet"</span></span><br><span class="line">    $ firewall-cmd --reload</span><br><span class="line"></span><br><span class="line">    $ firewall-cmd --direct --get-all-rules</span><br><span class="line">    ipv4 filter INPUT 1 -i docker0 -j ACCEPT -m comment --comment <span class="string">'kube-proxy redirects'</span></span><br><span class="line">    ipv4 filter FORWARD 1 -o docker0 -j ACCEPT -m comment --comment <span class="string">'docker subnet'</span></span><br><span class="line">    ipv4 filter FORWARD 1 -i flannel.1 -j ACCEPT -m comment --comment <span class="string">'flannel subnet'</span></span><br><span class="line">    ipv4 filter FORWARD 1 -o flannel.1 -j ACCEPT -m comment --comment <span class="string">'flannel subnet'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#在所有kubernetes节点上，删除iptables的设置，解决kube-proxy无法启用nodePort。（注意：每次重启firewalld必须执行以下命令）</span></span><br><span class="line">    iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited</span><br></pre></td></tr></table></figure><h4 id="3-3：配置文件初始化"><a href="#3-3：配置文件初始化" class="headerlink" title="3.3：配置文件初始化"></a>3.3：配置文件初始化</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#在所有master节点上获取代码，并进入代码目录</span></span><br><span class="line">    git <span class="built_in">clone</span> https://github.com/cookeem/kubeadm-ha</span><br><span class="line">    <span class="built_in">cd</span> kubeadm-ha</span><br><span class="line"></span><br><span class="line">    <span class="comment">#在所有master节点上设置初始化脚本配置，每一项配置参见脚本中的配置说明，请务必正确配置。该脚本用于生成相关重要的配置文件</span></span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># cat create-config.sh</span></span><br><span class="line">    <span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># local machine ip address</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_IPLOCAL=192.168.3.148</span><br><span class="line"></span><br><span class="line">    <span class="comment"># local machine etcd name, options: etcd1, etcd2, etcd3</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_ETCDNAME=etcd1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_KA_STATE=MASTER</span><br><span class="line"></span><br><span class="line">    <span class="comment"># local machine keepalived priority config, options: 102, 101, 100. MASTER must 102</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_KA_PRIO=102</span><br><span class="line"></span><br><span class="line">    <span class="comment"># local machine keepalived network interface name config, for example: eth0</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_KA_INTF=ens3</span><br><span class="line"></span><br><span class="line">    <span class="comment">#######################################</span></span><br><span class="line">    <span class="comment"># all masters settings below must be same</span></span><br><span class="line">    <span class="comment">#######################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># master keepalived virtual ip address</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_IPVIRTUAL=192.168.3.157</span><br><span class="line"></span><br><span class="line">    <span class="comment"># master01 ip address</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_IP1=192.168.3.148</span><br><span class="line"></span><br><span class="line">    <span class="comment"># master02 ip address</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_IP2=192.168.3.149</span><br><span class="line"></span><br><span class="line">    <span class="comment"># master03 ip address</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_IP3=192.168.3.150</span><br><span class="line"></span><br><span class="line">    <span class="comment"># master01 hostname</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_HOSTNAME1=k8s-master1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># master02 hostname</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_HOSTNAME2=k8s-master2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># master03 hostname</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_HOSTNAME3=k8s-master3</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keepalived auth_pass config, all masters must be same</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_KA_AUTH=4cdf7dc3b4c90194d1600c483e10ad1d</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kubernetes cluster token, you can use 'kubeadm token generate' to get a new one</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_TOKEN=7f276c.0741d82a5337f526</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kubernetes CIDR pod subnet, if CIDR pod subnet is "10.244.0.0/16" please set to "10.244.0.0\\/16"</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_CIDR=10.244.0.0\\/16</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kubernetes CIDR service subnet, if CIDR service subnet is "10.96.0.0/12" please set to "10.96.0.0\\/12"</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_SVC_CIDR=10.96.0.0\\/16</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calico network settings, set a reachable ip address for the cluster network interface, for example you can use the gateway ip address</span></span><br><span class="line">    <span class="built_in">export</span> K8SHA_CALICO_REACHABLE_IP=192.168.3.1</span><br><span class="line"></span><br><span class="line">    <span class="comment">##############################</span></span><br><span class="line">    <span class="comment"># please do not modify anything below</span></span><br><span class="line">    <span class="comment">##############################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># set etcd cluster docker-compose.yaml file</span></span><br><span class="line">    sed \</span><br><span class="line">    -e <span class="string">"s/K8SHA_ETCDNAME/<span class="variable">$K8SHA_ETCDNAME</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IPLOCAL/<span class="variable">$K8SHA_IPLOCAL</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP1/<span class="variable">$K8SHA_IP1</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP2/<span class="variable">$K8SHA_IP2</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP3/<span class="variable">$K8SHA_IP3</span>/g"</span> \</span><br><span class="line">    etcd/docker-compose.yaml.tpl &gt; etcd/docker-compose.yaml</span><br><span class="line"></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">'set etcd cluster docker-compose.yaml file success: etcd/docker-compose.yaml'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># set keepalived config file</span></span><br><span class="line">    mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak</span><br><span class="line"></span><br><span class="line">    cp keepalived/check_apiserver.sh /etc/keepalived/</span><br><span class="line"></span><br><span class="line">    sed \</span><br><span class="line">    -e <span class="string">"s/K8SHA_KA_STATE/<span class="variable">$K8SHA_KA_STATE</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_KA_INTF/<span class="variable">$K8SHA_KA_INTF</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IPLOCAL/<span class="variable">$K8SHA_IPLOCAL</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_KA_PRIO/<span class="variable">$K8SHA_KA_PRIO</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IPVIRTUAL/<span class="variable">$K8SHA_IPVIRTUAL</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_KA_AUTH/<span class="variable">$K8SHA_KA_AUTH</span>/g"</span> \</span><br><span class="line">    keepalived/keepalived.conf.tpl &gt; /etc/keepalived/keepalived.conf</span><br><span class="line"></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">'set keepalived config file success: /etc/keepalived/keepalived.conf'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># set nginx load balancer config file</span></span><br><span class="line">    sed \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP1/<span class="variable">$K8SHA_IP1</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP2/<span class="variable">$K8SHA_IP2</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP3/<span class="variable">$K8SHA_IP3</span>/g"</span> \</span><br><span class="line">    nginx-lb/nginx-lb.conf.tpl &gt; nginx-lb/nginx-lb.conf</span><br><span class="line"></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">'set nginx load balancer config file success: nginx-lb/nginx-lb.conf'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># set kubeadm init config file</span></span><br><span class="line">    sed \</span><br><span class="line">    -e <span class="string">"s/K8SHA_HOSTNAME1/<span class="variable">$K8SHA_HOSTNAME1</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_HOSTNAME2/<span class="variable">$K8SHA_HOSTNAME2</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_HOSTNAME3/<span class="variable">$K8SHA_HOSTNAME3</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP1/<span class="variable">$K8SHA_IP1</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP2/<span class="variable">$K8SHA_IP2</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IP3/<span class="variable">$K8SHA_IP3</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_IPVIRTUAL/<span class="variable">$K8SHA_IPVIRTUAL</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_TOKEN/<span class="variable">$K8SHA_TOKEN</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_CIDR/<span class="variable">$K8SHA_CIDR</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_SVC_CIDR/<span class="variable">$K8SHA_SVC_CIDR</span>/g"</span> \</span><br><span class="line">    kubeadm-init.yaml.tpl &gt; kubeadm-init.yaml</span><br><span class="line"></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">'set kubeadm init config file success: kubeadm-init.yaml'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># set canal deployment config file</span></span><br><span class="line"></span><br><span class="line">    sed \</span><br><span class="line">    -e <span class="string">"s/K8SHA_CIDR/<span class="variable">$K8SHA_CIDR</span>/g"</span> \</span><br><span class="line">    -e <span class="string">"s/K8SHA_CALICO_REACHABLE_IP/<span class="variable">$K8SHA_CALICO_REACHABLE_IP</span>/g"</span> \</span><br><span class="line">    kube-canal/canal.yaml.tpl &gt; kube-canal/canal.yaml</span><br><span class="line"></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">'set canal deployment config file success: kube-canal/canal.yaml'</span></span><br></pre></td></tr></table></figure><h4 id="3-4：在所有master节点上运行配置脚本，创建对应的配置文件，配置文件包括"><a href="#3-4：在所有master节点上运行配置脚本，创建对应的配置文件，配置文件包括" class="headerlink" title="3.4：在所有master节点上运行配置脚本，创建对应的配置文件，配置文件包括"></a>3.4：在所有master节点上运行配置脚本，创建对应的配置文件，配置文件包括</h4><p>1.etcd集群docker-compose.yaml文件<br>2.keepalived配置文件<br>3.nginx负载均衡集群docker-compose.yaml文件<br>4.kubeadm init 配置文件<br>5.canal配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">    $ ./create-config.sh</span><br><span class="line">    <span class="built_in">set</span> etcd cluster docker-compose.yaml file success: etcd/docker-compose.yaml</span><br><span class="line">    <span class="built_in">set</span> keepalived config file success: /etc/keepalived/keepalived.conf</span><br><span class="line">    <span class="built_in">set</span> nginx load balancer config file success: nginx-lb/nginx-lb.conf</span><br><span class="line">    <span class="built_in">set</span> kubeadm init config file success: kubeadm-init.yaml</span><br><span class="line">    <span class="built_in">set</span> canal deployment config file success: kube-canal/canal.yaml</span><br><span class="line"></span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># cat etcd/docker-compose.yaml</span></span><br><span class="line">    version: <span class="string">'2'</span></span><br><span class="line">    services:</span><br><span class="line">      etcd:</span><br><span class="line">        image: gcr.io/google_containers/etcd-amd64:3.1.10</span><br><span class="line">        container_name: etcd</span><br><span class="line">        hostname: etcd</span><br><span class="line">        volumes:</span><br><span class="line">        - /etc/ssl/certs:/etc/ssl/certs</span><br><span class="line">        - /var/lib/etcd-cluster:/var/lib/etcd</span><br><span class="line">        ports:</span><br><span class="line">        - 4001:4001</span><br><span class="line">        - 2380:2380</span><br><span class="line">        - 2379:2379</span><br><span class="line">        restart: always</span><br><span class="line">        <span class="built_in">command</span>: [<span class="string">"sh"</span>, <span class="string">"-c"</span>, <span class="string">"etcd --name=etcd1 \</span></span><br><span class="line"><span class="string">          --advertise-client-urls=http://192.168.3.148:2379,http://192.168.3.148:4001 \</span></span><br><span class="line"><span class="string">          --listen-client-urls=http://0.0.0.0:2379,http://0.0.0.0:4001 \</span></span><br><span class="line"><span class="string">          --initial-advertise-peer-urls=http://192.168.3.148:2380 \</span></span><br><span class="line"><span class="string">          --listen-peer-urls=http://0.0.0.0:2380 \</span></span><br><span class="line"><span class="string">          --initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \</span></span><br><span class="line"><span class="string">          --initial-cluster=etcd1=http://192.168.3.148:2380,etcd2=http://192.168.3.149:2380,etcd3=http://192.168.3.150:2380 \</span></span><br><span class="line"><span class="string">          --initial-cluster-state=new \</span></span><br><span class="line"><span class="string">          --auto-tls \</span></span><br><span class="line"><span class="string">          --peer-auto-tls \</span></span><br><span class="line"><span class="string">          --data-dir=/var/lib/etcd"</span>]</span><br><span class="line"></span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># ls keepalived/</span></span><br><span class="line">    check_apiserver.sh  keepalived.conf.tpl</span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># cat /etc/keepalived/keepalived.conf</span></span><br><span class="line">    ! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">    global_defs &#123;</span><br><span class="line">        router_id LVS_DEVEL</span><br><span class="line">    &#125;</span><br><span class="line">    vrrp_script chk_apiserver &#123;</span><br><span class="line">        script <span class="string">"/etc/keepalived/check_apiserver.sh"</span></span><br><span class="line">        interval 2</span><br><span class="line">        weight -5</span><br><span class="line">        fall 3  </span><br><span class="line">        rise 2</span><br><span class="line">    &#125;</span><br><span class="line">    vrrp_instance VI_1 &#123;</span><br><span class="line">        state MASTER</span><br><span class="line">        interface ens3</span><br><span class="line">        mcast_src_ip 192.168.3.148</span><br><span class="line">        virtual_router_id 157</span><br><span class="line">        priority 102</span><br><span class="line">        advert_int 2</span><br><span class="line">        authentication &#123;</span><br><span class="line">            auth_type PASS</span><br><span class="line">            auth_pass 4cdf7dc3b4c90194d1600c483e10ad1d</span><br><span class="line">        &#125;</span><br><span class="line">        virtual_ipaddress &#123;</span><br><span class="line">            192.168.3.157</span><br><span class="line">        &#125;</span><br><span class="line">        track_script &#123;</span><br><span class="line">           chk_apiserver</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># cat nginx-lb/docker-compose.yaml</span></span><br><span class="line">    version: <span class="string">'2'</span></span><br><span class="line">    services:</span><br><span class="line">      etcd:</span><br><span class="line">        image: nginx:latest</span><br><span class="line">        container_name: nginx-lb</span><br><span class="line">        hostname: nginx-lb</span><br><span class="line">        volumes:</span><br><span class="line">        - ./nginx-lb.conf:/etc/nginx/nginx.conf</span><br><span class="line">        ports:</span><br><span class="line">        - 16443:16443</span><br><span class="line">        restart: always</span><br><span class="line"></span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># cat kubeadm-init.yaml</span></span><br><span class="line">    apiVersion: kubeadm.k8s.io/v1alpha1</span><br><span class="line">    kind: MasterConfiguration</span><br><span class="line">    kubernetesVersion: v1.9.2</span><br><span class="line">    networking:</span><br><span class="line">      podSubnet: 10.244.0.0/16</span><br><span class="line">      serviceSubnet: 10.96.0.0/16</span><br><span class="line">    apiServerCertSANs:</span><br><span class="line">    - k8s-master1</span><br><span class="line">    - k8s-master2</span><br><span class="line">    - k8s-master3</span><br><span class="line">    - 192.168.3.148</span><br><span class="line">    - 192.168.3.149</span><br><span class="line">    - 192.168.3.150</span><br><span class="line">    - 192.168.3.157</span><br><span class="line">    - 127.0.0.1</span><br><span class="line">    etcd:</span><br><span class="line">      endpoints:</span><br><span class="line">      - http://192.168.3.148:2379</span><br><span class="line">      - http://192.168.3.149:2379</span><br><span class="line">      - http://192.168.3.150:2379</span><br><span class="line">    token: 7f276c.0741d82a5337f526</span><br><span class="line">    tokenTTL: <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#独立etcd集群部署,如已安装过，需清除历史数据</span></span><br><span class="line">    rm -rf /var/lib/etcd</span><br><span class="line">    docker-compose --file etcd/docker-compose.yaml stop</span><br><span class="line">    docker-compose --file etcd/docker-compose.yaml rm -f</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动etcd集群</span></span><br><span class="line">    docker-compose --file etcd/docker-compose.yaml up -d</span><br><span class="line"></span><br><span class="line"><span class="comment">#验证etcd集群状态是否正常</span></span><br><span class="line">    docker <span class="built_in">exec</span> -ti etcd etcdctl cluster-health</span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># docker exec -ti etcd etcdctl cluster-health</span></span><br><span class="line">    member 89e5a572671ddffc is healthy: got healthy result from http://192.168.3.148:2379</span><br><span class="line">    member bf48e493e3fe022d is healthy: got healthy result from http://192.168.3.149:2379</span><br><span class="line">    member d9c99f531d70dd69 is healthy: got healthy result from http://192.168.3.150:2379</span><br><span class="line">    cluster is healthy</span><br><span class="line"></span><br><span class="line">    docker <span class="built_in">exec</span> -ti etcd etcdctl member list</span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># docker exec -ti etcd etcdctl member list</span></span><br><span class="line">    89e5a572671ddffc: name=etcd1 peerURLs=http://192.168.3.148:2380 clientURLs=http://192.168.3.148:2379,http://192.168.3.148:4001 isLeader=<span class="literal">false</span></span><br><span class="line">    bf48e493e3fe022d: name=etcd2 peerURLs=http://192.168.3.149:2380 clientURLs=http://192.168.3.149:2379,http://192.168.3.149:4001 isLeader=<span class="literal">true</span></span><br><span class="line">    d9c99f531d70dd69: name=etcd3 peerURLs=http://192.168.3.150:2380 clientURLs=http://192.168.3.150:2379,http://192.168.3.150:4001 isLeader=<span class="literal">false</span></span><br></pre></td></tr></table></figure><h4 id="3-5：开始进行master初始化"><a href="#3-5：开始进行master初始化" class="headerlink" title="3.5：开始进行master初始化"></a>3.5：开始进行master初始化</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第一台master初始化</span></span><br><span class="line">kubeadm初始化：</span><br><span class="line">    kubeadm init --config=kubeadm-init.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#在所有master节点上设置kubectl客户端连接</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"export KUBECONFIG=/etc/kubernetes/admin.conf"</span> &gt;&gt; ~/.bashrc</span><br><span class="line">    <span class="built_in">source</span> ~/.bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装基础组件</span></span><br><span class="line"><span class="comment">#安装canal网络组件</span></span><br><span class="line">    kubectl apply -f kube-canal/</span><br><span class="line"></span><br><span class="line">    等待所有pods正常</span><br><span class="line">    kubectl get pods --all-namespaces -o wide</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装dashboard</span></span><br><span class="line">    kubectl apply -f kube-dashboard/</span><br><span class="line"></span><br><span class="line">    serviceaccount <span class="string">"admin-user"</span> created</span><br><span class="line">    clusterrolebinding <span class="string">"admin-user"</span> created</span><br><span class="line">    secret <span class="string">"kubernetes-dashboard-certs"</span> created</span><br><span class="line">    serviceaccount <span class="string">"kubernetes-dashboard"</span> created</span><br><span class="line">    role <span class="string">"kubernetes-dashboard-minimal"</span> created</span><br><span class="line">    rolebinding <span class="string">"kubernetes-dashboard-minimal"</span> created</span><br><span class="line">    deployment <span class="string">"kubernetes-dashboard"</span> created</span><br><span class="line">    service <span class="string">"kubernetes-dashboard"</span> created</span><br><span class="line"></span><br><span class="line">    通过浏览器访问dashboard地址</span><br><span class="line">    https://k8s-master1:30000/<span class="comment">#!/login</span></span><br><span class="line"></span><br><span class="line">    获取token，把token粘贴到login页面的token中，即可进入dashboard</span><br><span class="line">    kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="string">'&#123;print $1&#125;'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装heapster</span></span><br><span class="line">    kubectl apply -f kube-heapster/influxdb/</span><br><span class="line">    service <span class="string">"monitoring-grafana"</span> created</span><br><span class="line">    serviceaccount <span class="string">"heapster"</span> created</span><br><span class="line">    deployment <span class="string">"heapster"</span> created</span><br><span class="line">    service <span class="string">"heapster"</span> created</span><br><span class="line">    deployment <span class="string">"monitoring-influxdb"</span> created</span><br><span class="line">    service <span class="string">"monitoring-influxdb"</span> created</span><br><span class="line"></span><br><span class="line">    kubectl apply -f kube-heapster/rbac/</span><br><span class="line">    clusterrolebinding <span class="string">"heapster"</span> created</span><br><span class="line"></span><br><span class="line">    kubectl get pods --all-namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment">#master集群高可用设置</span></span><br><span class="line"><span class="comment">#复制配置</span></span><br><span class="line">    在k8s-master1上复制目录/etc/kubernetes/pki到k8s-master2、k8s-master3，从v1.9.x开始，kubeadm会检测pki目录是否有证书，如果已经存在证书则跳过证书生成的步骤</span><br><span class="line">    scp -r /etc/kubernetes/pki k8s-master2:/etc/kubernetes/</span><br><span class="line">    scp -r /etc/kubernetes/pki k8s-master3:/etc/kubernetes/</span><br><span class="line"></span><br><span class="line"><span class="comment">#其余master节点初始化</span></span><br><span class="line"><span class="comment">#在k8s-master2进行初始化，等待所有pods正常启动后再进行下一个master初始化，特别要保证kube-apiserver-&#123;current-node-name&#125;处于running状态</span></span><br><span class="line">    kubeadm init --config=kubeadm-init.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#在k8s-master3进行初始化，等待所有pods正常启动后再进行下一个master初始化，特别要保证kube-apiserver-&#123;current-node-name&#125;处于running状态</span></span><br><span class="line">    kubeadm init --config=kubeadm-init.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#dns支持多节点</span></span><br><span class="line">    kubectl scale --replicas=2 -n kube-system deployment/kube-dns</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查集群状态</span></span><br><span class="line">    kubectl get pods --all-namespaces -o wide </span><br><span class="line">    [root@k8s-master1 kubeadm-ha]<span class="comment"># kubectl get pods --all-namespaces</span></span><br><span class="line">    NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE</span><br><span class="line">    kube-system   canal-kcnfp                             3/3       Running   0          3h</span><br><span class="line">    kube-system   canal-lzzf8                             3/3       Running   17         2d</span><br><span class="line">    kube-system   canal-sf8l6                             3/3       Running   0          2d</span><br><span class="line">    kube-system   canal-trdqt                             3/3       Running   0          2d</span><br><span class="line">    kube-system   canal-znmxq                             3/3       Running   1          3h</span><br><span class="line">    kube-system   heapster-698c5f45bd-7xqc9               1/1       Running   0          5h</span><br><span class="line">    kube-system   kube-apiserver-k8s-master1              1/1       Running   13         4h</span><br><span class="line">    kube-system   kube-apiserver-k8s-master2              1/1       Running   0          5h</span><br><span class="line">    kube-system   kube-apiserver-k8s-master3              1/1       Running   0          2d</span><br><span class="line">    kube-system   kube-controller-manager-k8s-master1     1/1       Running   2          4h</span><br><span class="line">    kube-system   kube-controller-manager-k8s-master2     1/1       Running   0          5h</span><br><span class="line">    kube-system   kube-controller-manager-k8s-master3     1/1       Running   0          2d</span><br><span class="line">    kube-system   kube-dns-6f4fd4bdf-bjq2m                3/3       Running   0          5h</span><br><span class="line">    kube-system   kube-dns-6f4fd4bdf-bqxmc                3/3       Running   0          2d</span><br><span class="line">    kube-system   kube-proxy-5g4q7                        1/1       Running   0          4h</span><br><span class="line">    kube-system   kube-proxy-c2fwf                        1/1       Running   1          3h</span><br><span class="line">    kube-system   kube-proxy-j4fcd                        1/1       Running   0          4h</span><br><span class="line">    kube-system   kube-proxy-jqbgr                        1/1       Running   0          4h</span><br><span class="line">    kube-system   kube-proxy-wsqlr                        1/1       Running   1          3h</span><br><span class="line">    kube-system   kube-scheduler-k8s-master1              1/1       Running   1          4h</span><br><span class="line">    kube-system   kube-scheduler-k8s-master2              1/1       Running   0          5h</span><br><span class="line">    kube-system   kube-scheduler-k8s-master3              1/1       Running   0          2d</span><br><span class="line">    kube-system   kubernetes-dashboard-54cc6684f5-7t6nx   1/1       Running   0          5h</span><br><span class="line">    kube-system   monitoring-grafana-5ffb49ff84-cb7b5     1/1       Running   0          5h</span><br><span class="line">    kube-system   monitoring-influxdb-5b77d47fdd-lw76t    1/1       Running   0          5h</span><br><span class="line"></span><br><span class="line"><span class="comment">#keepalived配置,在master上重启keepalived</span></span><br><span class="line">    systemctl restart keepalived</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查虚拟IP是否正常</span></span><br><span class="line">    ping -c 2 192.168.3.157</span><br><span class="line"></span><br><span class="line"><span class="comment">#nginx负载均衡配置,在master上安装并启动nginx作为负载均衡</span></span><br><span class="line">docker-compose -f nginx-lb/docker-compose.yaml up -d</span><br><span class="line"></span><br><span class="line"><span class="comment">#在master上验证负载均衡和keepalived是否成功</span></span><br><span class="line">curl -k https://192.168.3.157:16443</span><br><span class="line"></span><br><span class="line"><span class="comment">#kube-proxy配置,在k8s-master1上设置proxy高可用，设置server指向高可用虚拟IP以及负载均衡的16443端口</span></span><br><span class="line">    kubectl edit -n kube-system configmap/kube-proxy</span><br><span class="line">    server: https://192.168.3.157:16443</span><br><span class="line"></span><br><span class="line"><span class="comment">#在master上重启proxy</span></span><br><span class="line">    kubectl delete pod -n kube-system $(kubectl get pods --all-namespaces -o wide | grep kube-proxy|awk <span class="string">'&#123;print $2&#125;'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#node节点加入高可用集群设置,kubeadm加入高可用集群</span></span><br><span class="line"><span class="comment">#在所有worker节点上进行加入kubernetes集群操作，这里统一使用k8s-master1的apiserver地址来加入集群</span></span><br><span class="line">    kubeadm join --token xxx 192.168.3.148:6443 --discovery-token-ca-cert-hash sha256:xxxx</span><br><span class="line"></span><br><span class="line"><span class="comment">#在所有worker节点上修改kubernetes集群设置，更改server为高可用虚拟IP以及负载均衡的16443端口</span></span><br><span class="line">    sed -i <span class="string">'/server:/s#https:.*#https://192.168.3.157:16443#g'</span> /etc/kubernetes/bootstrap-kubelet.conf</span><br><span class="line">    sed -i <span class="string">'/server:/s#https:.*#https://192.168.3.157:16443#g'</span> /etc/kubernetes/kubelet.conf</span><br><span class="line"></span><br><span class="line">    grep 192.168.3.157 /etc/kubernetes/*.conf</span><br><span class="line">    [root@k8s-node1 ~]<span class="comment"># grep 192.168.3.157 /etc/kubernetes/*.conf</span></span><br><span class="line">    /etc/kubernetes/bootstrap-kubelet.conf:    server: https://192.168.3.157:16443</span><br><span class="line">    /etc/kubernetes/kubelet.conf:    server: https://192.168.3.157:16443</span><br><span class="line"></span><br><span class="line">    systemctl restart docker kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置workers的节点标签</span></span><br><span class="line">    kubectl label nodes k8s-node1 role=worker</span><br><span class="line">    kubectl label nodes k8s-node2 role=worker</span><br><span class="line">    kubectl label nodes k8s-node3 role=worker</span><br></pre></td></tr></table></figure><h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a><em>4.总结</em></h3><p>总体部署下来步骤太多，后面写了一个部署脚本，只要写上节点IP就能一件部署，不过前期要做好资源本地化，就是要把准备好的工具，组件等放到内网，保证下载资源各方面都没问题，减少外部网络的影响。后面整理好再放到github上面。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-高可用集群概述&quot;&gt;&lt;a href=&quot;#1-高可用集群概述&quot; class=&quot;headerlink&quot; title=&quot;1.高可用集群概述&quot;&gt;&lt;/a&gt;&lt;em&gt;1.高可用集群概述&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;部署k8s的高可用集群，要做到无单电，主要是依靠负载均衡和k8s集群自身的高可用性。对于k8s的api，它本身是无状态的，自身不具备高可用，但是如果要它提供高可用的服务，做到其他节点和组件能无时无刻都可以访问它，需要在k8s api前面加一层tcp负载均衡器。一般k8s集群至少三个master节点，所以tcp转发层需要对这三台k8s api节点做负载均衡。这里有人可能问了，为什么需要tcp负载均衡，不做http负载均衡？原因就是k8s api的通信是https加密的，负载均衡器做https解析会比较麻烦，而且做tcp负载均衡效率也会高一些，也会比较方便，高版本的nginx就直接支持tcp转发。还有就是这三台master节点的服务器怎么放呢？这个要看个人的资源环境，比如你只是单机房的，那只能把master节点部署在一个机房里面了，然后三台服务器之间加个vip，使用keepalived做vip漂移，达到k8s api的高可用，如果你是同城三机房的，可以在每个机房部署一个master节点，然后每个机房部署一个高可用的负载均衡器，负载均衡器把请求转发到三个机房的k8s api上面去。然后对于etcd数据库，他本身自带高可用功能，部署三个以上节点的节点就好了，对于kube-scheduler组件，它们内部会自动选择一个leader节点，默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态，对于kube-controller-manager和kube-scheduler也是类似，默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态。对于k8s核心组件的功能，下面再说明一下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;kube-apiserver：&lt;/strong&gt;集群核心,集群API接口、集群各个组件通信的中枢；集群安全控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;etcd：&lt;/strong&gt;集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复；因此高可用集群部署首先就是etcd是高可用集群；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;kube-scheduler：&lt;/strong&gt;集群Pod的调度中心；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;kube-controller-manager：&lt;/strong&gt;集群状态管理器，当集群状态与期望不同时，kcm会努力让集群恢复期望状态，比如：当一个pod死掉，kcm会努力新建一个pod来恢复对应replicas set期望的状态；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;kubelet：&lt;/strong&gt;kubernetes node agent，负责与node上的docker engine打交道；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;kube-proxy：&lt;/strong&gt;每个node上一个，负责service vip到endpoint pod的流量转发，当前主要通过设置iptables规则实现。&lt;/p&gt;
&lt;h3 id=&quot;2-k8s高可用架构图&quot;&gt;&lt;a href=&quot;#2-k8s高可用架构图&quot; class=&quot;headerlink&quot; title=&quot;2.k8s高可用架构图&quot;&gt;&lt;/a&gt;&lt;em&gt;2.k8s高可用架构图&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;这里单机房和多机房的区别就在于单机房整个机房就一个vip，多机房就是每个机房都有一个vip，每个机房都有一个api访问入口。&lt;br&gt;&lt;img src=&quot;https://cloud.qingye.info/images/20200325/k8s-ha.png&quot; alt=&quot;Kubernetes 高可用架构图&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[4.k8s资源对象]</title>
    <link href="http://qingye.info/2020/03/25/k8s/K8S-4-Resource-object/"/>
    <id>http://qingye.info/2020/03/25/k8s/K8S-4-Resource-object/</id>
    <published>2020-03-24T16:39:07.000Z</published>
    <updated>2020-04-16T13:13:22.428Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、API对象资源"><a href="#一、API对象资源" class="headerlink" title="一、API对象资源"></a>一、API对象资源</h3><p>API对象是K8s集群中的管理操作单元。K8s集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set对应的API对象是RS。</p><p>每个API对象都有3大类属性：元数据metadata、规范spec和状态status。元数据是用来标识API对象的，每个对象都至少有3个元数据：namespace，name和uid；除此以外还有各种各样的标签labels用来标识和匹配不同的对象，例如用户可以用标签env来标识区分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、测试、生产的不同服务。规范描述了用户期望K8s集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器Replication Controller设置期望的Pod副本数为3；status描述了系统实际当前达到的状态（Status），例如系统当前实际的Pod副本数为2；那么复本控制器当前的程序逻辑就是自动启动新的Pod，争取达到副本数为3。</p><p>K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果，而给副本数加1的操作就不是声明式的，运行多次结果就错了。</p><h4 id="1-Pod"><a href="#1-Pod" class="headerlink" title="1.Pod"></a>1.Pod</h4><p>K8s有很多技术概念，同时对应很多API对象，最重要的也是最基础的是微服务Pod。Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod对多容器的支持是K8s最基础的设计理念。比如你运行一个操作系统发行版的软件仓库，一个Nginx容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像不太可能是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。</p><p>Pod是K8s集群中所有业务类型的基础，可以看作运行在K8s集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为Deployment、Job、DaemonSet和StatefulSet，本文后面会一一介绍。</p><h4 id="2-复制控制器（Replication-Controller，RC）"><a href="#2-复制控制器（Replication-Controller，RC）" class="headerlink" title="2.复制控制器（Replication Controller，RC）"></a>2.复制控制器（Replication Controller，RC）</h4><p>RC是K8s集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有1个Pod在运行。RC是K8s较早期的技术概念，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。</p><a id="more"></a><h4 id="3-副本集（Replica-Set，RS）"><a href="#3-副本集（Replica-Set，RS）" class="headerlink" title="3.副本集（Replica Set，RS）"></a>3.副本集（Replica Set，RS）</h4><p>RS是新一代RC，提供同样的高可用能力，区别主要在于RS后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数使用。</p><h4 id="4-部署-Deployment"><a href="#4-部署-Deployment" class="headerlink" title="4.部署(Deployment)"></a>4.部署(Deployment)</h4><p>部署表示用户对K8s集群的一次更新操作。部署是一个比RS应用模式更广的API对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新RS中副本数增加到理想状态，将旧RS中的副本数减小到0的复合操作；这样一个复合操作用一个RS是不太好描述的，所以用一个更通用的Deployment来描述。以K8s的发展方向，未来对所有长期伺服型的的业务的管理，都会通过Deployment来管理。</p><h4 id="5-服务（Service）"><a href="#5-服务（Service）" class="headerlink" title="5.服务（Service）"></a>5.服务（Service）</h4><p>RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在K8s集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。在K8s集群中微服务的负载均衡是由Kube-proxy实现的。Kube-proxy是K8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8s的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端使用反向代理作负载均衡，还要进一步解决反向代理的高可用问题。</p><h4 id="6-任务（Job）"><a href="#6-任务（Job）" class="headerlink" title="6.任务（Job）"></a>6.任务（Job）</h4><p>Job是K8s用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。</p><h4 id="7-后台支撑服务集（DaemonSet）"><a href="#7-后台支撑服务集（DaemonSet）" class="headerlink" title="7.后台支撑服务集（DaemonSet）"></a>7.后台支撑服务集（DaemonSet）</h4><p>长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的Pod，有些节点上又没有这类Pod运行；而后台支撑型服务的核心关注点在K8s集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类Pod运行。节点可能是所有集群节点也可能是通过nodeSelector选定的一些特定节点。典型的后台支撑型服务包括，存储，日志和监控等在每个节点上支撑K8s集群运行的服务。</p><h4 id="8-有状态服务集（StatefulSet）"><a href="#8-有状态服务集（StatefulSet）" class="headerlink" title="8.有状态服务集（StatefulSet）"></a>8.有状态服务集（StatefulSet）</h4><p>K8s在1.3版本里发布了Alpha版的PetSet以支持有状态服务，并从1.5版本开始重命名为StatefulSet。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数；而StatefulSet是用来控制有状态服务，StatefulSet中的每个Pod的名字都是事先确定的，不能更改。StatefulSet中Pod的名字的作用，并不是《千与千寻》的人性原因，而是关联与该Pod对应的状态。</p><p>对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别（这似乎也确实意味着失去了人性特征）；对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。</p><p>适合于StatefulSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。StatefulSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用StatefulSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，StatefulSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。StatefulSet还只在Alpha阶段，后面的设计如何演变，我们还要继续观察。</p><h4 id="9-集群联邦（Federation）"><a href="#9-集群联邦（Federation）" class="headerlink" title="9.集群联邦（Federation）"></a>9.集群联邦（Federation）</h4><p>K8s在1.3版本里发布了beta版的Federation功能。在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。K8s的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足K8s的调度和计算存储连接要求。而联合集群服务就是为提供跨Region跨服务商K8s集群服务而设计的。</p><p>每个K8s Federation有自己的分布式存储、API Server和Controller Manager。用户可以通过Federation的API Server注册该Federation的成员K8s Cluster。当用户通过Federation的API Server创建、更改API对象时，Federation API Server会在自己所有注册的子K8s Cluster都创建一份对应的API对象。在提供业务请求服务时，K8s Federation会先在自己的各个子Cluster之间做负载均衡，而对于发送到某个具体K8s Cluster的业务请求，会依照这个K8s Cluster独立提供服务时一样的调度模式去做K8s Cluster内部的负载均衡。而Cluster之间的负载均衡是通过域名服务的负载均衡来实现的。</p><p>所有的设计都尽量不影响K8s Cluster现有的工作机制，这样对于每个子K8s集群来说，并不需要更外层的有一个K8s Federation，也就是意味着所有现有的K8s代码和机制不需要因为Federation功能有任何变化。</p><h4 id="10-存储卷（Volume）"><a href="#10-存储卷（Volume）" class="headerlink" title="10.存储卷（Volume）"></a>10.存储卷（Volume）</h4><p>K8s集群中的存储卷跟Docker的存储卷有些类似，只不过Docker的存储卷作用范围为一个容器，而K8s的存储卷的生命周期和作用范围是一个Pod。每个Pod中声明的存储卷由Pod中的所有容器共享。K8s支持非常多的存储卷类型，特别的，支持多种公有云平台的存储，包括AWS，Google和Azure云；支持多种分布式存储包括GlusterFS和Ceph；也支持较容易使用的主机本地目录hostPath和NFS。K8s还支持使用Persistent Volume Claim即PVC这种逻辑存储，使用这种存储，使得存储的使用者可以忽略后台的实际存储技术（例如AWS，Google或GlusterFS和Ceph），而将有关存储实际技术的配置交给存储管理员通过Persistent Volume来配置。</p><h4 id="11-持久存储卷（Persistent-Volume，PV）和持久存储卷声明（Persistent-Volume-Claim，PVC）"><a href="#11-持久存储卷（Persistent-Volume，PV）和持久存储卷声明（Persistent-Volume-Claim，PVC）" class="headerlink" title="11.持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC）"></a>11.持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC）</h4><p>PV和PVC使得K8s集群具备了存储的逻辑抽象能力，使得在配置Pod的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。存储的PV和PVC的这种关系，跟计算的Node和Pod的关系是非常类似的；PV和Node是资源的提供者，根据集群的基础设施变化而变化，由K8s集群管理员配置；而PVC和Pod是资源的使用者，根据业务服务的需求变化而变化，由K8s集群的使用者即服务的管理员来配置。</p><h4 id="12-节点（Node）"><a href="#12-节点（Node）" class="headerlink" title="12.节点（Node）"></a>12.节点（Node）</h4><p>K8s集群中的计算能力由Node提供，最初Node称为服务节点Minion，后来改名为Node。K8s集群中的Node也就等同于Mesos集群中的Slave节点，是所有Pod运行所在的工作主机，可以是物理机也可以是虚拟机。不论是物理机还是虚拟机，工作主机的统一特征是上面要运行kubelet管理节点上运行的容器。</p><h4 id="13-密钥对象（Secret）"><a href="#13-密钥对象（Secret）" class="headerlink" title="13.密钥对象（Secret）"></a>13.密钥对象（Secret）</h4><p>Secret是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。使用Secret的好处是可以避免把敏感信息明文写在配置文件里。在K8s集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问AWS存储的用户名密码。为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个Secret对象，而在配置文件中通过Secret对象引用这些敏感信息。这种方式的好处包括：意图明确，避免重复，减少暴露机会。</p><h4 id="14-用户帐户（User-Account）和服务帐户（Service-Account）"><a href="#14-用户帐户（User-Account）和服务帐户（Service-Account）" class="headerlink" title="14.用户帐户（User Account）和服务帐户（Service Account）"></a>14.用户帐户（User Account）和服务帐户（Service Account）</h4><p>顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和K8s集群中运行的Pod提供账户标识。用户帐户和服务帐户的一个区别是作用范围；用户帐户对应的是人的身份，人的身份与服务的namespace无关，所以用户账户是跨namespace的；而服务帐户对应的是一个运行中程序的身份，与特定namespace是相关的。</p><h4 id="15-名字空间（Namespace）"><a href="#15-名字空间（Namespace）" class="headerlink" title="15.名字空间（Namespace）"></a>15.名字空间（Namespace）</h4><p>名字空间为K8s集群提供虚拟的隔离作用，K8s集群初始有两个名字空间，分别是默认名字空间default和系统名字空间kube-system，除此以外，管理员可以创建新的名字空间满足需要。</p><h4 id="16-RBAC访问授权"><a href="#16-RBAC访问授权" class="headerlink" title="16.RBAC访问授权"></a>16.RBAC访问授权</h4><p>K8s在1.3版本中发布了alpha版的基于角色的访问控制（Role-based Access Control，RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control，ABAC），RBAC主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。在ABAC中，K8s集群中的访问策略只能跟用户直接关联；而在RBAC中，访问策略可以跟某个角色关联，具体的用户在跟一个或多个角色相关联。显然，RBAC像其他新功能一样，每次引入新功能，都会引入新的API对象，从而引入新的概念抽象，而这一新的概念抽象一定会使集群服务管理和使用更容易扩展和重用。</p><h3 id="二、总结"><a href="#二、总结" class="headerlink" title="二、总结"></a>二、总结</h3><p>从K8s的系统架构、技术概念和设计理念，我们可以看到K8s系统最核心的两个设计理念：一个是<strong>容错性</strong>，一个是<strong>易扩展性</strong>。容错性实际是保证K8s系统稳定性和安全性的基础，易扩展性是保证K8s对变更友好，可以快速迭代增加新功能的基础。</p><h3 id="三、参考文档"><a href="#三、参考文档" class="headerlink" title="三、参考文档"></a>三、参考文档</h3><ul><li><a href="https://github.com/feiskyer/kubernetes-handbook/edit/master/architecture/concepts.md">https://github.com/feiskyer/kubernetes-handbook/edit/master/architecture/concepts.md</a></li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/principles.md">Kubernetes Design Principles</a></li><li><a href="http://www.infoq.com/cn/articles/kubernetes-and-cloud-native-applications-part01">Kubernetes与云原生应用</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;一、API对象资源&quot;&gt;&lt;a href=&quot;#一、API对象资源&quot; class=&quot;headerlink&quot; title=&quot;一、API对象资源&quot;&gt;&lt;/a&gt;一、API对象资源&lt;/h3&gt;&lt;p&gt;API对象是K8s集群中的管理操作单元。K8s集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set对应的API对象是RS。&lt;/p&gt;
&lt;p&gt;每个API对象都有3大类属性：元数据metadata、规范spec和状态status。元数据是用来标识API对象的，每个对象都至少有3个元数据：namespace，name和uid；除此以外还有各种各样的标签labels用来标识和匹配不同的对象，例如用户可以用标签env来标识区分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、测试、生产的不同服务。规范描述了用户期望K8s集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器Replication Controller设置期望的Pod副本数为3；status描述了系统实际当前达到的状态（Status），例如系统当前实际的Pod副本数为2；那么复本控制器当前的程序逻辑就是自动启动新的Pod，争取达到副本数为3。&lt;/p&gt;
&lt;p&gt;K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果，而给副本数加1的操作就不是声明式的，运行多次结果就错了。&lt;/p&gt;
&lt;h4 id=&quot;1-Pod&quot;&gt;&lt;a href=&quot;#1-Pod&quot; class=&quot;headerlink&quot; title=&quot;1.Pod&quot;&gt;&lt;/a&gt;1.Pod&lt;/h4&gt;&lt;p&gt;K8s有很多技术概念，同时对应很多API对象，最重要的也是最基础的是微服务Pod。Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod对多容器的支持是K8s最基础的设计理念。比如你运行一个操作系统发行版的软件仓库，一个Nginx容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像不太可能是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。&lt;/p&gt;
&lt;p&gt;Pod是K8s集群中所有业务类型的基础，可以看作运行在K8s集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为Deployment、Job、DaemonSet和StatefulSet，本文后面会一一介绍。&lt;/p&gt;
&lt;h4 id=&quot;2-复制控制器（Replication-Controller，RC）&quot;&gt;&lt;a href=&quot;#2-复制控制器（Replication-Controller，RC）&quot; class=&quot;headerlink&quot; title=&quot;2.复制控制器（Replication Controller，RC）&quot;&gt;&lt;/a&gt;2.复制控制器（Replication Controller，RC）&lt;/h4&gt;&lt;p&gt;RC是K8s集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有1个Pod在运行。RC是K8s较早期的技术概念，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。&lt;/p&gt;
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[3.k8s基础组件]</title>
    <link href="http://qingye.info/2020/03/24/k8s/K8S-3-Basic-components/"/>
    <id>http://qingye.info/2020/03/24/k8s/K8S-3-Basic-components/</id>
    <published>2020-03-24T15:55:07.000Z</published>
    <updated>2020-04-16T13:08:14.660Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-核心组件"><a href="#1-核心组件" class="headerlink" title="1.核心组件"></a><em>1.核心组件</em></h3><p>1.1 etcd 保存了整个集群的状态,etcd是Kubernetes提供默认的存储系统，保存所有集群数据，使用时需要为etcd数据提供备份计划</p><p>1.2 kube-apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制</p><p>1.3 kube-controller-manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等,kube-controller-manager运行管理控制器，它们是集群中处理常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成单个二进制文件，并在单个进程中运行.这些控制器包括：</p><ul><li>节点（Node）控制器</li><li>副本（Replication）控制器：负责维护系统中每个副本中的pod</li><li>端点（Endpoints）控制器：填充Endpoints对象（即连接Services＆Pods）</li><li>Service Account和Token控制器：为新的Namespace 创建默认帐户访问API Token</li></ul><p>1.4 kube-scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上</p><p>1.5 kubelet 负责维持容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理</p><p>1.6 Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI），默认的容器运行时为 Docker.</p><p>1.7 kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡.</p><p>1.8 docker服务，用于运行容器.</p><a id="more"></a><p><img src="https://cloud.qingye.info/images/20200324/components.png" alt="Kubernetes 组件工作图示"></p><h3 id="2-常用插件"><a href="#2-常用插件" class="headerlink" title="2.常用插件"></a><em>2.常用插件</em></h3><p>除了核心组件，还有一些推荐的 Add-ons：</p><ul><li>coredns 负责为整个集群提供 DNS 服务</li><li>CNI网络插件，常用calico，Flannel</li><li>Ingress Controller 为服务提供外网入口</li><li>Heapster 提供资源监控</li><li>Dashboard 提供 GUI</li><li>Federation 提供跨可用区的集群</li><li>Fluentd-elasticsearch 提供集群日志采集、存储与查询</li></ul><h3 id="3-组件高可用性"><a href="#3-组件高可用性" class="headerlink" title="3.组件高可用性"></a><em>3.组件高可用性</em></h3><p>3.1 k8s 高可用2个核心 apiserver master and etcd</p><p>3.2 apiserver master：（需高可用）集群核心，集群API接口、集群各个组件通信的中枢；集群安全控制；</p><p>3.3 etcd ：（需高可用）集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复；因此高可用集群部署首先就是etcd是高可用集群；</p><p>3.4 kube-scheduler：调度器 （内部自选举）集群Pod的调度中心；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态；</p><p>3.5 kube-controller-manager： 控制器 （内部自选举）集群状态管理器，当集群状态与期望不同时，kcm会努力让集群恢复期望状态，比如：当一个pod死掉，kcm会努力新建一个pod来恢复对应replicas set期望的状态；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态；</p><p>3.6 kubelet: agent node注册apiserver</p><p>3.7 kube-proxy: 每个node上一个，负责service vip到endpoint pod的流量转发，老版本主要通过设置iptables规则实现，新版1.9基于kube-proxy-lvs 实现</p><h3 id="4-参考文章"><a href="#4-参考文章" class="headerlink" title="4.参考文章"></a><em>4.参考文章</em></h3><ul><li><a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md">https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md</a></li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/principles.md">Kubernetes Design Principles</a></li><li><a href="http://www.infoq.com/cn/articles/kubernetes-and-cloud-native-applications-part01">Kubernetes与云原生应用</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-核心组件&quot;&gt;&lt;a href=&quot;#1-核心组件&quot; class=&quot;headerlink&quot; title=&quot;1.核心组件&quot;&gt;&lt;/a&gt;&lt;em&gt;1.核心组件&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;1.1 etcd 保存了整个集群的状态,etcd是Kubernetes提供默认的存储系统，保存所有集群数据，使用时需要为etcd数据提供备份计划&lt;/p&gt;
&lt;p&gt;1.2 kube-apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制&lt;/p&gt;
&lt;p&gt;1.3 kube-controller-manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等,kube-controller-manager运行管理控制器，它们是集群中处理常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成单个二进制文件，并在单个进程中运行.这些控制器包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点（Node）控制器&lt;/li&gt;
&lt;li&gt;副本（Replication）控制器：负责维护系统中每个副本中的pod&lt;/li&gt;
&lt;li&gt;端点（Endpoints）控制器：填充Endpoints对象（即连接Services＆Pods）&lt;/li&gt;
&lt;li&gt;Service Account和Token控制器：为新的Namespace 创建默认帐户访问API Token&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1.4 kube-scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上&lt;/p&gt;
&lt;p&gt;1.5 kubelet 负责维持容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理&lt;/p&gt;
&lt;p&gt;1.6 Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI），默认的容器运行时为 Docker.&lt;/p&gt;
&lt;p&gt;1.7 kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡.&lt;/p&gt;
&lt;p&gt;1.8 docker服务，用于运行容器.&lt;/p&gt;
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[2.k8s设计原则]</title>
    <link href="http://qingye.info/2020/03/24/k8s/K8S-2-Design-Principles/"/>
    <id>http://qingye.info/2020/03/24/k8s/K8S-2-Design-Principles/</id>
    <published>2020-03-24T15:36:07.000Z</published>
    <updated>2020-04-16T13:09:22.691Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-API设计原则"><a href="#1-API设计原则" class="headerlink" title="1.API设计原则"></a><em>1.API设计原则</em></h3><p>对于云计算系统，系统API实际上处于系统设计的统领地位。Kubernetes集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。理解掌握的API，就好比抓住了K8s系统的牛鼻子。Kubernetes系统API的设计有以下几条原则：</p><ul><li>所有API应该是声明式的。声明式的操作，相对于命令式操作，对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。另外，声明式操作更容易被用户使用，可以使系统向用户隐藏实现的细节，同时也保留了系统未来持续优化的可能性。此外，声明式的API还隐含了所有的API对象都是名词性质的，例如Service、Volume这些API都是名词，这些名词描述了用户所期望得到的一个目标对象。</li><li>API对象是彼此互补而且可组合的。这实际上鼓励API对象尽量实现面向对象设计时的要求，即“高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。</li><li>高层API以操作意图为基础设计。如何能够设计好API，跟如何能用面向对象的方法设计好应用系统有相通的地方，高层设计一定是从业务出发，而不是过早的从技术实现出发。因此，针对Kubernetes的高层API设计，一定是以K8s的业务为基础出发，也就是以系统调度管理容器的操作意图为基础设计。</li><li>低层API根据高层API的控制需要设计。设计实现低层API的目的，是为了被高层API使用，考虑减少冗余、提高重用性的目的，低层API的设计也要以需求为基础，要尽量抵抗受技术实现影响的诱惑。</li><li>尽量避免简单封装，不要有在外部API无法显式知道的内部隐藏的机制。简单的封装，实际没有提供新的功能，反而增加了对所封装API的依赖性。内部隐藏的机制也是非常不利于系统维护的设计方式，例如StatefulSet和ReplicaSet，本来就是两种Pod集合，那么Kubernetes就用不同API对象来定义它们，而不会说只用同一个ReplicaSet，内部通过特殊的算法再来区分这个ReplicaSet是有状态的还是无状态。</li><li>API操作复杂度与对象数量成正比。这一条主要是从系统性能角度考虑，要保证整个系统随着系统规模的扩大，性能不会迅速变慢到无法使用，那么最低的限定就是API的操作复杂度不能超过O(N)，N是对象的数量，否则系统就不具备水平伸缩性了。</li><li>API对象状态不能依赖于网络连接状态。由于众所周知，在分布式环境下，网络连接断开是经常发生的事情，因此要保证API对象状态能应对网络的不稳定，API对象的状态就不能依赖于网络连接状态。<br>尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的。<a id="more"></a></li></ul><p>Kubernetes 核心API图示:<br><img src="https://cloud.qingye.info/images/20200324/core-apis.png" alt="Kubernetes 核心API"></p><h3 id="2-控制机制设计原则"><a href="#2-控制机制设计原则" class="headerlink" title="2.控制机制设计原则"></a><em>2.控制机制设计原则</em></h3><ul><li><strong>控制逻辑应该只依赖于当前状态</strong>。这是为了保证分布式系统的稳定可靠，对于经常出现局部错误的分布式系统，如果控制逻辑只依赖当前状态，那么就非常容易将一个暂时出现故障的系统恢复到正常状态，因为你只要将该系统重置到某个稳定状态，就可以自信的知道系统的所有控制逻辑会开始按照正常方式运行。 </li><li><strong>假设任何错误的可能，并做容错处理</strong>。在一个分布式系统中出现局部和临时错误是大概率事件。错误可能来自于物理系统故障，外部系统故障也可能来自于系统自身的代码错误，依靠自己实现的代码不会出错来保证系统稳定其实也是难以实现的，因此要设计对任何可能错误的容错处理。 </li><li><strong>尽量避免复杂状态机，控制逻辑不要依赖无法监控的内部状态</strong>。因为分布式系统各个子系统都是不能严格通过程序内部保持同步的，所以如果两个子系统的控制逻辑如果互相有影响，那么子系统就一定要能互相访问到影响控制逻辑的状态，否则，就等同于系统里存在不确定的控制逻辑。 </li><li><strong>假设任何操作都可能被任何操作对象拒绝，甚至被错误解析</strong>。由于分布式系统的复杂性以及各子系统的相对独立性，不同子系统经常来自不同的开发团队，所以不能奢望任何操作被另一个子系统以正确的方式处理，要保证出现错误的时候，操作级别的错误不会影响到系统稳定性。 </li><li><strong>每个模块都可以在出错后自动恢复</strong>。由于分布式系统中无法保证系统各个模块是始终连接的，因此每个模块要有自我修复的能力，保证不会因为连接不到其他模块而自我崩溃。 </li><li><strong>每个模块都可以在必要时优雅地降级服务</strong>。所谓优雅地降级服务，是对系统鲁棒性的要求，即要求在设计实现模块时划分清楚基本功能和高级功能，保证基本功能不会依赖高级功能，这样同时就保证了不会因为高级功能出现故障而导致整个模块崩溃。根据这种理念实现的系统，也更容易快速地增加新的高级功能，因为不必担心引入高级功能影响原有的基本功能。</li></ul><h3 id="3-架构设计原则"><a href="#3-架构设计原则" class="headerlink" title="3.架构设计原则"></a><em>3.架构设计原则</em></h3><ul><li><a href="http://issue.k8s.io/246">Self-hosting</a> 是目标</li><li>减少依赖，特别是稳态运行的依赖</li><li>通过分层的原则管理依赖</li><li>循环依赖问题的原则<ul><li>同时还接受其他方式的数据输入（比如本地文件等），这样在其他服务不可用时还可以手动配置引导服务</li><li>状态应该是可恢复或可重新发现的</li><li>支持简单的启动临时实例来创建稳态运行所需要的状态；使用分布式锁或文件锁等来协调不同状态的切换（通常称为<code>pivoting</code>技术）</li><li>自动重启异常退出的服务，比如副本或者进程管理器等</li></ul></li></ul><h3 id="4-参考文章"><a href="#4-参考文章" class="headerlink" title="4.参考文章"></a><em>4.参考文章</em></h3><ul><li><a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md">https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md</a></li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/principles.md">Kubernetes Design Principles</a></li><li><a href="http://www.infoq.com/cn/articles/kubernetes-and-cloud-native-applications-part01">Kubernetes与云原生应用</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-API设计原则&quot;&gt;&lt;a href=&quot;#1-API设计原则&quot; class=&quot;headerlink&quot; title=&quot;1.API设计原则&quot;&gt;&lt;/a&gt;&lt;em&gt;1.API设计原则&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;对于云计算系统，系统API实际上处于系统设计的统领地位。Kubernetes集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。理解掌握的API，就好比抓住了K8s系统的牛鼻子。Kubernetes系统API的设计有以下几条原则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有API应该是声明式的。声明式的操作，相对于命令式操作，对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。另外，声明式操作更容易被用户使用，可以使系统向用户隐藏实现的细节，同时也保留了系统未来持续优化的可能性。此外，声明式的API还隐含了所有的API对象都是名词性质的，例如Service、Volume这些API都是名词，这些名词描述了用户所期望得到的一个目标对象。&lt;/li&gt;
&lt;li&gt;API对象是彼此互补而且可组合的。这实际上鼓励API对象尽量实现面向对象设计时的要求，即“高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。&lt;/li&gt;
&lt;li&gt;高层API以操作意图为基础设计。如何能够设计好API，跟如何能用面向对象的方法设计好应用系统有相通的地方，高层设计一定是从业务出发，而不是过早的从技术实现出发。因此，针对Kubernetes的高层API设计，一定是以K8s的业务为基础出发，也就是以系统调度管理容器的操作意图为基础设计。&lt;/li&gt;
&lt;li&gt;低层API根据高层API的控制需要设计。设计实现低层API的目的，是为了被高层API使用，考虑减少冗余、提高重用性的目的，低层API的设计也要以需求为基础，要尽量抵抗受技术实现影响的诱惑。&lt;/li&gt;
&lt;li&gt;尽量避免简单封装，不要有在外部API无法显式知道的内部隐藏的机制。简单的封装，实际没有提供新的功能，反而增加了对所封装API的依赖性。内部隐藏的机制也是非常不利于系统维护的设计方式，例如StatefulSet和ReplicaSet，本来就是两种Pod集合，那么Kubernetes就用不同API对象来定义它们，而不会说只用同一个ReplicaSet，内部通过特殊的算法再来区分这个ReplicaSet是有状态的还是无状态。&lt;/li&gt;
&lt;li&gt;API操作复杂度与对象数量成正比。这一条主要是从系统性能角度考虑，要保证整个系统随着系统规模的扩大，性能不会迅速变慢到无法使用，那么最低的限定就是API的操作复杂度不能超过O(N)，N是对象的数量，否则系统就不具备水平伸缩性了。&lt;/li&gt;
&lt;li&gt;API对象状态不能依赖于网络连接状态。由于众所周知，在分布式环境下，网络连接断开是经常发生的事情，因此要保证API对象状态能应对网络的不稳定，API对象的状态就不能依赖于网络连接状态。&lt;br&gt;尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的。
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>k8s专题[1.k8s基础概念]</title>
    <link href="http://qingye.info/2020/03/22/k8s/K8S-1-Basic-concepts/"/>
    <id>http://qingye.info/2020/03/22/k8s/K8S-1-Basic-concepts/</id>
    <published>2020-03-22T05:07:07.000Z</published>
    <updated>2020-04-16T13:09:14.870Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a><em>1.概念</em></h3><p>Kubernetes是一个容器编排系统，也就是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。 Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。 Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。</p><h3 id="2-特点"><a href="#2-特点" class="headerlink" title="2.特点"></a><em>2.特点</em></h3><ul><li><p>可移植: 支持公有云，私有云，混合云，多重云（multi-cloud）</p></li><li><p>可扩展: 模块化, 插件化, 可挂载, 可组合</p></li><li><p>自动化: 自动部署，自动重启，自动复制，自动伸缩/扩展</p></li></ul><h3 id="3-Kubernetes-架构"><a href="#3-Kubernetes-架构" class="headerlink" title="3.Kubernetes 架构"></a><em>3.Kubernetes 架构</em></h3><a id="more"></a><p><img src="https://cloud.qingye.info/images/20200322/architecture.png" alt="Kubernetes 架构图"></p><h3 id="4-分层架构"><a href="#4-分层架构" class="headerlink" title="4.分层架构"></a><em>4.分层架构</em></h3><p>Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示：</p><p><img src="https://cloud.qingye.info/images/20200322/14937095836427.jpg" alt="Kubernetes 分层架构"></p><ul><li>核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境</li><li>应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等）</li><li>管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等）</li><li>接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦</li><li>生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴<br>Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS 应用、ChatOps 等<br>Kubernetes 内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等</li></ul><h3 id="5-生态系统"><a href="#5-生态系统" class="headerlink" title="5.生态系统"></a><em>5.生态系统</em></h3><p><img src="https://cloud.qingye.info/images/20200322/core-ecosystem.png" alt="Kubernetes 生态"></p><h3 id="6-参考文章"><a href="#6-参考文章" class="headerlink" title="6.参考文章"></a><em>6.参考文章</em></h3><ul><li><a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md">https://github.com/feiskyer/kubernetes-handbook/blob/master/architecture/architecture.md</a></li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md</a></li><li><a href="http://queue.acm.org/detail.cfm?id=2898444">http://queue.acm.org/detail.cfm?id=2898444</a></li><li><a href="http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf">http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf</a></li><li><a href="http://thenewstack.io/kubernetes-an-overview">http://thenewstack.io/kubernetes-an-overview</a></li><li><a href="https://github.com/kubernetes/community/tree/master/sig-architecture">https://github.com/kubernetes/community/tree/master/sig-architecture</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;headerlink&quot; title=&quot;1.概念&quot;&gt;&lt;/a&gt;&lt;em&gt;1.概念&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;Kubernetes是一个容器编排系统，也就是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。 Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。 Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。&lt;/p&gt;
&lt;h3 id=&quot;2-特点&quot;&gt;&lt;a href=&quot;#2-特点&quot; class=&quot;headerlink&quot; title=&quot;2.特点&quot;&gt;&lt;/a&gt;&lt;em&gt;2.特点&lt;/em&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可移植: 支持公有云，私有云，混合云，多重云（multi-cloud）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;可扩展: 模块化, 插件化, 可挂载, 可组合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;自动化: 自动部署，自动重启，自动复制，自动伸缩/扩展&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;3-Kubernetes-架构&quot;&gt;&lt;a href=&quot;#3-Kubernetes-架构&quot; class=&quot;headerlink&quot; title=&quot;3.Kubernetes 架构&quot;&gt;&lt;/a&gt;&lt;em&gt;3.Kubernetes 架构&lt;/em&gt;&lt;/h3&gt;
    
    </summary>
    
    
      <category term="k8s专题" scheme="http://qingye.info/categories/k8s%E4%B8%93%E9%A2%98/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>k8s集群更新证书</title>
    <link href="http://qingye.info/2020/03/21/k8s/K8S-UPDATE-CERTS/"/>
    <id>http://qingye.info/2020/03/21/k8s/K8S-UPDATE-CERTS/</id>
    <published>2020-03-21T04:46:07.000Z</published>
    <updated>2020-04-16T13:25:18.245Z</updated>
    
    <content type="html"><![CDATA[<p>(版权声明：本文转载博客园「aguncn」的文章。<a href="https://www.cnblogs.com/aguncn/p/10488130.html">原文链接</a>)</p><h3 id="1-问题起源"><a href="#1-问题起源" class="headerlink" title="1.问题起源"></a><em>1.问题起源</em></h3><p>kubeadm 是 kubernetes 提供的一个初始化集群的工具，使用起来非常方便。但是它创建的apiserver、controller-manager等证书默认只有一年的有效期，同时kubelet 证书也只有一年有效期，一年之后 kubernetes 将停止服务。<br>官方推荐一年之内至少用 kubeadm upgrade 更新一次 kubernetes 系统，更新时也会自动更新证书。不过，在产线环境或者无法连接外网的环境频繁更新 kubernetes 不太现实。我们可以在过期之前或之后，使用kubeadm alpha phase里的certs和kubeconfig命令，同时配合kubelet证书自动轮换机制来解决这个问题。</p><h3 id="2-k8s里面的证书详解"><a href="#2-k8s里面的证书详解" class="headerlink" title="2.k8s里面的证书详解"></a><em>2.k8s里面的证书详解</em></h3><ul><li>Kubernetes 集群根证书</li></ul><p>/etc/kubernetes/pki/ca.crt</p><p>/etc/kubernetes/pki/ca.key</p><ul><li>由此根证书签发的证书有:</li></ul><p>1.kube-apiserver 组件持有的服务端证书</p><p>/etc/kubernetes/pki/apiserver.crt</p><p>/etc/kubernetes/pki/apiserver.key</p><p>2.kubelet 组件持有的客户端证书,kubelet 上一般不会明确指定服务端证书, 而是只指定 ca 根证书, 让 kubelet 根据本地主机信息自动生成服务端证书并保存到配置的cert-dir文件夹中。</p><p>/etc/kubernetes/pki/apiserver-kubelet-client.crt</p><p>/etc/kubernetes/pki/apiserver-kubelet-client.key</p><ul><li>汇聚层(aggregator)证书<a id="more"></a></li></ul><p>/etc/kubernetes/pki/front-proxy-ca.crt</p><p>/etc/kubernetes/pki/front-proxy-ca.key</p><p>由此根证书签发的证书只有一组:</p><p>1.代理端使用的客户端证书, 用作代用户与 kube-apiserver 认证</p><p>/etc/kubernetes/pki/front-proxy-client.crt</p><p>/etc/kubernetes/pki/front-proxy-client.key</p><ul><li>etcd 集群根证书</li></ul><p>/etc/kubernetes/pki/etcd/ca.crt</p><p>/etc/kubernetes/pki/etcd/ca.key</p><p>由此根证书签发机构签发的证书有:</p><p>1.etcd server 持有的服务端证书</p><p>/etc/kubernetes/pki/etcd/server.crt</p><p>/etc/kubernetes/pki/etcd/server.key</p><p>2.peer 集群中节点互相通信使用的客户端证书</p><p>/etc/kubernetes/pki/etcd/peer.crt</p><p>/etc/kubernetes/pki/etcd/peer.key</p><p>3.pod 中定义 Liveness 探针使用的客户端证书</p><p>/etc/kubernetes/pki/etcd/healthcheck-client.crt</p><p>/etc/kubernetes/pki/etcd/healthcheck-client.key</p><p>4.配置在 kube-apiserver 中用来与 etcd server 做双向认证的客户端证书</p><p>/etc/kubernetes/pki/apiserver-etcd-client.crt</p><p>/etc/kubernetes/pki/apiserver-etcd-client.key</p><ul><li>Serveice Account秘钥</li></ul><p>这组的密钥对儿仅提供给 kube-controller-manager 使用. kube-controller-manager 通过 sa.key 对 token 进行签名, master 节点通过公钥 sa.pub 进行签名的验证.</p><p>API Server的authenticating环节支持多种身份校验方式：client cert、bearer token、static password auth等，这些方式中有一种方式通过authenticating（Kubernetes API Server会逐个方式尝试），那么身份校验就会通过。一旦API Server发现client发起的request使用的是service account token的方式，API Server就会自动采用signed bearer token方式进行身份校验。而request就会使用携带的service account token参与验证。该token是API Server在创建service account时用API server启动参数：–service-account-key-file的值签署(sign)生成的。如果–service-account-key-file未传入任何值，那么将默认使用–tls-private-key-file的值，即API Server的私钥（server.key）。</p><p>通过authenticating后，API Server将根据Pod username所在的group：system:serviceaccounts和system:serviceaccounts:(NAMESPACE)的权限对其进行authority 和admission control两个环节的处理。在这两个环节中，cluster管理员可以对service account的权限进行细化设置。</p><p>/etc/kubernetes/pki/sa.key</p><p>/etc/kubernetes/pki/sa.pub</p><p>kubeadm 创建的集群, kube-proxy ,flannel,coreDNS是以 pod 形式运行的, 在 pod 中, 直接使用 service account 与 kube-apiserver 进行认证, 此时就不需要再单独为 kube-proxy 创建证书.</p><h3 id="3-Kubeadm本地读取集群配置"><a href="#3-Kubeadm本地读取集群配置" class="headerlink" title="3.Kubeadm本地读取集群配置"></a><em>3.Kubeadm本地读取集群配置</em></h3><p>正如默认的kubeadm 安装k8s集群时，会从外网拉取镜像。在kubeadm命令升级master证书时，它也会默认从网上读取一个stable.txt的文件。由于公司实际情况，这个问题得解决掉。<br>解决这个问题的办法，就是生成一个集群配置的yaml文件，然后，在运行命令时指定这个Yaml文件即可。<br>如何生居一个集群配置的yaml文件呢？命令如下：<br>kubeadm config view &gt; cluster.yaml</p><h3 id="4-重新生成master证书"><a href="#4-重新生成master证书" class="headerlink" title="4.重新生成master证书"></a><em>4.重新生成master证书</em></h3><p>建议不要重新生成ca证书，因为更新了ca证书，集群节点就需要手工操作，才能让集群正常(会涉及重新join)。<br>操作之前，先将/etc/kubernetes/pki下的证书文件，mv到其它文件夹，作个临时备份，不要删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs etcd-healthcheck-client --config cluster.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config cluster.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-server --config cluster.yaml</span><br><span class="line">kubeadm alpha phase certs front-proxy-client--config cluster.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config cluster.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-kubelet-client --config cluster.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver --config cluster.yaml</span><br><span class="line">kubeadm alpha phase certs sa --config cluster.yaml <span class="comment">#千万别更换这个证书，有坑</span></span><br></pre></td></tr></table></figure><h3 id="5-重新生成kubeconfig配置文件"><a href="#5-重新生成kubeconfig配置文件" class="headerlink" title="5.重新生成kubeconfig配置文件"></a><em>5.重新生成kubeconfig配置文件</em></h3><p>在生成这些新的证书文件之后，再需要kubeadm alpha phase config命令，重新生成新的kubeconfig文件。<br>操作之前，先将/etc/kubernetes/下的kubeconfig，mv到其它文件夹，作个临时备份，不要删除。<br>kubeadm alpha phase kubeconfig all –config cluster.yaml<br>所有的kubeconfig重新生成以后，替换到kubectl使用的config文件之后（默认位置为~.kube/config），即可正常操作kubectl命令了。</p><h3 id="6-Kubelet证书自动轮换"><a href="#6-Kubelet证书自动轮换" class="headerlink" title="6.Kubelet证书自动轮换"></a><em>6.Kubelet证书自动轮换</em></h3><p>kubelet证书分为server和client两种， k8s 1.10默认启用了client证书的自动轮换，但server证书自动轮换需要用户开启。</p><h3 id="7-Service-Account密钥更新"><a href="#7-Service-Account密钥更新" class="headerlink" title="7.Service Account密钥更新"></a><em>7.Service Account密钥更新</em></h3><p>由于service account的密钥是以rsa密钥对形式生成，所以没有过期时间。<br>如无必要，千万不要生成重新生成sa密钥。因为sa密钥关联到一切系统pod内的进程访问api server时的认证。<br>如果更新了sa，则需要先重新生成这些pod加截的token，再删除这些pod之后，重新加载token文件。<br>经过测试，这些系统级pod包括但不限于kube-proxy,flannel,kubenetes-dashboard, kube-stat-metricst等所有用到sa认证的pod(我也是因为重新生成了这个证书，导致kube-proxy无法访问k8s api ,好久才找出来是这个原因)</p><h3 id="8-其他问题"><a href="#8-其他问题" class="headerlink" title="8.其他问题"></a><em>8.其他问题</em></h3><p>之前我用新的ca根证书重新生成一套证书，替换后，master节点都没问题了，但是node节点无法和master节点通讯，想接入新的节点也加入不了，也是一个坑。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(版权声明：本文转载博客园「aguncn」的文章。&lt;a href=&quot;https://www.cnblogs.com/aguncn/p/10488130.html&quot;&gt;原文链接&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&quot;1-问题起源&quot;&gt;&lt;a href=&quot;#1-问题起源&quot; class=&quot;headerlink&quot; title=&quot;1.问题起源&quot;&gt;&lt;/a&gt;&lt;em&gt;1.问题起源&lt;/em&gt;&lt;/h3&gt;&lt;p&gt;kubeadm 是 kubernetes 提供的一个初始化集群的工具，使用起来非常方便。但是它创建的apiserver、controller-manager等证书默认只有一年的有效期，同时kubelet 证书也只有一年有效期，一年之后 kubernetes 将停止服务。&lt;br&gt;官方推荐一年之内至少用 kubeadm upgrade 更新一次 kubernetes 系统，更新时也会自动更新证书。不过，在产线环境或者无法连接外网的环境频繁更新 kubernetes 不太现实。我们可以在过期之前或之后，使用kubeadm alpha phase里的certs和kubeconfig命令，同时配合kubelet证书自动轮换机制来解决这个问题。&lt;/p&gt;
&lt;h3 id=&quot;2-k8s里面的证书详解&quot;&gt;&lt;a href=&quot;#2-k8s里面的证书详解&quot; class=&quot;headerlink&quot; title=&quot;2.k8s里面的证书详解&quot;&gt;&lt;/a&gt;&lt;em&gt;2.k8s里面的证书详解&lt;/em&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Kubernetes 集群根证书&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;/etc/kubernetes/pki/ca.crt&lt;/p&gt;
&lt;p&gt;/etc/kubernetes/pki/ca.key&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由此根证书签发的证书有:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1.kube-apiserver 组件持有的服务端证书&lt;/p&gt;
&lt;p&gt;/etc/kubernetes/pki/apiserver.crt&lt;/p&gt;
&lt;p&gt;/etc/kubernetes/pki/apiserver.key&lt;/p&gt;
&lt;p&gt;2.kubelet 组件持有的客户端证书,kubelet 上一般不会明确指定服务端证书, 而是只指定 ca 根证书, 让 kubelet 根据本地主机信息自动生成服务端证书并保存到配置的cert-dir文件夹中。&lt;/p&gt;
&lt;p&gt;/etc/kubernetes/pki/apiserver-kubelet-client.crt&lt;/p&gt;
&lt;p&gt;/etc/kubernetes/pki/apiserver-kubelet-client.key&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;汇聚层(aggregator)证书
    
    </summary>
    
    
      <category term="k8s" scheme="http://qingye.info/categories/k8s/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="k8s" scheme="http://qingye.info/tags/k8s/"/>
    
      <category term="容器" scheme="http://qingye.info/tags/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="docker" scheme="http://qingye.info/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>GlusterFS集群文件系统专题二(主要术语)</title>
    <link href="http://qingye.info/2020/02/28/linux/GlusterFS%20Topic%202(Main%20terms)/"/>
    <id>http://qingye.info/2020/02/28/linux/GlusterFS%20Topic%202(Main%20terms)/</id>
    <published>2020-02-28T14:51:30.000Z</published>
    <updated>2020-04-16T13:18:02.892Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Trusted-Storage-Pool"><a href="#1-Trusted-Storage-Pool" class="headerlink" title="1.Trusted Storage  Pool"></a><em>1.Trusted Storage  Pool</em></h3><ul><li>一堆存储节点的集合</li><li>通过一个节点“邀请”其他节点创建，这里叫probe</li><li>成员可以动态加入，动态删除<br>添加命令如下：<br>node1# gluster peer probe node2<br>删除命令如下：<br>node1# gluster peer detach node3</li></ul><p><img src="https://cloud.qingye.info/images/20200226/glusterfs2_1.png" alt="图1 GlusterFS存储池"></p><h3 id="2-Bricks"><a href="#2-Bricks" class="headerlink" title="2.Bricks"></a><em>2.Bricks</em></h3><ul><li>Brick是一个节点和一个导出目录的集合，e.g. node1:/brick1</li><li>Brick是底层的RAID或磁盘经XFS或ext4文件系统格式化而来，所以继承了文件系统的限制</li><li>每个节点上的brick数是不限的</li><li>理想的状况是，一个集群的所有Brick大小都一样<a id="more"></a><img src="https://cloud.qingye.info/images/20200226/glusterfs2_2.png" alt="图2 GlusterFS Bricks"></li></ul><h3 id="3-Volumes"><a href="#3-Volumes" class="headerlink" title="3.Volumes"></a><em>3.Volumes</em></h3><ul><li>Volume是brick的逻辑组合</li><li>创建时命名来识别</li><li>Volume是一个可挂载的目录</li><li>每个节点上的brick数是不变的,e.g.mount –t glusterfs <a href="http://www.std.com:test">www.std.com:test</a> /mnt/gls</li><li>一个节点上的不同brick可以属于不同的卷</li><li>支持如下种类：<br>a) 分布式卷<br>b) 条带卷<br>c) 复制卷<br>d) 分布式复制卷<br>e) 条带复制卷<br>f) 分布式条带复制卷</li></ul><h4 id="3-1-分布式卷"><a href="#3-1-分布式卷" class="headerlink" title="3.1 分布式卷"></a><em>3.1 分布式卷</em></h4><ul><li>文件分布存在不同的brick里</li><li>目录在每个brick里都可见</li><li>单个brick失效会带来数据丢失</li><li>无需额外元数据服务器</li></ul><p>gluster是没有元数据服务器的，它定位文件和寻址都是通过哈希算法，这里使用的叫Davies-Meyer hash algorithm，可寻址空间为2^32次方，即0-4294967296，例如这里有四个节点，那么0-1073741824为node1的可寻址空间，1073741825-214748348为node2的可寻址空间，以此类推。访问一个文件时，通过文件名计算出一个地址，例如2142011129，属于1073741825-214748348，则将它存在node2中。</p><p>分布式卷内部的hash分布如下：</p><p><img src="https://cloud.qingye.info/images/20200226/glusterfs2_3.png" alt="图3 GlusterFS 分布式卷内部的hash分布"></p><p>分布式卷的读写如下图所示:</p><p><img src="https://cloud.qingye.info/images/20200226/glusterfs2_7.png" alt="图4 GlusterFS 分布式卷"></p><h3 id="3-2-复制卷"><a href="#3-2-复制卷" class="headerlink" title="3.2 复制卷"></a><em>3.2 复制卷</em></h3><ul><li>同步复制所有的目录和文件</li><li>节点故障时保持数据高可用</li><li>事务性操作，保持一致性</li><li>有changelog</li><li>副本数任意定</li></ul><p>复制卷的读写如下图所示：</p><p><img src="https://cloud.qingye.info/images/20200226/glusterfs2_4.jpg" alt="图5 GlusterFS 复制卷"></p><h3 id="3-3条带卷"><a href="#3-3条带卷" class="headerlink" title="3.3条带卷"></a><em>3.3条带卷</em></h3><ul><li>文件切分成一个个的chunk，存放于不同的brick上</li><li>只建议在非常大的文件时使用（比硬盘大小还大）</li><li>Brick故障会导致数据丢失，建议和复制卷同时使用</li><li>Chunks are files with holes – this helps in maintaining offset consistency</li></ul><p><img src="https://cloud.qingye.info/images/20200226/glusterfs2_9.png" alt="图6 GlusterFS 条带卷"></p><h3 id="3-3条带复制卷"><a href="#3-3条带复制卷" class="headerlink" title="3.3条带复制卷"></a><em>3.3条带复制卷</em></h3><ul><li>数据将进行切片，切片在复本卷内进行复制，在不同卷间进行分布</li></ul><p><img src="https://cloud.qingye.info/images/20200226/glusterfs2_6.png" alt="图7 GlusterFS 条带复制卷"></p><h3 id="3-3分布式复制卷"><a href="#3-3分布式复制卷" class="headerlink" title="3.3分布式复制卷"></a><em>3.3分布式复制卷</em></h3><ul><li>最常见的一种模式</li><li>读操作可以做到负载均衡</li><li>复本卷的组成依赖于指定brick的顺序，brick必须为复本数K的N倍,brick列表将以K个为一组，形成N个复本卷</li></ul><p><img src="https://cloud.qingye.info/images/20200226/glusterfs2_5.jpg" alt="图8 GlusterFS 分布式复制卷"></p><h3 id="3-4分布式条带复制卷"><a href="#3-4分布式条带复制卷" class="headerlink" title="3.4分布式条带复制卷"></a><em>3.4分布式条带复制卷</em></h3><ul><li>bricks数量为stripe个数N，和repl个数M的积N*M的整数倍</li><li>exp1 exp2 exp3 exp4组成一个分布卷，exp1和exp2组成一个stripe卷，exp3和exp4组成另一个stripe卷，1和2，3和4互为复本卷，exp4-exp8组成另一个分布卷</li></ul><p><img src="https://cloud.qingye.info/images/20200226/glusterfs2_8.png" alt="图9 GlusterFS 分布式条带复制卷"></p><h3 id="4-其他术语"><a href="#4-其他术语" class="headerlink" title="4.其他术语"></a><em>4.其他术语</em></h3><ul><li>Client:挂载了GFS卷的设备</li><li>Extended Attributes:xattr:是一个文件系统的特性,其支持用户或程序关联文件/目录和元数据。</li><li>FUSE:Filesystem Userspace,是一个可加载的内核模块，其支持非特权用户创建自己的文件系统而不需要修改内核代码,通过在用户空间运行文件系统的代码通过FUSE代码与内核进行桥接</li><li>Geo-Replication：异地备份，提供了一种持续，异步，增量数据备份策略，可以通过局域网，广域网，英特网来进行</li><li>GFID:GFS卷中的每个文件或目录都有一个唯一的128位的数据相关联，其用于模拟inode</li><li>Namespace:每个Gluster卷都导出单个ns作为POSIX的挂载点</li><li>Node:一个拥有若干brick的设备</li><li>RDMA:远程直接内存访问,支持不通过双方的OS进行直接内存访问</li><li>RRDNS:round robin DNS是一种通过DNS轮转返回不同的设备以进行负载均衡的方法</li><li>Self-heal:用于后台运行检测复本卷中文件和目录的不一致性并解决这些不一致</li><li>Split-brain:脑裂</li><li>Volfile:glusterfs进程的配置文件,通常位于/var/lib/glusterd/vols/volname</li><li>Volume:一组bricks的逻辑集合</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-Trusted-Storage-Pool&quot;&gt;&lt;a href=&quot;#1-Trusted-Storage-Pool&quot; class=&quot;headerlink&quot; title=&quot;1.Trusted Storage  Pool&quot;&gt;&lt;/a&gt;&lt;em&gt;1.Trusted Storage  Pool&lt;/em&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;一堆存储节点的集合&lt;/li&gt;
&lt;li&gt;通过一个节点“邀请”其他节点创建，这里叫probe&lt;/li&gt;
&lt;li&gt;成员可以动态加入，动态删除&lt;br&gt;添加命令如下：&lt;br&gt;node1# gluster peer probe node2&lt;br&gt;删除命令如下：&lt;br&gt;node1# gluster peer detach node3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://cloud.qingye.info/images/20200226/glusterfs2_1.png&quot; alt=&quot;图1 GlusterFS存储池&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-Bricks&quot;&gt;&lt;a href=&quot;#2-Bricks&quot; class=&quot;headerlink&quot; title=&quot;2.Bricks&quot;&gt;&lt;/a&gt;&lt;em&gt;2.Bricks&lt;/em&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Brick是一个节点和一个导出目录的集合，e.g. node1:/brick1&lt;/li&gt;
&lt;li&gt;Brick是底层的RAID或磁盘经XFS或ext4文件系统格式化而来，所以继承了文件系统的限制&lt;/li&gt;
&lt;li&gt;每个节点上的brick数是不限的&lt;/li&gt;
&lt;li&gt;理想的状况是，一个集群的所有Brick大小都一样
    
    </summary>
    
    
      <category term="glusterfs" scheme="http://qingye.info/categories/glusterfs/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="glusterfs" scheme="http://qingye.info/tags/glusterfs/"/>
    
  </entry>
  
  <entry>
    <title>GlusterFS集群文件系统专题一(基础原理)</title>
    <link href="http://qingye.info/2020/02/25/linux/GlusterFS%20Topic%202(Basic%20principles)/"/>
    <id>http://qingye.info/2020/02/25/linux/GlusterFS%20Topic%202(Basic%20principles)/</id>
    <published>2020-02-25T15:20:56.000Z</published>
    <updated>2020-04-16T13:19:01.160Z</updated>
    
    <content type="html"><![CDATA[<p>(版权声明：本文为CSDN博主「刘爱贵」的原创文章。<a href="https://blog.csdn.net/liuaigui/article/details/6284551">原文链接</a>)</p><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h3><p>GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBand RDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。GlusterFS基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。</p><p><img src="https://cloud.qingye.info/images/20200225/glusterfs1_1.png" alt="图1 GlusterFS统一的挂载点"></p><p>GlusterFS支持运行在任何标准IP网络上标准应用程序的标准客户端，如图2所示，用户可以在全局统一的命名空间中使用NFS/CIFS等标准协议来访问应用数据。GlusterFS使得用户可摆脱原有的独立、高成本的封闭存储系统，能够利用普通廉价的存储设备来部署可集中管理、横向扩展、虚拟化的存储池，存储容量可扩展至TB/PB级。GlusterFS主要特征如下：</p><ul><li><strong>扩展性和高性能</strong></li></ul><p>GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（Elastic Hash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。</p><ul><li><p><strong>高可用性</strong></p><a id="more"></a><p>GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至是在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。GlusterFS没有设计自己的私有数据文件格式，而是采用操作系统中主流标准的磁盘文件系统（如EXT3、ZFS）来存储文件，因此数据可以使用各种标准工具进行复制和访问。</p></li><li><p><strong>全局统一命名空间</strong></p></li></ul><p>全局统一命名空间将磁盘和内存资源聚集成一个单一的虚拟存储池，对上层用户和应用屏蔽了底层的物理硬件。存储资源可以根据需要在虚拟存储池中进行弹性扩展，比如扩容或收缩。当存储虚拟机映像时，存储的虚拟映像文件没有数量限制，成千虚拟机均通过单一挂载点进行数据共享。虚拟机I/O可在命名空间内的所有服务器上自动进行负载均衡，消除了SAN环境中经常发生的访问热点和性能瓶颈问题。</p><ul><li><strong>一致性哈希算法</strong></li></ul><p>GlusterFS采用弹性哈希算法在存储池中定位数据，而不是采用集中式或分布式元数据服务器索引。在其他的Scale-Out存储系统中，元数据服务器通常会导致I/O性能瓶颈和单点故障问题。GlusterFS中，所有在Scale-Out存储配置中的存储系统都可以智能地定位任意数据分片，不需要查看索引或者向其他服务器查询。这种设计机制完全并行化了数据访问，实现了真正的线性性能扩展。</p><ul><li><strong>弹性卷管理</strong></li></ul><p>数据储存在逻辑卷中，逻辑卷可以从虚拟化的物理存储池进行独立逻辑划分而得到。存储服务器可以在线进行增加和移除，不会导致应用中断。逻辑卷可以在所有配置服务器中增长和缩减，可以在不同服务器迁移进行容量均衡，或者增加和移除系统，这些操作都可在线进行。文件系统配置更改也可以实时在线进行并应用，从而可以适应工作负载条件变化或在线性能调优。</p><ul><li><strong>工作于标准协议</strong></li></ul><p>Gluster存储服务支持NFS, CIFS, HTTP, FTP以及Gluster原生协议，完全与POSIX标准兼容。现有应用程序不需要作任何修改或使用专用API，就可以对Gluster中的数据进行访问。这在公有云环境中部署Gluster时非常有用，Gluster对云服务提供商专用API进行抽象，然后提供标准POSIX接口。</p><h3 id="2-设计目标"><a href="#2-设计目标" class="headerlink" title="2.设计目标"></a>2.设计目标</h3><p>GlusterFS的设计思想显著区别有现有并行/集群/分布式文件系统。如果GlusterFS在设计上没有本质性的突破，难以在与Lustre、PVFS2、Ceph等的竞争中占据优势，更别提与GPFS、StorNext、ISILON、IBRIX等具有多年技术沉淀和市场积累的商用文件系统竞争。其核心设计目标包括如下三个：</p><ul><li><strong>弹性存储系统（Elasticity）</strong></li></ul><p>存储系统具有弹性能力，意味着企业可以根据业务需要灵活地增加或缩减数据存储以及增删存储池中的资源，而不需要中断系统运行。GlusterFS设计目标之一就是弹性，允许动态增删数据卷、扩展或缩减数据卷、增删存储服务器等，不影响系统正常运行和业务服务。GlusterFS早期版本中弹性不足，部分管理工作需要中断服务，目前最新的3.1.X版本已经弹性十足，能够满足对存储系统弹性要求高的应用需求，尤其是对云存储服务系统而言意义更大。GlusterFS主要通过存储虚拟化技术和逻辑卷管理来实现这一设计目标。</p><ul><li><strong>线性横向扩展（Linear Scale-Out）</strong></li></ul><p>线性扩展对于存储系统而言是非常难以实现的，通常系统规模扩展与性能提升之间是LOG对数曲线关系，因为同时会产生相应负载而消耗了部分性能的提升。现在的很多并行/集群/分布式文件系统都具很高的扩展能力，Luster存储节点可以达到1000个以上，客户端数量能够达到25000以上，这个扩展能力是非常强大的，但是Lustre也不是线性扩展的。</p><p>纵向扩展（Scale-Up）旨在提高单个节点的存储容量或性能，往往存在理论上或物理上的各种限制，而无法满足存储需求。横向扩展（Scale-Out）通过增加存储节点来提升整个系统的容量或性能，这一扩展机制是目前的存储技术热点，能有效应对容量、性能等存储需求。目前的并行/集群/分布式文件系统大多都具备横向扩展能力。</p><p>GlusterFS是线性横向扩展架构，它通过横向扩展存储节点即可以获得线性的存储容量和性能的提升。因此，结合纵向扩展GlusterFS可以获得多维扩展能力，增加每个节点的磁盘可增加存储容量，增加存储节点可以提高性能，从而将更多磁盘、内存、I/O资源聚集成更大容量、更高性能的虚拟存储池。GlusterFS利用三种基本技术来获得线性横向扩展能力：</p><p>1)消除元数据服务</p><p>2)高效数据分布，获得扩展性和可靠性</p><p>3)通过完全分布式架构的并行化获得性能的最大化</p><ul><li><strong>可靠性（Reliability）</strong></li></ul><p>与GFS（Google File System）类似，GlusterFS可以构建在普通的服务器和存储设备之上，因此可靠性显得尤为关键。GlusterFS从设计之初就将可靠性纳入核心设计，采用了多种技术来实现这一设计目标。首先，它假设故障是正常事件，包括硬件、磁盘、网络故障以及管理员误操作造成的数据损坏等。GlusterFS设计支持自动复制和自动修复功能来保证数据可靠性，不需要管理员的干预。其次，GlusterFS利用了底层EXT3/ZFS等磁盘文件系统的日志功能来提供一定的数据可靠性，而没有自己重新发明轮子。再次，GlusterFS是无元数据服务器设计，不需要元数据的同步或者一致性维护，很大程度上降低了系统复杂性，不仅提高了性能，还大大提高了系统可靠性。</p><h3 id="3-技术特点"><a href="#3-技术特点" class="headerlink" title="3.技术特点"></a>3.技术特点</h3><p>GlusterFS在技术实现上与传统存储系统或现有其他分布式文件系统有显著不同之处，主要体现在如下几个方面。</p><ul><li><strong>全软件实现（Software Only）</strong></li></ul><p>GlusterFS认为存储是软件问题，不能够把用户局限于使用特定的供应商或硬件配置来解决。GlusterFS采用开放式设计，广泛支持工业标准的存储、网络和计算机设备，而非与定制化的专用硬件设备捆绑。对于商业客户，GlusterFS可以以虚拟装置的形式交付，也可以与虚拟机容器打包，或者是公有云中部署的映像。开源社区中，GlusterFS被大量部署在基于廉价闲置硬件的各种操作系统上，构成集中统一的虚拟存储资源池。简而言之，GlusterFS是开放的全软件实现，完全独立于硬件和操作系统。</p><ul><li><strong>完整的存储操作系统栈（Complete Storage Operating System Stack）</strong></li></ul><p>GlusterFS不仅提供了一个分布式文件系统，而且还提供了许多其他重要的分布式功能，比如分布式内存管理、I/O调度、软RAID和自我修复等。GlusterFS汲取了微内核架构的经验教训，借鉴了GNU/Hurd操作系统的设计思想，在用户空间实现了完整的存储操作系统栈。</p><ul><li><strong>用户空间实现（User Space）</strong></li></ul><p>与传统的文件系统不同，GlusterFS在用户空间实现，这使得其安装和升级特别简便。另外，这也极大降低了普通用户基于源码修改GlusterFS的门槛，仅仅需要通用的C程序设计技能，而不需要特别的内核编程经验。</p><ul><li><strong>模块化堆栈式架构（Modular Stackable Architecture）</strong></li></ul><p>GlusterFS采用模块化、堆栈式的架构，可通过灵活的配置支持高度定制化的应用环境，比如大文件存储、海量小文件存储、云存储、多传输协议应用等。每个功能以模块形式实现，然后以积木方式进行简单的组合，即可实现复杂的功能。比如，Replicate模块可实现RAID1，Stripe模块可实现RAID0，通过两者的组合可实现RAID10和RAID01，同时获得高性能和高可靠性。</p><ul><li><strong>原始数据格式存储（Data Stored in Native Formats）</strong></li></ul><p>GlusterFS以原始数据格式（如EXT3、EXT4、XFS、ZFS）储存数据，并实现多种数据自动修复机制。因此，系统极具弹性，即使离线情形下文件也可以通过其他标准工具进行访问。如果用户需要从GlusterFS中迁移数据，不需要作任何修改仍然可以完全使用这些数据。</p><ul><li><strong>无元数据服务设计（No Metadata with the Elastic Hash Algorithm）</strong></li></ul><p>对Scale-Out存储系统而言，最大的挑战之一就是记录数据逻辑与物理位置的映像关系，即数据元数据，可能还包括诸如属性和访问权限等信息。传统分布式存储系统使用集中式或分布式元数据服务来维护元数据，集中式元数据服务会导致单点故障和性能瓶颈问题，而分布式元数据服务存在性能负载和元数据同步一致性问题。特别是对于海量小文件的应用，元数据问题是个非常大的挑战。</p><p>GlusterFS独特地采用无元数据服务的设计，取而代之使用算法来定位文件，元数据和数据没有分离而是一起存储。集群中的所有存储系统服务器都可以智能地对文件数据分片进行定位，仅仅根据文件名和路径并运用算法即可，而不需要查询索引或者其他服务器。这使得数据访问完全并行化，从而实现真正的线性性能扩展。无元数据服务器极大提高了GlusterFS的性能、可靠性和稳定性。</p><h3 id="4-总体架构与设计"><a href="#4-总体架构与设计" class="headerlink" title="4.总体架构与设计"></a>4.总体架构与设计</h3><p><img src="https://cloud.qingye.info/images/20200225/glusterfs1_2.png" alt="图2 GlusterFS架构和组成"></p><p>GlusterFS总体架构与组成部分如图2所示，它主要由存储服务器（Brick Server）、客户端以及NFS/Samba存储网关组成。不难发现，GlusterFS架构中没有元数据服务器组件，这是其最大的设计这点，对于提升整个系统的性能、可靠性和稳定性都有着决定性的意义。GlusterFS支持TCP/IP和InfiniBand RDMA高速网络互联，客户端可通过原生Glusterfs协议访问数据，其他没有运行GlusterFS客户端的终端可通过NFS/CIFS标准协议通过存储网关访问数据。</p><p>存储服务器主要提供基本的数据存储功能，最终的文件数据通过统一的调度策略分布在不同的存储服务器上。它们上面运行着Glusterfsd进行，负责处理来自其他组件的数据服务请求。如前所述，数据以原始格式直接存储在服务器的本地文件系统上，如EXT3、EXT4、XFS、ZFS等，运行服务时指定数据存储路径。多个存储服务器可以通过客户端或存储网关上的卷管理器组成集群，如Stripe（RAID0）、Replicate（RAID1）和DHT（分布式Hash）存储集群，也可利用嵌套组合构成更加复杂的集群，如RAID10。</p><p>由于没有了元数据服务器，客户端承担了更多的功能，包括数据卷管理、I/O调度、文件定位、数据缓存等功能。客户端上运行Glusterfs进程，它实际是Glusterfsd的符号链接，利用FUSE（File system in User Space）模块将GlusterFS挂载到本地文件系统之上，实现POSIX兼容的方式来访问系统数据。在最新的3.1.X版本中，客户端不再需要独立维护卷配置信息，改成自动从运行在网关上的glusterd弹性卷管理服务进行获取和更新，极大简化了卷管理。GlusterFS客户端负载相对传统分布式文件系统要高，包括CPU占用率和内存占用。</p><p>GlusterFS存储网关提供弹性卷管理和NFS/CIFS访问代理功能，其上运行Glusterd和Glusterfs进程，两者都是Glusterfsd符号链接。卷管理器负责逻辑卷的创建、删除、容量扩展与缩减、容量平滑等功能，并负责向客户端提供逻辑卷信息及主动更新通知功能等。GlusterFS 3.1.X实现了逻辑卷的弹性和自动化管理，不需要中断数据服务或上层应用业务。对于Windows客户端或没有安装GlusterFS的客户端，需要通过NFS/CIFS代理网关来访问，这时网关被配置成NFS或Samba服务器。相对原生客户端，网关在性能上要受到NFS/Samba的制约。</p><p><img src="https://cloud.qingye.info/images/20200225/glusterfs1_3.png" alt="图3 GlusterFS模块化堆栈式设计"></p><p>GlusterFS是模块化堆栈式的架构设计，如图3所示。模块称为Translator，是GlusterFS提供的一种强大机制，借助这种良好定义的接口可以高效简便地扩展文件系统的功能。服务端与客户端模块接口是兼容的，同一个translator可同时在两边加载。每个translator都是SO动态库，运行时根据配置动态加载。每个模块实现特定基本功能，GlusterFS中所有的功能都是通过translator实现，比如Cluster, Storage, Performance, Protocol, Features等，基本简单的模块可以通过堆栈式的组合来实现复杂的功能。这一设计思想借鉴了GNU/Hurd微内核的虚拟文件系统设计，可以把对外部系统的访问转换成目标系统的适当调用。大部分模块都运行在客户端，比如合成器、I/O调度器和性能优化等，服务端相对简单许多。客户端和存储服务器均有自己的存储栈，构成了一棵Translator功能树，应用了若干模块。模块化和堆栈式的架构设计，极大降低了系统设计复杂性，简化了系统的实现、升级以及系统维护。</p><h3 id="5-弹性哈希算法"><a href="#5-弹性哈希算法" class="headerlink" title="5.弹性哈希算法"></a>5.弹性哈希算法</h3><p>对于分布式系统而言，元数据处理是决定系统扩展性、性能以及稳定性的关键。GlusterFS另辟蹊径，彻底摒弃了元数据服务，使用弹性哈希算法代替传统分布式文件系统中的集中或分布式元数据服务。这根本性解决了元数据这一难题，从而获得了接近线性的高扩展性，同时也提高了系统性能和可靠性。GlusterFS使用算法进行数据定位，集群中的任何服务器和客户端只需根据路径和文件名就可以对数据进行定位和读写访问。换句话说，GlusterFS不需要将元数据与数据进行分离，因为文件定位可独立并行化进行。GlusterFS中数据访问流程如下：</p><p>1、计算hash值，输入参数为文件路径和文件名；</p><p>2、根据hash值在集群中选择子卷（存储服务器），进行文件定位；</p><p>3、对所选择的子卷进行数据访问。</p><p>GlusterFS目前使用Davies-Meyer算法计算文件名hash值，获得一个32位整数。Davies-Meyer算法具有非常好的hash分布性，计算效率很高。假设逻辑卷中的存储服务器有N个，则32位整数空间被平均划分为N个连续子空间，每个空间分别映射到一个存储服务器。这样，计算得到的32位hash值就会被投射到一个存储服务器，即我们要选择的子卷。难道真是如此简单？现在让我们来考虑一下存储节点加入和删除、文件改名等情况，GlusterFS如何解决这些问题而具备弹性的呢？</p><p>逻辑卷中加入一个新存储节点，如果不作其他任何处理，hash值映射空间将会发生变化，现有的文件目录可能会被重新定位到其他的存储服务器上，从而导致定位失败。解决问题的方法是对文件目录进行重新分布，把文件移动到正确的存储服务器上去，但这大大加重了系统负载，尤其是对于已经存储大量的数据的海量存储系统来说显然是不可行的。另一种方法是使用一致性哈希算法，修改新增节点及相邻节点的hash映射空间，仅需要移动相邻节点上的部分数据至新增节点，影响相对小了很多。然而，这又带来另外一个问题，即系统整体负载不均衡。GlusterFS没有采用上述两种方法，而是设计了更为弹性的算法。GlusterFS的哈希分布是以目录为基本单位的，文件的父目录利用扩展属性记录了子卷映射信息，其下面子文件目录在父目录所属存储服务器中进行分布。由于文件目录事先保存了分布信息，因此新增节点不会影响现有文件存储分布，它将从此后的新创建目录开始参与存储分布调度。这种设计，新增节点不需要移动任何文件，但是负载均衡没有平滑处理，老节点负载较重。GlusterFS在设计中考虑了这一问题，在新建文件时会优先考虑容量负载最轻的节点，在目标存储节点上创建文件链接直向真正存储文件的节点。另外，GlusterFS弹性卷管理工具可以在后台以人工方式来执行负载平滑，将进行文件移动和重新分布，此后所有存储服务器都会均会被调度。</p><p>GlusterFS目前对存储节点删除支持有限，还无法做到完全无人干预的程度。如果直接删除节点，那么所在存储服务器上的文件将无法浏览和访问，创建文件目录也会失败。当前人工解决方法有两个，一是将节点上的数据重新复制到GlusterFS中，二是使用新的节点来替换删除节点并保持原有数据。</p><p>如果一个文件被改名，显然hash算法将产生不同的值，非常可能会发生文件被定位到不同的存储服务器上，从而导致文件访问失败。采用数据移动的方法，对于大文件是很难在实时完成的。为了不影响性能和服务中断，GlusterFS采用了文件链接来解决文件重命名问题，在目标存储服务器上创建一个链接指向实际的存储服务器，访问时由系统解析并进行重定向。另外，后台同时进行文件迁移，成功后文件链接将被自动删除。对于文件移动也作类似处理，好处是前台操作可实时处理，物理数据迁移置于后台选择适当时机执行。</p><p><img src="https://cloud.qingye.info/images/20200225/glusterfs1_4.png" alt="图4 GlusterFS弹性卷管理"></p><p>弹性哈希算法为文件分配逻辑卷，那么GlusterFS如何为逻辑卷分配物理卷呢？GlusterFS3.1.X实现了真正的弹性卷管理，如图4所示。存储卷是对底层硬件的抽象，可以根据需要进行扩容和缩减，以及在不同物理系统之间进行迁移。存储服务器可以在线增加和移除，并能在集群之间自动进行数据负载平衡，数据总是在线可用，没有应用中断。文件系统配置更新也可以在线执行，所作配置变动能够快速动态地在集群中传播，从而自动适应负载波动和性能调优。</p><p>弹性哈希算法本身并没有提供数据容错功能，GlusterFS使用镜像或复制来保证数据可用性，推荐使用镜像或3路复制。复制模式下，存储服务器使用同步写复制到其他的存储服务器，单个服务器故障完全对客户端透明。此外，GlusterFS没有对复制数量进行限制，读被分散到所有的镜像存储节点，可以提高读性能。弹性哈希算法分配文件到唯一的逻辑卷，而复制可以保证数据至少保存在两个不同存储节点，两者结合使得GlusterFS具备更高的弹性。</p><h3 id="6-Translators"><a href="#6-Translators" class="headerlink" title="6.Translators"></a>6.Translators</h3><p>如前所述，Translators是GlusterFS提供的一种强大文件系统功能扩展机制，这一设计思想借鉴于GNU/Hurd微内核操作系统。GlusterFS中所有的功能都通过Translator机制实现，运行时以动态库方式进行加载，服务端和客户端相互兼容。GlusterFS 3.1.X中，主要包括以下几类Translator：</p><p>（1）  Cluster：存储集群分布，目前有AFR, DHT, Stripe三种方式</p><p>（2）  Debug：跟踪GlusterFS内部函数和系统调用</p><p>（3）  Encryption：简单的数据加密实现</p><p>（4）  Features：访问控制、锁、Mac兼容、静默、配额、只读、回收站等</p><p>（5）  Mgmt：弹性卷管理</p><p>（6）  Mount：FUSE接口实现</p><p>（7）  Nfs：内部NFS服务器</p><p>（8）  Performance：io-cache, io-threads, quick-read, read-ahead, stat-prefetch, sysmlink-cache, write-behind等性能优化</p><p>（9）  Protocol：服务器和客户端协议实现</p><p>（10）Storage：底层文件系统POSIX接口实现</p><p>这里我们重点介绍一下Cluster Translators，它是实现GlusterFS集群存储的核心，它包括AFR（Automatic File Replication）、DHT（Distributed Hash Table）和Stripe三种类型。</p><p>AFR相当于RAID1，同一文件在多个存储节点上保留多份，主要用于实现高可用性以及数据自动修复。AFR所有子卷上具有相同的名字空间，查找文件时从第一个节点开始，直到搜索成功或最后节点搜索完毕。读数据时，AFR会把所有请求调度到所有存储节点，进行负载均衡以提高系统性能。写数据时，首先需要在所有锁服务器上对文件加锁，默认第一个节点为锁服务器，可以指定多个。然后，AFR以日志事件方式对所有服务器进行写数据操作，成功后删除日志并解锁。AFR会自动检测并修复同一文件的数据不一致性，它使用更改日志来确定好的数据副本。自动修复在文件目录首次访问时触发，如果是目录将在所有子卷上复制正确数据，如果文件不存则创建，文件信息不匹配则修复，日志指示更新则进行更新。</p><p>DHT即上面所介绍的弹性哈希算法，它采用hash方式进行数据分布，名字空间分布在所有节点上。查找文件时，通过弹性哈希算法进行，不依赖名字空间。但遍历文件目录时，则实现较为复杂和低效，需要搜索所有的存储节点。单一文件只会调度到唯一的存储节点，一旦文件被定位后，读写模式相对简单。DHT不具备容错能力，需要借助AFR实现高可用性, 如图5所示应用案例。</p><p>Stripe相当于RAID0，即分片存储，文件被划分成固定长度的数据分片以Round-Robin轮转方式存储在所有存储节点。Stripe所有存储节点组成完整的名字空间，查找文件时需要询问所有节点，这点非常低效。读写数据时，Stripe涉及全部分片存储节点，操作可以在多个节点之间并发执行，性能非常高。Stripe通常与AFR组合使用，构成RAID10/RAID01，同时获得高性能和高可用性，当然存储利用率会低于50%。</p><p><img src="https://cloud.qingye.info/images/20200225/glusterfs1_5.png" alt="图5　GlusterFS应用案例：AFR+DHT"></p><h3 id="7-设计讨论"><a href="#7-设计讨论" class="headerlink" title="7. 设计讨论"></a>7. 设计讨论</h3><p>GlusterFS是一个具有高扩展性、高性能、高可用性、可横向扩展的弹性分布式文件系统，在架构设计上非常有特点，比如无元数据服务器设计、堆栈式架构等。然而，存储应用问题是很复杂的，GlusterFS也不可能满足所有的存储需求，设计实现上也一定有考虑不足之处，下面我们作简要分析。</p><ul><li><strong>元数据服务器</strong> <strong>vs</strong> <strong>元数据服务器</strong></li></ul><p>无元数据服务器设计的好处是没有单点故障和性能瓶颈问题，可提高系统扩展性、性能、可靠性和稳定性。对于海量小文件应用，这种设计能够有效解决元数据的难点问题。它的负面影响是，数据一致问题更加复杂，文件目录遍历操作效率低下，缺乏全局监控管理功能。同时也导致客户端承担了更多的职能，比如文件定位、名字空间缓存、逻辑卷视图维护等等，这些都增加了客户端的负载，占用相当的CPU和内存。</p><ul><li><strong>户空间</strong> <strong>vs</strong> <strong>内核空间</strong></li></ul><p>用户空间实现起来相对要简单许多，对开发者技能要求较低，运行相对安全。用户空间效率低，数据需要多次与内核空间交换，另外GlusterFS借助FUSE来实现标准文件系统接口，性能上又有所损耗。内核空间实现可以获得很高的数据吞吐量，缺点是实现和调试非常困难，程序出错经常会导致系统崩溃，安全性低。纵向扩展上，内核空间要优于用户空间，GlusterFS有横向扩展能力来弥补。</p><ul><li><strong>栈式</strong> <strong>vs</strong> <strong>非堆栈式</strong></li></ul><p>这有点像操作系统的微内核设计与单一内核设计之争。GlusterFS堆栈式设计思想源自GNU/Hurd微内核操作系统，具有很强的系统扩展能力，系统设计实现复杂性降低很多，基本功能模块的堆栈式组合就可以实现强大的功能。查看GlusterFS卷配置文件我们可以发现，translator功能树通常深达10层以上，一层一层进行调用，效率可见一斑。非堆栈式设计可看成类似Linux的单一内核设计，系统调用通过中断实现，非常高效。后者的问题是系统核心臃肿，实现和扩展复杂，出现问题调试困难。</p><ul><li><strong>始存储格式</strong> <strong>vs</strong> <strong>私有存储格式</strong></li></ul><p>usterFS使用原始格式存储文件或数据分片，可以直接使用各种标准的工具进行访问，数据互操作性好，迁移和数据管理非常方便。然而，数据安全成了问题，因为数据是以平凡的方式保存的，接触数据的人可以直接复制和查看。这对很多应用显然是不能接受的，比如云存储系统，用户特别关心数据安全，这也是影响公有云存储发展的一个重要原因。私有存储格式可以保证数据的安全性，即使泄露也是不可知的。GlusterFS要实现自己的私有格式，在设计实现和数据管理上相对复杂一些，也会对性能产生一定影响。</p><ul><li><strong>文件</strong> <strong>vs</strong> <strong>小文件</strong></li></ul><p>GlusterFS适合大文件还是小文件存储？弹性哈希算法和Stripe数据分布策略，移除了元数据依赖，优化了数据分布，提高数据访问并行性，能够大幅提高大文件存储的性能。对于小文件，无元数据服务设计解决了元数据的问题。但GlusterFS并没有在I/O方面作优化，在存储服务器底层文件系统上仍然是大量小文件，本地文件系统元数据访问是一个瓶颈，数据分布和并行性也无法充分发挥作用。因此，GlusterFS适合存储大文件，小文件性能较差，还存在很大优化空间。</p><ul><li><strong>高可用性</strong> <strong>vs</strong> <strong>存储利用率</strong></li></ul><p>GlusterFS使用复制技术来提供数据高可用性，复制数量没有限制，自动修复功能基于复制来实现。可用性与存储利用率是一个矛盾体，可用性高存储利用率就低，反之亦然。采用复制技术，存储利用率为1/复制数，镜像是50%，三路复制则只有33%。其实，可以有方法来同时提高可用性和存储利用率，比如RAID5的利用率是(n-1)/n，RAID6是(n-2)/n，而纠删码技术可以提供更高的存储利用率。但是，鱼和熊掌不可得兼，它们都会对性能产生较大影响。</p><p>另外，GlusterFS目前的代码实现不够好，系统不够稳定，BUGS数量相对还比较多。从其官方网站的部署情况来看，测试用户非常多，但是真正在生产环境中的应用较少，存储部署容量几TB－几十TB的占很大比率，数百TB－PB级案例非常少。这也可以从另一个方面说明，GlusterFS目前还不够稳定，需要更长的时间来检验。然而不可否认，GlusterFS是一个有着光明前景的集群文件系统，线性横向扩展能力使它具有天生的优势，尤其是对于云存储系统。</p><h3 id="8-参考文献"><a href="#8-参考文献" class="headerlink" title="8.参考文献"></a>8.参考文献</h3><p>[1] Gluster: <a href="http://www.gluster.com/products/gluster-file-system-architecture-white-paper/">http://www.gluster.com/products/gluster-file-system-architecture-white-paper/</a></p><p>[2] Gluster: <a href="http://www.gluster.com/products/performance-in-a-gluster-system-white-paper/">http://www.gluster.com/products/performance-in-a-gluster-system-white-paper/</a></p><p>[3] Gluster: <a href="http://gluster.com/community/documentation/index.php/Main_Page">http://gluster.com/community/documentation/index.php/Main_Page</a></p><p>[4] GlusterFS-Design: <a href="http://edwyseguru.wordpress.com/2010/06/11/glusterfs-design/">http://edwyseguru.wordpress.com/2010/06/11/glusterfs-design/</a></p><p>[5] GlusterFS users: <a href="http://www.gluster.org/gluster-users/">http://www.gluster.org/gluster-users/</a></p><p>[6] GlusterFS sources: <a href="http://download.gluster.com/pub/gluster/glusterfs/3.1/">http://download.gluster.com/pub/gluster/glusterfs/3.1/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(版权声明：本文为CSDN博主「刘爱贵」的原创文章。&lt;a href=&quot;https://blog.csdn.net/liuaigui/article/details/6284551&quot;&gt;原文链接&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1.概述&quot;&gt;&lt;/a&gt;1.概述&lt;/h3&gt;&lt;p&gt;GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBand RDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。GlusterFS基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cloud.qingye.info/images/20200225/glusterfs1_1.png&quot; alt=&quot;图1 GlusterFS统一的挂载点&quot;&gt;&lt;/p&gt;
&lt;p&gt;GlusterFS支持运行在任何标准IP网络上标准应用程序的标准客户端，如图2所示，用户可以在全局统一的命名空间中使用NFS/CIFS等标准协议来访问应用数据。GlusterFS使得用户可摆脱原有的独立、高成本的封闭存储系统，能够利用普通廉价的存储设备来部署可集中管理、横向扩展、虚拟化的存储池，存储容量可扩展至TB/PB级。GlusterFS主要特征如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;扩展性和高性能&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（Elastic Hash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;高可用性&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="glusterfs" scheme="http://qingye.info/categories/glusterfs/"/>
    
    
      <category term="linux" scheme="http://qingye.info/tags/linux/"/>
    
      <category term="centos" scheme="http://qingye.info/tags/centos/"/>
    
      <category term="glusterfs" scheme="http://qingye.info/tags/glusterfs/"/>
    
  </entry>
  
</feed>
